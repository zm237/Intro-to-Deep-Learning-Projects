{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Grab the MNIST dataset\n",
    "training_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testing_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "tfm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset_full_CIFAR10 = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=tfm)\n",
    "testset_full_CIFAR10  = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=tfm)"
   ],
   "id": "fffd1952ab91cd32",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verify that GPU is connected and available\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "id": "62c84a2afc22de88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T04:36:31.304468Z",
     "start_time": "2025-12-09T04:36:31.295854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CIFAR10_Classifier(nn.Module):\n",
    "    def __init__(self, C_dropout, F_dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        conv2d_dropout = C_dropout\n",
    "\n",
    "        conv_layer_1 = 30\n",
    "        conv_layer_2 = 64\n",
    "\n",
    "        conv_layer_3 = 128\n",
    "        conv_layer_4 = 256\n",
    "\n",
    "        self.forward_funnel_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=conv_layer_1, kernel_size=5),   # Extract useful features from the beginning\n",
    "            nn.BatchNorm2d(num_features=conv_layer_1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(conv2d_dropout),\n",
    "\n",
    "            nn.Conv2d(in_channels=conv_layer_1, out_channels=conv_layer_2, kernel_size=3),  # Extract useful features from the learned features\n",
    "            nn.BatchNorm2d(num_features=conv_layer_2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(conv2d_dropout),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                       # Reduce dimensionality\n",
    "        )\n",
    "\n",
    "        self.forward_funnel_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=conv_layer_2, out_channels=conv_layer_3, kernel_size=3),   # Extract useful features from the beginning\n",
    "            nn.BatchNorm2d(num_features=conv_layer_3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(conv2d_dropout),\n",
    "\n",
    "            nn.Conv2d(in_channels=conv_layer_3, out_channels=conv_layer_4, kernel_size=3),  # Extract useful features from the learned features\n",
    "            nn.BatchNorm2d(num_features=conv_layer_4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(conv2d_dropout),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # Compute the number of features after the input has passed the funnel\n",
    "        with torch.no_grad():\n",
    "            test_input = torch.zeros(1, 3, 32, 32)\n",
    "\n",
    "            test_input.to(device)\n",
    "\n",
    "            features = self.forward_funnel_1(test_input)\n",
    "            features = self.forward_funnel_2(features)\n",
    "\n",
    "            total_count = features.view(1, -1).size(1)\n",
    "\n",
    "        full_node_dropout = F_dropout\n",
    "\n",
    "\n",
    "        lin_layer_1_size = 1000\n",
    "        lin_layer_2_size = 500\n",
    "        lin_layer_3_size = 250\n",
    "\n",
    "\n",
    "\n",
    "        self.output_nodes = 100\n",
    "\n",
    "        self.classifer = nn.Sequential(\n",
    "            nn.Flatten(),                                           # Flatten the image from the funnel\n",
    "            nn.Linear(in_features=total_count, out_features=lin_layer_1_size),\n",
    "            nn.LayerNorm(lin_layer_1_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(full_node_dropout),\n",
    "\n",
    "            nn.Linear(in_features=lin_layer_1_size, out_features=lin_layer_2_size),\n",
    "            nn.LayerNorm(lin_layer_2_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(full_node_dropout),\n",
    "\n",
    "            nn.Linear(in_features=lin_layer_2_size, out_features=lin_layer_3_size),\n",
    "            nn.LayerNorm(lin_layer_3_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(full_node_dropout),\n",
    "\n",
    "            nn.Linear(in_features=lin_layer_3_size, out_features=self.output_nodes),\n",
    "            nn.LayerNorm(self.output_nodes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(full_node_dropout),\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(in_features=self.output_nodes, out_features=10)\n",
    "\n",
    "    def partial_forward(self, x):\n",
    "        x = self.forward_funnel_1(x)\n",
    "        x = self.forward_funnel_2(x)\n",
    "        x = self.classifer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.partial_forward(x)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        return logits"
   ],
   "id": "9eda8e70dd5aafa0",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T04:37:23.912401Z",
     "start_time": "2025-12-09T04:37:23.909207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epoch_over_training_loss_CIFAR10 = []\n",
    "epoch_over_testing_loss_CIFAR10 = []\n",
    "\n",
    "'''\n",
    "Form of the data\n",
    "\n",
    "data =\n",
    "{\n",
    "    epoch: int\n",
    "    training/testing loss: float\n",
    "}\n",
    "'''"
   ],
   "id": "d77732032a602e3d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nForm of the data\\n\\ndata = \\n{\\n    epoch: int\\n    training/testing loss: float\\n}\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T04:39:28.145754Z",
     "start_time": "2025-12-09T04:37:24.689466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameter setup\n",
    "epochs = 15\n",
    "batch_size = 64\n",
    "learning_rate = 5e-4\n",
    "decay_rate = 4e-4\n",
    "\n",
    "c_dropout = 0.25\n",
    "f_dropout = 0.25\n",
    "\n",
    "print('######## Begining training for CIFAR10 classifier ##########')\n",
    "\n",
    "# Setup data loaders\n",
    "trainset_loader_CIFAR10 = data.DataLoader(trainset_full_CIFAR10,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   # num_workers=5,\n",
    "                                   pin_memory=True)\n",
    "\n",
    "testset_loader_CIFAR10 = data.DataLoader(testset_full_CIFAR10,\n",
    "                                   batch_size=batch_size,\n",
    "                                   # num_workers=5,\n",
    "                                   shuffle=False,\n",
    "                                   pin_memory=True)\n",
    "\n",
    "model = CIFAR10_Classifier(c_dropout, f_dropout)\n",
    "model.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(),\n",
    "                       lr=learning_rate,\n",
    "                       weight_decay=decay_rate\n",
    "                       )\n",
    "\n",
    "# Have references to variables outside of the epoch loop\n",
    "avg_training_loss = 0\n",
    "avg_testing_loss = 0\n",
    "\n",
    "# Epoch Loop\n",
    "for epoch in range(epochs):\n",
    "    print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "\n",
    "    avg_training_loss = 0\n",
    "    avg_testing_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for x, Y in tqdm(trainset_loader_CIFAR10, desc='Training', unit=' batch'):\n",
    "        # Transfer images to GPU\n",
    "        x = x.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Send images to model\n",
    "        x_pred = model(x)\n",
    "\n",
    "        # Calc loss\n",
    "        loss = loss_function(x_pred, Y)\n",
    "\n",
    "        # Calc gradient and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            avg_training_loss += loss.item()\n",
    "\n",
    "    # Switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, Y in tqdm(testset_loader_CIFAR10, desc='Testing', unit=' batches'):\n",
    "            # Move the images to the GPU\n",
    "            x = x.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            # Get logits and sum up total loss\n",
    "            x_pred = model(x)\n",
    "            avg_testing_loss += loss_function(x_pred, Y).item()\n",
    "\n",
    "    # Get training loss\n",
    "    avg_training_loss /= len(trainset_loader_CIFAR10)\n",
    "\n",
    "     # Get testing loss\n",
    "    avg_testing_loss /= len(testset_loader_CIFAR10)\n",
    "\n",
    "    # Switch model back to training mode\n",
    "    model.train()\n",
    "\n",
    "    epoch_over_training_loss_CIFAR10.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"training_loss\": avg_training_loss\n",
    "        })\n",
    "\n",
    "    epoch_over_testing_loss_CIFAR10.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"testing_loss\": avg_testing_loss\n",
    "        })\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    print(f'   -> Training Loss: {avg_training_loss: .4f}\\n')\n",
    "    print(f'   -> Testing Loss: {avg_testing_loss: .4f}\\n')\n"
   ],
   "id": "88b8d83432a78c99",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Begining training for CIFAR10 classifier ##########\n",
      "----- Epoch: 1/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:07<00:00, 106.90 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 151.55 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.9341\n",
      "\n",
      "   -> Testing Loss:  1.5643\n",
      "\n",
      "----- Epoch: 2/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:07<00:00, 108.34 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 152.62 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.6016\n",
      "\n",
      "   -> Testing Loss:  1.4643\n",
      "\n",
      "----- Epoch: 3/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:07<00:00, 106.98 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 151.72 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.4427\n",
      "\n",
      "   -> Testing Loss:  1.2137\n",
      "\n",
      "----- Epoch: 4/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:07<00:00, 107.35 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:00<00:00, 158.85 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.3388\n",
      "\n",
      "   -> Testing Loss:  1.1131\n",
      "\n",
      "----- Epoch: 5/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:07<00:00, 108.73 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 153.85 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.2517\n",
      "\n",
      "   -> Testing Loss:  1.0442\n",
      "\n",
      "----- Epoch: 6/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:07<00:00, 109.14 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:00<00:00, 160.72 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.1807\n",
      "\n",
      "   -> Testing Loss:  0.9841\n",
      "\n",
      "----- Epoch: 7/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:07<00:00, 108.53 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 156.33 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.1149\n",
      "\n",
      "   -> Testing Loss:  0.9577\n",
      "\n",
      "----- Epoch: 8/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:07<00:00, 109.14 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:00<00:00, 157.78 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0459\n",
      "\n",
      "   -> Testing Loss:  0.8625\n",
      "\n",
      "----- Epoch: 9/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:07<00:00, 107.74 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:00<00:00, 159.76 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0025\n",
      "\n",
      "   -> Testing Loss:  0.8238\n",
      "\n",
      "----- Epoch: 10/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:07<00:00, 108.34 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 156.21 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9518\n",
      "\n",
      "   -> Testing Loss:  0.8015\n",
      "\n",
      "----- Epoch: 11/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:07<00:00, 108.79 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 153.66 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9113\n",
      "\n",
      "   -> Testing Loss:  0.7908\n",
      "\n",
      "----- Epoch: 12/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:07<00:00, 108.60 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:00<00:00, 159.46 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.8819\n",
      "\n",
      "   -> Testing Loss:  0.7434\n",
      "\n",
      "----- Epoch: 13/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:07<00:00, 108.99 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 155.45 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.8407\n",
      "\n",
      "   -> Testing Loss:  0.7338\n",
      "\n",
      "----- Epoch: 14/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:07<00:00, 108.47 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:00<00:00, 160.60 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.8101\n",
      "\n",
      "   -> Testing Loss:  0.7141\n",
      "\n",
      "----- Epoch: 15/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:07<00:00, 108.65 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:00<00:00, 159.55 batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.7777\n",
      "\n",
      "   -> Testing Loss:  0.7039\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T04:46:42.444001Z",
     "start_time": "2025-12-09T04:46:42.440889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CIFAR10_Transformed_Classifier(CIFAR10_Classifier):\n",
    "    def __init__(self, C_dropout, F_dropout):\n",
    "        super().__init__(C_dropout, F_dropout)\n",
    "\n",
    "        self.output_layer = nn.Linear(in_features=self.output_nodes, out_features=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.partial_forward(x)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        return logits"
   ],
   "id": "8dac7b53ccced4eb",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T04:46:42.635867Z",
     "start_time": "2025-12-09T04:46:42.633347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "epoch_over_training_loss_CIFAR10_R = []\n",
    "epoch_over_testing_loss_CIFAR10_R = []"
   ],
   "id": "eaa5b68487e45af7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T04:53:49.783253Z",
     "start_time": "2025-12-09T04:46:43.006985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameter setup\n",
    "epochs = 50\n",
    "batch_size = 50\n",
    "learning_rate = 5e-4\n",
    "decay_rate = 1e-3\n",
    "\n",
    "c_dropout = 0.30\n",
    "f_dropout = 0.30\n",
    "\n",
    "\n",
    "print('######## Begining training for CIFAR10 classifier on rotated images ##########')\n",
    "\n",
    "# Setup data loaders\n",
    "trainset_loader_CIFAR10_R = data.DataLoader(trainset_full_CIFAR10,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   # num_workers=5,\n",
    "                                   pin_memory=True)\n",
    "\n",
    "testset_loader_CIFAR10_R = data.DataLoader(testset_full_CIFAR10,\n",
    "                                   batch_size=batch_size,\n",
    "                                   # num_workers=5,\n",
    "                                   shuffle=False,\n",
    "                                   pin_memory=True)\n",
    "\n",
    "model = CIFAR10_Transformed_Classifier(c_dropout, f_dropout)\n",
    "model.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(),\n",
    "                       lr=learning_rate,\n",
    "                       weight_decay=decay_rate\n",
    "                       )\n",
    "\n",
    "# Have references to variables outside of the epoch loop\n",
    "avg_training_loss = 0\n",
    "avg_testing_loss = 0\n",
    "\n",
    "# Epoch Loop\n",
    "for epoch in range(epochs):\n",
    "    print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "\n",
    "    avg_training_loss = 0\n",
    "    avg_testing_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for x, _ in tqdm(trainset_loader_CIFAR10_R, desc='Training', unit=' batch'):\n",
    "\n",
    "        labels_upright = torch.zeros(x.size(0), dtype=torch.long)\n",
    "\n",
    "        images_rotated = torch.rot90(x, 1, [2, 3])\n",
    "        labels_rotated = torch.ones(x.size(0), dtype=torch.long)\n",
    "\n",
    "        all_images = torch.cat([x, images_rotated])\n",
    "        all_labels = torch.cat([labels_upright, labels_rotated])\n",
    "\n",
    "\n",
    "        # Transfer images to GPU\n",
    "        all_images = all_images.to(device)\n",
    "        all_labels = all_labels.to(device)\n",
    "\n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Send images to model\n",
    "        x_pred = model(all_images)\n",
    "\n",
    "        # Calc loss\n",
    "        loss = loss_function(x_pred, all_labels)\n",
    "\n",
    "        # Calc gradient and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            avg_training_loss += loss.item()\n",
    "\n",
    "    # Switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in tqdm(testset_loader_CIFAR10_R, desc='Testing', unit=' batches'):\n",
    "\n",
    "            labels_upright = torch.zeros(x.size(0), dtype=torch.long)\n",
    "\n",
    "            images_rotated = torch.rot90(x, 1, [2, 3])\n",
    "            labels_rotated = torch.ones(x.size(0), dtype=torch.long)\n",
    "\n",
    "            all_images = torch.cat([x, images_rotated])\n",
    "            all_labels = torch.cat([labels_upright, labels_rotated])\n",
    "\n",
    "            # Transfer images to GPU\n",
    "            all_images = all_images.to(device)\n",
    "            all_labels = all_labels.to(device)\n",
    "\n",
    "            # Move the images to the GPU\n",
    "            all_images = all_images.to(device)\n",
    "            all_labels = all_labels.to(device)\n",
    "\n",
    "            # Get logits and sum up total loss\n",
    "            x_pred = model(all_images)\n",
    "            avg_testing_loss += loss_function(x_pred, all_labels).item()\n",
    "\n",
    "    # Get training loss\n",
    "    avg_training_loss /= len(trainset_loader_CIFAR10_R)\n",
    "\n",
    "     # Get testing loss\n",
    "    avg_testing_loss /= len(testset_loader_CIFAR10_R)\n",
    "\n",
    "    # Switch model back to training mode\n",
    "    model.train()\n",
    "\n",
    "    epoch_over_training_loss_CIFAR10_R.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"training_loss\": avg_training_loss\n",
    "        })\n",
    "\n",
    "    epoch_over_testing_loss_CIFAR10_R.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"testing_loss\": avg_testing_loss\n",
    "        })\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    print(f'   -> Training Loss: {avg_training_loss: .4f}\\n')\n",
    "    print(f'   -> Testing Loss: {avg_testing_loss: .4f}\\n')"
   ],
   "id": "4fed0f69c11fe1f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Begining training for CIFAR10 classifier on rotated images ##########\n",
      "----- Epoch: 1/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.69 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 185.14 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.5700\n",
      "\n",
      "   -> Testing Loss:  0.4855\n",
      "\n",
      "----- Epoch: 2/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.65 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 179.68 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4979\n",
      "\n",
      "   -> Testing Loss:  0.4475\n",
      "\n",
      "----- Epoch: 3/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.60 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 183.99 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4669\n",
      "\n",
      "   -> Testing Loss:  0.4248\n",
      "\n",
      "----- Epoch: 4/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.14 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 181.22 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4467\n",
      "\n",
      "   -> Testing Loss:  0.4051\n",
      "\n",
      "----- Epoch: 5/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.92 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 187.75 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4296\n",
      "\n",
      "   -> Testing Loss:  0.3868\n",
      "\n",
      "----- Epoch: 6/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.54 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 185.62 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4154\n",
      "\n",
      "   -> Testing Loss:  0.3709\n",
      "\n",
      "----- Epoch: 7/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.38 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 181.68 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4052\n",
      "\n",
      "   -> Testing Loss:  0.3579\n",
      "\n",
      "----- Epoch: 8/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.73 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 187.03 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3954\n",
      "\n",
      "   -> Testing Loss:  0.3492\n",
      "\n",
      "----- Epoch: 9/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.94 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 179.70 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3865\n",
      "\n",
      "   -> Testing Loss:  0.3425\n",
      "\n",
      "----- Epoch: 10/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.47 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 178.19 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3783\n",
      "\n",
      "   -> Testing Loss:  0.3343\n",
      "\n",
      "----- Epoch: 11/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.36 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 182.76 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3704\n",
      "\n",
      "   -> Testing Loss:  0.3316\n",
      "\n",
      "----- Epoch: 12/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.02 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 181.74 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3636\n",
      "\n",
      "   -> Testing Loss:  0.3223\n",
      "\n",
      "----- Epoch: 13/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.42 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 181.71 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3545\n",
      "\n",
      "   -> Testing Loss:  0.3137\n",
      "\n",
      "----- Epoch: 14/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.22 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 184.71 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3496\n",
      "\n",
      "   -> Testing Loss:  0.3092\n",
      "\n",
      "----- Epoch: 15/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.07 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 176.27 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3422\n",
      "\n",
      "   -> Testing Loss:  0.3015\n",
      "\n",
      "----- Epoch: 16/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.37 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 184.08 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3367\n",
      "\n",
      "   -> Testing Loss:  0.3002\n",
      "\n",
      "----- Epoch: 17/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.87 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 183.62 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3321\n",
      "\n",
      "   -> Testing Loss:  0.2996\n",
      "\n",
      "----- Epoch: 18/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.10 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 184.50 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3263\n",
      "\n",
      "   -> Testing Loss:  0.2980\n",
      "\n",
      "----- Epoch: 19/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.64 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 187.14 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3217\n",
      "\n",
      "   -> Testing Loss:  0.2945\n",
      "\n",
      "----- Epoch: 20/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.73 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 185.94 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3159\n",
      "\n",
      "   -> Testing Loss:  0.2911\n",
      "\n",
      "----- Epoch: 21/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.75 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 184.50 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3103\n",
      "\n",
      "   -> Testing Loss:  0.2841\n",
      "\n",
      "----- Epoch: 22/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.45 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 183.15 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3036\n",
      "\n",
      "   -> Testing Loss:  0.2817\n",
      "\n",
      "----- Epoch: 23/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.92 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 187.48 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3003\n",
      "\n",
      "   -> Testing Loss:  0.2785\n",
      "\n",
      "----- Epoch: 24/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.28 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 184.96 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2952\n",
      "\n",
      "   -> Testing Loss:  0.2756\n",
      "\n",
      "----- Epoch: 25/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.81 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 183.68 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2920\n",
      "\n",
      "   -> Testing Loss:  0.2713\n",
      "\n",
      "----- Epoch: 26/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.41 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 182.79 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2856\n",
      "\n",
      "   -> Testing Loss:  0.2715\n",
      "\n",
      "----- Epoch: 27/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.78 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 182.94 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2808\n",
      "\n",
      "   -> Testing Loss:  0.2746\n",
      "\n",
      "----- Epoch: 28/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 116.78 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 186.28 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2768\n",
      "\n",
      "   -> Testing Loss:  0.2686\n",
      "\n",
      "----- Epoch: 29/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.98 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 186.38 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2724\n",
      "\n",
      "   -> Testing Loss:  0.2669\n",
      "\n",
      "----- Epoch: 30/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.88 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 185.27 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2672\n",
      "\n",
      "   -> Testing Loss:  0.2680\n",
      "\n",
      "----- Epoch: 31/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.60 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 185.84 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2647\n",
      "\n",
      "   -> Testing Loss:  0.2714\n",
      "\n",
      "----- Epoch: 32/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.47 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 185.02 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2591\n",
      "\n",
      "   -> Testing Loss:  0.2638\n",
      "\n",
      "----- Epoch: 33/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 118.37 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 184.57 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2549\n",
      "\n",
      "   -> Testing Loss:  0.2656\n",
      "\n",
      "----- Epoch: 34/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 118.50 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 185.56 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2530\n",
      "\n",
      "   -> Testing Loss:  0.2668\n",
      "\n",
      "----- Epoch: 35/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 118.09 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 178.38 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2491\n",
      "\n",
      "   -> Testing Loss:  0.2620\n",
      "\n",
      "----- Epoch: 36/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 118.19 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 183.59 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2441\n",
      "\n",
      "   -> Testing Loss:  0.2602\n",
      "\n",
      "----- Epoch: 37/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.75 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 184.87 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2404\n",
      "\n",
      "   -> Testing Loss:  0.2618\n",
      "\n",
      "----- Epoch: 38/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.64 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 184.62 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2370\n",
      "\n",
      "   -> Testing Loss:  0.2617\n",
      "\n",
      "----- Epoch: 39/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.79 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 188.04 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2319\n",
      "\n",
      "   -> Testing Loss:  0.2619\n",
      "\n",
      "----- Epoch: 40/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.88 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 186.43 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2294\n",
      "\n",
      "   -> Testing Loss:  0.2684\n",
      "\n",
      "----- Epoch: 41/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 118.69 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 183.41 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2240\n",
      "\n",
      "   -> Testing Loss:  0.2695\n",
      "\n",
      "----- Epoch: 42/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 118.40 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 187.93 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2215\n",
      "\n",
      "   -> Testing Loss:  0.2660\n",
      "\n",
      "----- Epoch: 43/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.63 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 187.33 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2182\n",
      "\n",
      "   -> Testing Loss:  0.2616\n",
      "\n",
      "----- Epoch: 44/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:08<00:00, 117.20 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 180.27 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2124\n",
      "\n",
      "   -> Testing Loss:  0.2649\n",
      "\n",
      "----- Epoch: 45/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 373/1000 [00:03<00:05, 116.54 batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 73\u001B[0m\n\u001B[1;32m     70\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_function(x_pred, all_labels)\n\u001B[1;32m     72\u001B[0m \u001B[38;5;66;03m# Calc gradient and update weights\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     76\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n",
      "File \u001B[0;32m~/Projects/Intro-to-Deep-Learning-Projects/Project 4/.venv/lib/python3.10/site-packages/torch/_tensor.py:625\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    615\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    616\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    617\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    618\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    623\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    624\u001B[0m     )\n\u001B[0;32m--> 625\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    626\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    627\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/Intro-to-Deep-Learning-Projects/Project 4/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    349\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    351\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    353\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 354\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    355\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    356\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    357\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    358\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    359\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    360\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    362\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/Intro-to-Deep-Learning-Projects/Project 4/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:841\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[0;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[1;32m    839\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[1;32m    840\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 841\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    842\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    843\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[1;32m    844\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    845\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Save the model weights from problem 5\n",
    "torch.save(model.state_dict(), 'HW4_Rotation_Model_Weights.pth')"
   ],
   "id": "456a8aea45a3281",
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T04:35:13.132433Z",
     "start_time": "2025-12-09T04:35:13.127978Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def transfer_weights(model_final, model_src, k, is_frozen):\n",
    "\n",
    "    for i in range(len(model_final.forward_funnel_1)):\n",
    "\n",
    "        if k == 0: return\n",
    "\n",
    "        src_layer = model_src.forward_funnel_1[i]\n",
    "        final_layer = model_final.forward_funnel_1[i]\n",
    "\n",
    "        if (hasattr(src_layer, 'weight') and hasattr(final_layer, 'weight')):\n",
    "            final_layer.weight.data = src_layer.weight.data.clone()\n",
    "            if is_frozen:\n",
    "                final_layer.weight.requires_grad = False\n",
    "\n",
    "            # This will always run if we get here. I do not intend on making layers with biases\n",
    "            if (hasattr(src_layer, 'bias') and hasattr(final_layer, 'bias')):\n",
    "                final_layer.bias.data = src_layer.bias.data.clone()\n",
    "                if is_frozen:\n",
    "                    final_layer.bias.requires_grad = False\n",
    "\n",
    "                k -= 1\n",
    "\n",
    "\n",
    "    for i in range(len(model_final.classifer)):\n",
    "\n",
    "        if k == 0: return\n",
    "\n",
    "        src_layer = model_src.classifer[i]\n",
    "        final_layer = model_final.classifer[i]\n",
    "\n",
    "        if (hasattr(src_layer, 'weight') and hasattr(final_layer, 'weight')):\n",
    "            final_layer.weight.data = src_layer.weight.data.clone()\n",
    "            if is_frozen:\n",
    "                final_layer.weight.requires_grad = False\n",
    "\n",
    "            # This will always run if we get here. I do not intend on making layers with biases\n",
    "            if (hasattr(src_layer, 'bias') and hasattr(final_layer, 'bias')):\n",
    "                final_layer.bias.data = src_layer.bias.data.clone()\n",
    "                if is_frozen:\n",
    "                    final_layer.bias.requires_grad = False\n",
    "\n",
    "                k -= 1"
   ],
   "id": "a560e7788ae2baf5",
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "source": "",
   "id": "d0d978d9df97f047",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
