{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d36974d",
   "metadata": {},
   "source": [
    "# Architecture and Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16e49b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for this project\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53853ba1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Tuple' from 'types' (C:\\Python313\\Lib\\types.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m torch.cuda.is_available()\n\u001b[32m      2\u001b[39m torch.cuda.get_device_name(\u001b[32m0\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tuple\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'Tuple' from 'types' (C:\\Python313\\Lib\\types.py)"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14f38af",
   "metadata": {},
   "source": [
    "## 1. The Basic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90214f0",
   "metadata": {},
   "source": [
    "#### The model has 28 ∗ 28 = 784 input features, and 10 output features. How many parameters does it have, in total?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87232b90",
   "metadata": {},
   "source": [
    "7850 parameters.\n",
    "\n",
    "In total, there is an individual parameter for each input feature so, 784, but we need to consider each class, 10 in total, so it would be 784 * 10 = 7840 parameters. However, each class has an associated bias, so the total would be 7840 + 10 = 7850 parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197af201",
   "metadata": {},
   "source": [
    "##### Train a simple linear softmax model, and try to minimize its loss without overfitting. What is the minimal loss you achieve on the training set, and corresponding loss on the testing set? Accuracy on the training set, and corresponding accuracy on the testing set? Explain the choices you made, including step size, and how you knew that training further was not going to be worthwhile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3803d897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the MNIST dataset\n",
    "training_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testing_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "160f4a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin data preprocessing\n",
    "\n",
    "def data_prepocessor(dataset: torch.utils.data.Dataset):\n",
    "    \"\"\" \n",
    "    Used to prepare the data for processing by normalizing the data and flattening the images.\n",
    "\n",
    "    Args:\n",
    "        dataset (torch.utils.data.Dataset): The dataset to preprocess.\n",
    "    \n",
    "    Returns:\n",
    "        return_tensors Tuple[torch.Tensor, torch.Tensor]: A tuple containing the preprocessed data and the corresponding\n",
    "    \"\"\"\n",
    "    \n",
    "    new_data = (dataset.data / 255.0) - 0.5\n",
    "    flattened_img_data = new_data.view(new_data.shape[0], -1)\n",
    "    targets = dataset.targets\n",
    "\n",
    "    return flattened_img_data, targets\n",
    "\n",
    "x_train, y_train = data_prepocessor(training_set)\n",
    "x_test, y_test = data_prepocessor(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471760b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Establish a NN class for multi-class classification of MNIST dataset. \n",
    "\"\"\"\n",
    "\n",
    "class NumberClassifierNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple neural network for classifying MNIST digits.\n",
    "    \n",
    "    Emulates logistic regression by using a single linear layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(784, 10)  # Input layer representing all 784 input features mapped to 10 output classes\n",
    "\n",
    "        # No activation function needed for logits output\n",
    "        self.activation_function = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the neural network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, 784)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, 10) representing class scores for each digit (0-9)\n",
    "        \"\"\"\n",
    "\n",
    "        logits = self.layer1(x)\n",
    "        logits = self.activation_function(logits)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a0e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training block\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = NumberClassifierNN()\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.20)\n",
    "\n",
    "# Convert training data to TensorDataset for DataLoader\n",
    "training_dataset = data.TensorDataset(x_train, y_train)\n",
    "\n",
    "batch_size = len(training_dataset)\n",
    "\n",
    "# DataLoader for batching\n",
    "data_loader = data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    # Run through the data. Will be one big batch for now\n",
    "    for inputs, labels in data_loader:\n",
    "        # Forward pass + computing the loss of the batch\n",
    "        logits = model(inputs)\n",
    "        loss = loss_function(logits, labels)\n",
    "\n",
    "        # Clear the gradient from previous step, and perform gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # Update the gradients\n",
    "        optimizer.step() # Update the weights\n",
    "\n",
    "        total_loss += loss.item() # Accumulate the loss, though there will only be one batch\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f3faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip mode of model\n",
    "model.eval()\n",
    "\n",
    "# Find accuracy and loss on training set\n",
    "with torch.no_grad():\n",
    "    train_logits = model(x_train)\n",
    "    train_loss = loss_function(train_logits, y_train).item()\n",
    "    train_pred = torch.argmax(train_logits, dim=1)\n",
    "    train_acc = (train_pred == y_train).float().mean().item()\n",
    "\n",
    "# Find accuracy and loss on testing set\n",
    "with torch.no_grad():\n",
    "    test_logits = model(x_test)\n",
    "    test_loss = loss_function(test_logits, y_test).item()\n",
    "    test_pred = torch.argmax(test_logits, dim=1)\n",
    "    test_acc = (test_pred == y_test).float().mean().item()\n",
    "\n",
    "print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc * 100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283c0dca",
   "metadata": {},
   "source": [
    "Analysis of Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c2077b",
   "metadata": {},
   "source": [
    "Results\n",
    "\n",
    "After training my simple linear softmax model, I was able to achieve a minimal loss of 0.4321 and 0.4120 on the training data and testing data respectively. As such, my model has an accuracy of 88.55% and 89.17% on the training and testing data respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672a8a21",
   "metadata": {},
   "source": [
    "Decisions Made\n",
    "\n",
    "As I began training the model, I first chose a learning rate of 0.1 as I thought it would be safe to see how fast the model would minimize the loss over several epochs, 10 epochs for the first run. For this run, I saw that the model was learning at a steady rate but did not reach its full potential, around 1.58 total loss, due to the number of epochs I set. As such, I kept the rate the same and increased the epochs to 25. \n",
    "\n",
    "During the second run, I was able to get a loss of 1.083, but I noticed that the rate of loss improvement was not decreasing, so I tried experiementing with a different learning rate. \n",
    "\n",
    "For the third run, I tried a rate of 0.2 and had 25 epochs. This time, I noticed that the model was able to get a loss of 0.8568 which was better than the previous run but still could be improved upon.\n",
    "\n",
    "For the fourth run, I tried the same learning rate of 0.2 but increased the number of epochs to 50 and got a loss of 0.6082. Though this was the best run I got, I decided to try a greater learning rate.\n",
    "\n",
    "For the fifth run, I used a learning rate of 0.25 and 50 epochs and had a loss of 0.6081. Again, this was better than the previous attempt, but I noticed that every other epoch would cause the loss to jump up then decrease for the next epoch after it. As such, I determined that a learning rate greater than this wouldn't be a good idea. As a result, I tried experimenting with learing rates between 0.2 - 0.25 and saw that a learning rate of 0.2 was optimal since it never showed that rebounding behaviour. \n",
    "\n",
    "For my next significant attempt, I wanted to increase the number of epochs to see how low the loss could go, so I used a learning rate of 0.2 and 100 epochs. I was able to obtain a loss of 0.4786 but wanted to see how far I could train the model before I overfitted. So, I began testing greater and greater epochs and used the testing data to verify that I was doing good.\n",
    "\n",
    "After this process, I was finally able to land on a loss of 0.4328 by using a learning rate of 0.2 and 150 epochs. Anything above 150 epochs would keep the loss at a range between 0.43 - 0.44."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef42effa",
   "metadata": {},
   "source": [
    "##### Does the end result change if you start with a different initialization for your parameters? And if so, in what way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f53a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training block\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = NumberClassifierNN()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.20)\n",
    "\n",
    "# Convert training data to TensorDataset for DataLoader\n",
    "training_dataset = data.TensorDataset(x_train, y_train)\n",
    "\n",
    "batch_size = len(training_dataset)\n",
    "\n",
    "# DataLoader for batching\n",
    "data_loader = data.DataLoader(training_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "# # Set initial values to zero for weights and biases\n",
    "# nn.init.constant_(model.layer1.weight, 1000.0)\n",
    "# nn.init.constant_(model.layer1.bias, 10000.0)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    # Run through the data. Will be one big batch for now\n",
    "    for inputs, labels in data_loader:\n",
    "        # Forward pass + computing the loss of the batch\n",
    "        logits = model(inputs)\n",
    "        loss = loss_function(logits, labels)\n",
    "\n",
    "        # Clear the gradient from previous step, and perform gradient descent\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # Update the gradients\n",
    "        optimizer.step() # Update the weights\n",
    "\n",
    "        total_loss += loss.item() # Accumulate the loss, though there will only be one batch\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20e648b",
   "metadata": {},
   "source": [
    "## 2. A Fixed-Size Layer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c394708",
   "metadata": {},
   "source": [
    "#### In this section, I want to consider a model with hidden layers. In particular, assume that the network has k ≥ 1 hidden layers, and each layer has m ≥ 1 nodes. Notice, I’m fixing each hidden layer to be the same size (for simplicity’s sake). We can assume a tanh activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2f94b1",
   "metadata": {},
   "source": [
    "##### Find a formula for Parameters(k, m), the number of trainable parameters in a network of this shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aafab6e",
   "metadata": {},
   "source": [
    "The first layer has 784 nodes, number of pixels in each photo, and each node in the first layer connects to each node in the second layer with m nodes. As such, each individual node has Z parameters where Z is...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46731d59",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "$Z = (\\sum_{i=1}^{784}{1}) + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b063fba",
   "metadata": {},
   "source": [
    "The summation represents the number of weights attached to an individual node from the input layer and the single 1 represents the bias for the current node in the first hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635d81ed",
   "metadata": {},
   "source": [
    "Since this second layer has m nodes, we can generalize the formula for the total number of parameters in the second layer to..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4b1116",
   "metadata": {},
   "source": [
    "$Z * m = m * ((\\sum_{i=1}^{784}{1}) + 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a96c039",
   "metadata": {},
   "source": [
    "If the model has more than one hidden layer, then we know that the third layer also has m nodes. Each node in this third layer will have Q parameters where Q is..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad984d9",
   "metadata": {},
   "source": [
    "$Q = (\\sum_{i=1}^{m}1) + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a22f006",
   "metadata": {},
   "source": [
    "Furthermore, the entire third layer would have... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5076ee",
   "metadata": {},
   "source": [
    "$Q * m = m * ((\\sum_{i=1}^{m}1) + 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae5b7ab",
   "metadata": {},
   "source": [
    "For any hidden layer beyond the first hidden layer, we can make a general formula for the number of parameters represented with S..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75ec1f4",
   "metadata": {},
   "source": [
    "$S = (k - 1) * (Q * m) = (k - 1) * (m * ((\\sum_{i=1}^{m}1) + 1))$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9491af",
   "metadata": {},
   "source": [
    "We do k - 1 instead of just k since parameters are connected between nodes rather than being just the individual layer of nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e387c9f",
   "metadata": {},
   "source": [
    "When the hidden layer is connected to the output layer, we know that there are 10 output nodes. As such, the number of parameters for one node in the output layer would be V..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5399cd",
   "metadata": {},
   "source": [
    "$V = Q = (\\sum_{i=1}^{m}1) + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956863ec",
   "metadata": {},
   "source": [
    "Which can be generalized for the entire output layer..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d73f311",
   "metadata": {},
   "source": [
    "$V * 10 = 10 * ((\\sum_{i=1}^{m}1) + 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d198742",
   "metadata": {},
   "source": [
    "For the entire NN, we can say that the number of parameters it has is"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030bce62",
   "metadata": {},
   "source": [
    "$Parameters(k, m) = (Z * m) + (k - 1)(Q * m) + (V * 10)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d6049",
   "metadata": {},
   "source": [
    "$= m * ((\\sum_{i=1}^{784}{1}) + 1) + (k - 1) * (m * ((\\sum_{i=1}^{m}1) + 1)) +  10 * ((\\sum_{i=1}^{m}1) + 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5026d5",
   "metadata": {},
   "source": [
    "$= 785m + (k-1)(m^2 + m) + 10m + 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d313de15",
   "metadata": {},
   "source": [
    "$= 795m + km^2 + km -m^2 - m + 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c1f220",
   "metadata": {},
   "source": [
    "$= km^2 - m^2 + 794m + km + 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37175db5",
   "metadata": {},
   "source": [
    "$= (k - 1)m^2 + (794 + k)m + 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f10779",
   "metadata": {},
   "source": [
    "##### For a fixed number of parameters P , what are the smallest and largest values k can have such that Parameters(k, m) = P ? Note, I am essentially asking the smallest and largest number of layers a network like this could have, with a fixed number of parameters. Let $k_P$ be this max number of layers. Note: What should you do if $k_P$ is not an integer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff4d682",
   "metadata": {},
   "source": [
    "Lets start by finding the smallest value for k. Since k is just the number of layers for this model, k can just be of size 1. This means that..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3976b67b",
   "metadata": {},
   "source": [
    "$P = (k - 1)m^2 + (794 + k)m + 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff615b8",
   "metadata": {},
   "source": [
    "$P = (1 - 1)m^2 + (794 + 1)m + 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c2a3e",
   "metadata": {},
   "source": [
    "$P = 795m + 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4afe42",
   "metadata": {},
   "source": [
    "As such, the minimum size of k is just 1 and depends on your choice of P."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd39bc3",
   "metadata": {},
   "source": [
    "To find $k_p$, we can use the same equation from above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9adb759",
   "metadata": {},
   "source": [
    "$P = (k - 1)m^2 + (794 + k)m + 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841e0b15",
   "metadata": {},
   "source": [
    "Given in the problem statement, we know that P is some constant. Looking at the equation, we can see that if I were to increase m by some factor, k would need to decrease in order to keep P constant. Using this logic, we can see that to get the largest possible k, it would be best to let m = 1. So..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91c151d",
   "metadata": {},
   "source": [
    "$P = k - 1 + 794 + k + 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e5257",
   "metadata": {},
   "source": [
    "$P = 2k + 803$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4922553",
   "metadata": {},
   "source": [
    "$k = \\frac{P - 803}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a560b72",
   "metadata": {},
   "source": [
    "$k_p = \\frac{P - 803}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a124b1d9",
   "metadata": {},
   "source": [
    "Now, we know that $k_p$ is dependent on the number of parameters. In this case, $k_p$ is found with a neural net of m = 1 and some specificied constant number of parameters, P."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4727a7c5",
   "metadata": {},
   "source": [
    "If $k_p$ ever ends up being not an integer, then it would be best to floor $k_p$ since if we ceil it, we would end up trying to build an additional layer that is missing the full m nodes required for it. If you floor it, you will be able to create a neural net with a number of layers that has m nodes per layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48171f18",
   "metadata": {},
   "source": [
    "##### For a given number of parameters P , let $m_{P(k)}$ be the number of nodes per layer that make Parameters(k, m) as close to P as possible. So a network with k layers and $m_{P(k)}$ nodes per layer should have approximately P total parameters.\n",
    "\n",
    "\n",
    "##### For a given P value, we can plot a graph of ‘final training loss’ for networks of shape (k, $m_{P(k)}$) as a function of k. For (at least) three sufficiently large values of P (so that the networks are non-trivial) that are sufficiently distinct (so that you don’t accidentally create the same network shapes for two different P values), plot the three ’final training loss’ curves on the same axis. What trends to you observe, and how do these compare to the baseline model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88d96a7",
   "metadata": {},
   "source": [
    "For this problem, when we are trying to find $m_p(k)$, we can treat P and k as constant since both will be known numbers when we are graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f783bfff",
   "metadata": {},
   "source": [
    "Using the solved number of parameters formula..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79cda92",
   "metadata": {},
   "source": [
    "$P = (k - 1)m^2 + (794 + k)m + 10$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76358890",
   "metadata": {},
   "source": [
    "$(k - 1)m^2 + (794 + k)m + (10 - P) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53927c6",
   "metadata": {},
   "source": [
    "We can now use the Quadratic formula to find $m_{P(k)}$..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615adbbe",
   "metadata": {},
   "source": [
    "$m_{P(k)} = \\frac{-(794 + k) \\pm \\sqrt{(794 + k)^2 - 4(k - 1)(10 - P)}}{2(k - 1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6a93c1",
   "metadata": {},
   "source": [
    "Now, we can move onto defining our NN class and training/graphing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_NN(nn.Module):\n",
    "    \"\"\"\n",
    "    A full feed-forward neural network with k hidden layers of size m.\n",
    "    \"\"\"\n",
    "    def __init__(self, k: int, m: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        # Input layer\n",
    "        layers.append(nn.Linear(784, m))\n",
    "        layers.append(nn.Tanh())\n",
    "\n",
    "        # k-1 hidden layers\n",
    "        for _ in range(k - 1):\n",
    "            layers.append(nn.Linear(m, m))\n",
    "            layers.append(nn.Tanh())\n",
    "            \n",
    "        # Encapsulate all hidden layers\n",
    "        self.hidden_layers = nn.Sequential(*layers)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(m, 10)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a877bee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea5a867c",
   "metadata": {},
   "source": [
    "## 3. Improving Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cf47ef",
   "metadata": {},
   "source": [
    "##### For a P of your choice and the optimal network shape as determined above - try to find an even better network shape (layers of unequal size, for instance) that gives better results for the same (approximate) total number of parameters. Is it better to have uniform layers? Layers of decreasing size? Increasing size? Experiment with it, and summarize your results. You may want to save the best model you find. Bonus: Does regularization (/weight decay) help?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e8eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
