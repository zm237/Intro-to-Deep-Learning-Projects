{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b3234c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for this project\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ea66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the MNIST dataset\n",
    "training_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testing_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040f5af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "The system GPU | NVIDIA GeForce RTX 4060 Laptop GPU | is AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "# Verify that GPU is connected and available\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'The system GPU | {torch.cuda.get_device_name(0) if not None else 'Error'} | is {'AVAILABLE' if torch.cuda.is_available() else 'NOT AVAILABLE' }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c0c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Auto_Encoder(nn.Module):\n",
    "    def __init__(self, bottleneck, hidden_layer_size=784):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Defined number of features for the image\n",
    "        input_size=784\n",
    "\n",
    "        # Encoder section\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=input_size, out_features=hidden_layer_size), # Input to hidden\n",
    "            nn.ReLU(inplace=True),                                             # Activation Function\n",
    "            nn.Linear(in_features=hidden_layer_size, out_features=bottleneck)  # Hidden to bottleneck\n",
    "        )\n",
    "\n",
    "        # Decoder Section\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=bottleneck, out_features=hidden_layer_size), # Bottleneck to hidden\n",
    "            nn.ReLU(inplace=True),                                             # Activation Function\n",
    "            nn.Linear(in_features=hidden_layer_size, out_features=input_size), # Hidden to output\n",
    "            nn.Sigmoid(inplace=True)                                           # Force output pixels to be between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \n",
    "        # Extract batch size and resize input into the nn\n",
    "        batch_size = x.size(0)\n",
    "        x_flattened = x.view(batch_size, -1)\n",
    "\n",
    "        # Encode then decode the input\n",
    "        x_encoded = self.encoder(x_flattened)\n",
    "        x_decoded = self.decoder(x_encoded)\n",
    "\n",
    "        # Reshape decoded x for final result\n",
    "        x_predicted = x_decoded.view(batch_size, 1, 28, 28)\n",
    "        \n",
    "        return x_predicted\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63aac2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data for this test\n",
    "k_over_training_loss = []\n",
    "k_over_testing_loss = [] \n",
    "\n",
    "\n",
    "epoch_over_training_loss = []\n",
    "epoch_over_testing_loss = []\n",
    "\n",
    "'''\n",
    "Data for the graph will be in the form of\n",
    "{\n",
    "    k: int\n",
    "    final_training/testing_loss: float\n",
    "}\n",
    "'''\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144822a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Question specific hyperparameters\n",
    "hidden_layer_size = 1000\n",
    "k_start = 748\n",
    "k_end = 0\n",
    "k_step = 160\n",
    "\n",
    "# Loop through each bottlenecks\n",
    "for k in range(k_start, k_end, k_step):\n",
    "    \n",
    "    print(f'########## With bottleneck = {k} ############')\n",
    "    \n",
    "    train_loader = data.DataLoader(training_set, \n",
    "                                    batch_size=batch_size, \n",
    "                                    shuffle=True,\n",
    "                                    num_workers=5, \n",
    "                                    pin_memory=True\n",
    "                               )\n",
    "    \n",
    "    test_loader = data.DataLoader(testing_set, \n",
    "                                    batch_size=batch_size, \n",
    "                                    shuffle=False,\n",
    "                                    num_workers=5, \n",
    "                                    pin_memory=True\n",
    "                               )\n",
    "\n",
    "    model = MNIST_Auto_Encoder(k, hidden_layer_size=hidden_layer_size)\n",
    "    model.to(device)\n",
    "\n",
    "    total_params, trainable_params = count_parameters(model)\n",
    "\n",
    "    print(f' ~~ Total/Trainable Parameters: {total_params}')\n",
    "\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Have references to variables outside of the epoch loop\n",
    "    avg_training_loss = 0\n",
    "    avg_testing_loss = 0\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    # Epoch Loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "\n",
    "        # Per epoch reset accumulated loss\n",
    "        avg_training_loss = 0\n",
    "        avg_testing_loss = 0\n",
    "\n",
    "        # Run through each batch\n",
    "        for images, _ in tqdm(train_loader, desc='Training', unit=' batch'):\n",
    "            # Move data into the GPU\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Zero out gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calc loss\n",
    "            generated_images = model(images)\n",
    "            loss = loss_function(generated_images, images)\n",
    "\n",
    "            # Calc gradient and step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                avg_training_loss += loss.item()\n",
    "\n",
    "        # Set up for calculating testing loss\n",
    "        avg_training_loss /=  len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Calc training loss\n",
    "        with torch.no_grad():\n",
    "            for test_images, _ in tqdm(test_loader, desc='Testing', unit=' batch'):\n",
    "                test_images = test_images.to(device)\n",
    "                generated_test_images = model(test_images)\n",
    "                \n",
    "                avg_testing_loss += loss_function(generated_test_images, test_images).item()\n",
    "\n",
    "        avg_testing_loss /=  len(test_loader)\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        print(\"\")\n",
    "\n",
    "        print(f'   -> Training Loss: {avg_training_loss: .4f}\\n')\n",
    "        print(f'   -> Testing Loss: {avg_testing_loss: .4f}\\n')\n",
    "\n",
    "    # Add end results to the lists\n",
    "    k_over_training_loss.append({\n",
    "        \"k\": k,\n",
    "        \"training_loss\": avg_training_loss \n",
    "    })\n",
    "\n",
    "    k_over_testing_loss.append({\n",
    "        \"k\": k,\n",
    "        \"testing_loss\": avg_testing_loss\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5446db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [d[\"k\"] for d in k_over_training_loss]\n",
    "train_loss = [d[\"training_loss\"] for d in k_over_training_loss]\n",
    "test_loss = [d[\"testing_loss\"] for d in k_over_testing_loss]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(ks, train_loss, label=\"Training Loss\")\n",
    "plt.plot(ks, test_loss, label=\"Testing Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"MNIST AutoEncoder: Bottleneck vs. Training/Testing Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47bdb94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor of the data will be....\\n\\ndata = {\\n    epoch: int\\n    training/testing loss: float\\n    model_name: str\\n}\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def freeze_module(m):\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "def unfreeze_module(m):\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "epoch_over_training_loss_per_model = []\n",
    "epoch_over_testing_loss_per_model = []\n",
    "\n",
    "'''\n",
    "For of the data will be....\n",
    "\n",
    "data = {\n",
    "    epoch: int\n",
    "    training/testing loss: float\n",
    "    model_name: str\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f262febb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m optimal_k = \u001b[32m10\u001b[39m \u001b[38;5;66;03m# PLACEHOLDER\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Have reference\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m src_model = \u001b[43mmodel\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 1. Untrained encoder not frozen\u001b[39;00m\n\u001b[32m      7\u001b[39m encoder_clean_unfrozen = MNIST_Auto_Encoder(optimal_k, hidden_layer_size=hidden_layer_size).to(device).encoder\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "optimal_k = 10 # PLACEHOLDER\n",
    "\n",
    "# Have reference\n",
    "src_model = model\n",
    "\n",
    "# 1. Untrained encoder not frozen\n",
    "encoder_clean_unfrozen = MNIST_Auto_Encoder(optimal_k, hidden_layer_size=hidden_layer_size).to(device).encoder\n",
    "\n",
    "# 2. Untrained encoder but frozen\n",
    "encoder_clean_frozen = MNIST_Auto_Encoder(optimal_k, hidden_layer_size=hidden_layer_size).to(device).encoder\n",
    "freeze_module(encoder_clean_frozen)\n",
    "unfreeze_module(encoder_clean_frozen[-1])\n",
    "\n",
    "# 3. Trained encoder not frozen\n",
    "encoder_trained_unfrozen = MNIST_Auto_Encoder(optimal_k, hidden_layer_size=hidden_layer_size).to(device).encoder\n",
    "encoder_trained_unfrozen.load_state_dict(src_model.encoder.state_dict())\n",
    "\n",
    "# 4. Trained encoder but frozen\n",
    "encoder_trained_frozen = MNIST_Auto_Encoder(optimal_k, hidden_layer_size=hidden_layer_size).to(device).encoder\n",
    "encoder_trained_frozen.load_state_dict(src_model.encoder.state_dict())\n",
    "\n",
    "freeze_module(encoder_trained_frozen)\n",
    "unfreeze_module(encoder_trained_frozen[-1])\n",
    "\n",
    "models = [encoder_clean_unfrozen, encoder_clean_unfrozen, encoder_trained_unfrozen, encoder_trained_frozen]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ceeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "\n",
    "print(f'######## Begin Training Encoders on bottleneck={optimal_k} ########')\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(training_set, \n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=True,\n",
    "                                num_workers=5, \n",
    "                                pin_memory=True\n",
    "                            )\n",
    "\n",
    "test_loader = data.DataLoader(testing_set, \n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=False,\n",
    "                                num_workers=5, \n",
    "                                pin_memory=True\n",
    "                            )\n",
    "\n",
    "for sample_model, model_name in zip(models, ['Unfrozen Clean Encoder', 'Frozen Clean Encoder', 'Unfrozen Pretrained Encoder', 'Frozen Pretrained Encoder']):\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(sample_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    avg_training_loss = 0\n",
    "    avg_testing_loss = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "\n",
    "        # Per epoch reset accumulated loss\n",
    "        avg_training_loss = 0\n",
    "        avg_testing_loss = 0\n",
    "\n",
    "        # Run through each batch\n",
    "        for images, labels in tqdm(train_loader, desc='Training', unit=' batch'):\n",
    "            # Move data into the GPU\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Zero out gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calc loss\n",
    "            logits = sample_model(images)\n",
    "            loss = loss_function(logits, labels)\n",
    "\n",
    "            # Calc gradient and step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                avg_training_loss += loss.item()\n",
    "\n",
    "        # Set up for calculating testing loss\n",
    "        avg_training_loss /=  len(train_loader)\n",
    "\n",
    "        sample_model.eval()\n",
    "\n",
    "        # Calc training loss\n",
    "        with torch.no_grad():\n",
    "            for test_images, labels in tqdm(test_loader, desc='Testing', unit=' batch'):\n",
    "                test_images = test_images.to(device)\n",
    "                logits = sample_model(test_images)\n",
    "                \n",
    "                avg_testing_loss += loss_function(logits, labels).item()\n",
    "\n",
    "        avg_testing_loss /=  len(test_loader)\n",
    "\n",
    "        sample_model.train()\n",
    "        \n",
    "        print(\"\")\n",
    "\n",
    "        print(f'   -> Training Loss: {avg_training_loss: .4f}\\n')\n",
    "        print(f'   -> Testing Loss: {avg_testing_loss: .4f}\\n')\n",
    "\n",
    "        # Add end results to the lists\n",
    "        epoch_over_training_loss_per_model.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"training_loss\": avg_training_loss,\n",
    "            \"model_name\": model_name\n",
    "        })\n",
    "\n",
    "        epoch_over_testing_loss_per_model.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"training_loss\": avg_training_loss,\n",
    "            \"model_name\": model_name\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2150c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42bdca29",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "### Problem 3: Build and train a network for classifying CIFAR, with at least 5 layers, and at least one of them being a CNN layer. Your goal should be good performance, so keep track of the testing loss as you do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f663ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset_full_CIFAR10 = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=tfm)\n",
    "testset_full_CIFAR10  = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8295131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10_Classifier(nn.Module):\n",
    "    def __init__(self, C_dropout, F_dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        conv2d_dropout = C_dropout\n",
    "\n",
    "        conv_layer_1 = 30\n",
    "        conv_layer_2 = 64\n",
    "        \n",
    "        conv_layer_3 = 128\n",
    "        conv_layer_4 = 256\n",
    "\n",
    "        self.forward_funnel_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=conv_layer_1, kernel_size=5),   # Extract useful features from the beginning\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(conv2d_dropout),\n",
    "\n",
    "            nn.Conv2d(in_channels=conv_layer_1, out_channels=conv_layer_2, kernel_size=3),  # Extract useful features from the learned features\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(conv2d_dropout),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                       # Reduce dimensionality\n",
    "        )\n",
    "\n",
    "        self.forward_funnel_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=conv_layer_2, out_channels=conv_layer_3, kernel_size=3),   # Extract useful features from the beginning\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(conv2d_dropout),\n",
    "\n",
    "            nn.Conv2d(in_channels=conv_layer_3, out_channels=conv_layer_4, kernel_size=3),  # Extract useful features from the learned features\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(conv2d_dropout),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # Compute the number of features after the input has passed the funnel\n",
    "        with torch.no_grad():\n",
    "            test_input = torch.zeros(1, 3, 32, 32)\n",
    "\n",
    "            test_input.to(device)\n",
    "\n",
    "            features = self.forward_funnel_1(test_input)\n",
    "            features = self.forward_funnel_2(features)\n",
    "\n",
    "            total_count = features.view(1, -1).size(1)\n",
    "\n",
    "        full_node_dropout = F_dropout\n",
    "        self.output_nodes = 100\n",
    "\n",
    "        self.classifer = nn.Sequential(\n",
    "            nn.Flatten(),                                           # Flatten the image from the funnel\n",
    "            nn.Linear(in_features=total_count, out_features=1000), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(full_node_dropout),\n",
    "\n",
    "            nn.Linear(in_features=1000, out_features=500),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(full_node_dropout),\n",
    "\n",
    "            nn.Linear(in_features=500, out_features=250),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(full_node_dropout),\n",
    "\n",
    "            nn.Linear(in_features=250, out_features=self.output_nodes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(full_node_dropout),\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(in_features=self.output_nodes, out_features=10)\n",
    "    \n",
    "    def partial_forward(self, x):\n",
    "        x = self.forward_funnel_1(x)\n",
    "        x = self.forward_funnel_2(x)\n",
    "        x = self.classifer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.partial_forward(x)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d588522c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nForm of the data\\n\\ndata = \\n{\\n    epoch: int\\n    training/testing loss: float\\n}\\n'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_over_training_loss_CIFAR10 = []\n",
    "epoch_over_testing_loss_CIFAR10 = []\n",
    "\n",
    "'''\n",
    "Form of the data\n",
    "\n",
    "data = \n",
    "{\n",
    "    epoch: int\n",
    "    training/testing loss: float\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b6d8d1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Begining training for CIFAR10 classifier ##########\n",
      "----- Epoch: 1/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 73.15 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 113.78 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.9835\n",
      "\n",
      "   -> Testing Loss:  1.6358\n",
      "\n",
      "----- Epoch: 2/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 71.95 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 121.53 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.6406\n",
      "\n",
      "   -> Testing Loss:  1.4000\n",
      "\n",
      "----- Epoch: 3/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 77.58 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 121.37 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.4772\n",
      "\n",
      "   -> Testing Loss:  1.2939\n",
      "\n",
      "----- Epoch: 4/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 74.20 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 114.75 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.3731\n",
      "\n",
      "   -> Testing Loss:  1.2180\n",
      "\n",
      "----- Epoch: 5/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 76.93 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 122.03 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.2839\n",
      "\n",
      "   -> Testing Loss:  1.1045\n",
      "\n",
      "----- Epoch: 6/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 77.05 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 119.01 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.2103\n",
      "\n",
      "   -> Testing Loss:  1.0442\n",
      "\n",
      "----- Epoch: 7/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 75.09 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 120.31 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.1515\n",
      "\n",
      "   -> Testing Loss:  1.0042\n",
      "\n",
      "----- Epoch: 8/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 75.91 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 120.32 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0890\n",
      "\n",
      "   -> Testing Loss:  0.9703\n",
      "\n",
      "----- Epoch: 9/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 77.46 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 122.87 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0346\n",
      "\n",
      "   -> Testing Loss:  0.9094\n",
      "\n",
      "----- Epoch: 10/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 77.76 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 119.10 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9870\n",
      "\n",
      "   -> Testing Loss:  0.8829\n",
      "\n",
      "----- Epoch: 11/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 77.17 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 117.74 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9384\n",
      "\n",
      "   -> Testing Loss:  0.8758\n",
      "\n",
      "----- Epoch: 12/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 75.07 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 113.31 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9021\n",
      "\n",
      "   -> Testing Loss:  0.8511\n",
      "\n",
      "----- Epoch: 13/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 74.11 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 113.70 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.8649\n",
      "\n",
      "   -> Testing Loss:  0.8256\n",
      "\n",
      "----- Epoch: 14/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 77.53 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 122.93 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.8303\n",
      "\n",
      "   -> Testing Loss:  0.8115\n",
      "\n",
      "----- Epoch: 15/15 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 78.07 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 122.39 batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.7996\n",
      "\n",
      "   -> Testing Loss:  0.8072\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter setup\n",
    "epochs = 15\n",
    "batch_size = 64\n",
    "learning_rate = 5e-4\n",
    "decay_rate = 4e-4\n",
    "\n",
    "c_dropout = 0.25\n",
    "f_dropout = 0.25\n",
    "\n",
    "print('######## Begining training for CIFAR10 classifier ##########')\n",
    "\n",
    "# Setup data loaders\n",
    "trainset_loader_CIFAR10 = data.DataLoader(trainset_full_CIFAR10,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   # num_workers=5,\n",
    "                                   pin_memory=True)\n",
    "\n",
    "testset_loader_CIFAR10 = data.DataLoader(testset_full_CIFAR10,\n",
    "                                   batch_size=batch_size,\n",
    "                                   # num_workers=5,\n",
    "                                   shuffle=False,\n",
    "                                   pin_memory=True)\n",
    "\n",
    "model = CIFAR10_Classifier(c_dropout, f_dropout)\n",
    "model.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                       lr=learning_rate, \n",
    "                       weight_decay=decay_rate\n",
    "                       )\n",
    "\n",
    "# Have references to variables outside of the epoch loop\n",
    "avg_training_loss = 0\n",
    "avg_testing_loss = 0\n",
    "\n",
    "# Epoch Loop\n",
    "for epoch in range(epochs):\n",
    "    print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "    \n",
    "    avg_training_loss = 0\n",
    "    avg_testing_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for x, Y in tqdm(trainset_loader_CIFAR10, desc='Training', unit=' batch'):\n",
    "        # Transfer images to GPU\n",
    "        x = x.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Send images to model\n",
    "        x_pred = model(x)\n",
    "\n",
    "        # Calc loss\n",
    "        loss = loss_function(x_pred, Y)\n",
    "\n",
    "        # Calc gradient and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            avg_training_loss += loss.item()\n",
    "\n",
    "    # Switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, Y in tqdm(testset_loader_CIFAR10, desc='Testing', unit=' batches'):\n",
    "            # Move the images to the GPU\n",
    "            x = x.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            # Get logits and sum up total loss\n",
    "            x_pred = model(x)\n",
    "            avg_testing_loss += loss_function(x_pred, Y).item()\n",
    "\n",
    "    # Get training loss\n",
    "    avg_training_loss /= len(trainset_loader_CIFAR10)\n",
    "\n",
    "     # Get testing loss\n",
    "    avg_testing_loss /= len(testset_loader_CIFAR10)\n",
    "\n",
    "    # Switch model back to training mode\n",
    "    model.train()\n",
    "\n",
    "    epoch_over_training_loss_CIFAR10.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"training_loss\": avg_training_loss\n",
    "        })\n",
    "    \n",
    "    epoch_over_testing_loss_CIFAR10.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"testing_loss\": avg_testing_loss\n",
    "        })\n",
    "    \n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    print(f'   -> Training Loss: {avg_training_loss: .4f}\\n')\n",
    "    print(f'   -> Testing Loss: {avg_testing_loss: .4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "75e63add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.7297\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in testset_loader_CIFAR10:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total += yb.numel()\n",
    "\n",
    "final_accuracy = float(correct / total)\n",
    "\n",
    "print(f\"Final Accuracy: {final_accuracy}\")\n",
    "\n",
    "epochs = [d[\"epoch\"] for d in epoch_over_training_loss_CIFAR10]\n",
    "train_loss = [d[\"training_loss\"] for d in epoch_over_training_loss_CIFAR10]\n",
    "test_loss = [d[\"testing_loss\"] for d in epoch_over_testing_loss_CIFAR10]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_loss, label=\"Training Loss\")\n",
    "plt.plot(epochs, test_loss, label=\"Testing Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"CIFAR10 Classifer: Training and Testing Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a605f5ad",
   "metadata": {},
   "source": [
    "#### Results \n",
    "\n",
    "Testing Accuracy: 72.97%\n",
    "\n",
    "Training Loss: 0.7996\n",
    "\n",
    "Testing Loss: 0.8072\n",
    "\n",
    "#### Final Model Hyperparameters\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "learning_rate = 5e-4\n",
    "\n",
    "decay_rate = 4e-4\n",
    "\n",
    "c_dropout = 0.25\n",
    "\n",
    "f_dropout = 0.25\n",
    "\n",
    "\n",
    "#### Model Architecture 1\n",
    "\n",
    "In the beginning, I decided that I wanted to have two convolutional layers, one that extracted a lot of features from the image data and one that found more advanced features. Since the first layer would find as many initial features to start, I decided to make the out channel 128 and kernel size 5x5. Then, the second layer would condense those features, so I chose an out channel size of 64 and kernel size of 3x3. Additionally, I stacked 4 decreasing, funneling, fully-connected layers, 1000 -> 1000 -> 500 -> 100, that finally connected to the output layer. My reasoning was that doing this would allow me to find complex features that would become more and more concentrated and meaningful. Finally, I added a max pooling layer after the convolutional layers to help reduce dimensionality of the data and dropout layers after every layer to prevent overfitting. \n",
    "\n",
    "#### Optimizer\n",
    "I went with Adam since I wanted to experiment with using different options. Furthermore, I wanted my loss to be as low as possible, so I wanted to use weight decay. As such, the final optimizer I used was AdamW since it handles weight decay better.\n",
    "\n",
    "#### Initial training\n",
    "\n",
    "I started with initial hyperparamters of batch_size = 64, learning_rate = 1e-3, decay_rate = 5e-4 (weight decay), c_dropout = 0.40 (the dropout probability for each convolutional layer), and f_dropout = 0.60 (the dropout probability for each fully-connected layer). After the first training, I noticed that my losses were not really good for this data set, training loss ~= 1.3 and testing loss ~= 1.25. Specially, they could never get past the 1.2 barrier and started to stagnate at the 14th epoch. Since I did not think overfitting was an issue yet, I decided to tweak the learning rate from 1e-3 to 5e-4.\n",
    "\n",
    "In the second trial, I noticed that the issue of the model not being able to learn was still present, losses similar to the first trial, so I thought the maybe my dropout p's were a bit too high. This caused me to set both of them to 0.4.\n",
    "\n",
    "In my next training attempt, I saw I was able to break past the 1.2 loss barrier by the 10th epoch which solidified my reasoning that my p's were too high, but I was unable to get a testing loss lower than ~1.10 after the 15th epoch. However, I did notice that model was still learning well, it was able to get training loss lower than 1.10, so I decided that I should try changing the weight decay to 4e-4 to see if it would help. I did not increase the weight decay since I thought that would cause my losses to be greater since I was already using a good amount of dropout.\n",
    "\n",
    "For the fourth trial, I saw that I got similar testing loss, ~1.08, so I decided to try changing the other hyperparameters. Yet, no matter which ones I tried to change, I was unable to break the testing loss = 1.00 barrier, even though the model was learning well beyond that point. This led me to think that my architecture was a bit too complex, so I decided to rethink of what I was doing and change my model.\n",
    "\n",
    "#### Model Architecture 2\n",
    "\n",
    "For this iteration of the model, I decided to change the number of channels in my convolutional layers from 128 -> 64 to 30 -> 64. This happened because I decided to look at the image size which is 32x32 and realized that I might be trying to find too many features in the beginning but then sampling them down too much in the second layer. Instead, it would probably be better to find fewer initial features and then find complex relationships between them in the second convolutional layer. The second major change I made was making the fully-connected layers continously decreasing from 1000 -> 1000 -> 500 -> 100 to 1000 -> 500 -> 250 -> 100 since I wanted to downscale the model.\n",
    "\n",
    "#### Training for Second Model\n",
    "\n",
    "I decided to keep the hyperparameters that I obtained from the initial model since I thought they might work well with this new model, batch_size = 64, learning_rate = 5e-4, and decay_rate = 4e-4. However, I chose to decrease the dropouts both to 0.25 since I wanted to start at a new, lower point and tweak the dropout as I trained this new model.\n",
    "\n",
    "For the first training attempt, my model performed way better than the first model both in terms of speed and accuracy since it was able to get training loss ~= 0.9 and testing loss ~= 0.89 at the 5th epoch. Beyond that point though, the model started overfitting severally and could not get past the testing loss being ~0.89. From this point owards, I tried tweaking the other hyperparameters but no matter what I changed and how I changed it. I could never get a testing loss beyond 0.9. This lead me to the conclusion that my model needed to be changed once again.\n",
    "\n",
    "#### Model Architecture 3\n",
    "\n",
    "For this model, I decided to add more convolutional layers plus an additional max pooling layer since the features that they can extract would be more useful compared to just plain fully-connected layers. As such, this model mirrors the second model but the initial convolution layers were changed from 30 -> 64 to 30 -> 64 -> 128 -> 256. I chose to make the channels increase since I thought that it would be better to learn more and more complex combinations of the initial features rather than trying to funnel them down. However, the pooling layer would help with funneling the features anyways, so it would be better to not do that inside of the convolutional layers themselves. These new layers also have kernel size 3x3.\n",
    "\n",
    "#### Training for Third Model\n",
    "\n",
    "With the optimal hyperparameters from the second model, batch_size = 64, learning_rate = 5e-4, decay_rate = 4e-4, c_dropout = 0.25, and f_dropout = 0.25, I was able to finally break the loss barrier and achieve a testing loss ~= 0.80 at the 15th epoch. Sadly, tuning the hyperparameters did not result in any new finds which makes me think that to get better accuracy, I would need to append more and more convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49beb7f3",
   "metadata": {},
   "source": [
    "### Problem 4: Using a model of the same architecture (except for the last layer), train a model to detect whether a CIFAR image is upright, or has been rotated 90 degrees. (Note: You will need to modify the CIFAR data to construct the data you need for this task. How can you easily do this kind of rotation, with the image as a tensor?) Again, you want good performance, so make sure to track the testing loss as well (as defined by this new classification task)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ccfc756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10_Transformed_Classifier(CIFAR10_Classifier):\n",
    "    def __init__(self, C_dropout, F_dropout):\n",
    "        super().__init__(C_dropout, F_dropout)\n",
    "        \n",
    "        self.output_layer = nn.Linear(in_features=self.output_nodes, out_features=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.partial_forward(x)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "50be51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_over_training_loss_CIFAR10_R = []\n",
    "epoch_over_testing_loss_CIFAR10_R = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9bfa844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Begining training for CIFAR10 classifier on rotated images ##########\n",
      "----- Epoch: 1/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 51.72 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 101.32 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.5809\n",
      "\n",
      "   -> Testing Loss:  0.4976\n",
      "\n",
      "----- Epoch: 2/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.44 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 102.16 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.5171\n",
      "\n",
      "   -> Testing Loss:  0.4958\n",
      "\n",
      "----- Epoch: 3/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 52.69 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 100.74 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4993\n",
      "\n",
      "   -> Testing Loss:  0.4724\n",
      "\n",
      "----- Epoch: 4/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 52.14 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:02<00:00, 99.13 batches/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4852\n",
      "\n",
      "   -> Testing Loss:  0.4594\n",
      "\n",
      "----- Epoch: 5/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 52.15 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:02<00:00, 97.73 batches/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4758\n",
      "\n",
      "   -> Testing Loss:  0.4368\n",
      "\n",
      "----- Epoch: 6/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 51.62 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:02<00:00, 97.79 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4653\n",
      "\n",
      "   -> Testing Loss:  0.4454\n",
      "\n",
      "----- Epoch: 7/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 52.88 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 102.15 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4538\n",
      "\n",
      "   -> Testing Loss:  0.4215\n",
      "\n",
      "----- Epoch: 8/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 52.91 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 100.99 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4476\n",
      "\n",
      "   -> Testing Loss:  0.4177\n",
      "\n",
      "----- Epoch: 9/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 52.26 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:02<00:00, 98.34 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4396\n",
      "\n",
      "   -> Testing Loss:  0.4169\n",
      "\n",
      "----- Epoch: 10/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 52.20 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 101.62 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4340\n",
      "\n",
      "   -> Testing Loss:  0.3938\n",
      "\n",
      "----- Epoch: 11/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 52.99 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 102.01 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4234\n",
      "\n",
      "   -> Testing Loss:  0.3838\n",
      "\n",
      "----- Epoch: 12/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 52.94 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 101.68 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4192\n",
      "\n",
      "   -> Testing Loss:  0.3789\n",
      "\n",
      "----- Epoch: 13/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.36 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 104.32 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4138\n",
      "\n",
      "   -> Testing Loss:  0.3749\n",
      "\n",
      "----- Epoch: 14/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 54.03 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 103.10 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4056\n",
      "\n",
      "   -> Testing Loss:  0.3715\n",
      "\n",
      "----- Epoch: 15/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 52.89 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 105.10 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.4018\n",
      "\n",
      "   -> Testing Loss:  0.3621\n",
      "\n",
      "----- Epoch: 16/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.92 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 102.36 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3964\n",
      "\n",
      "   -> Testing Loss:  0.3675\n",
      "\n",
      "----- Epoch: 17/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.92 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 102.51 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3893\n",
      "\n",
      "   -> Testing Loss:  0.3540\n",
      "\n",
      "----- Epoch: 18/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.78 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 104.77 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3880\n",
      "\n",
      "   -> Testing Loss:  0.3466\n",
      "\n",
      "----- Epoch: 19/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.42 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 103.89 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3829\n",
      "\n",
      "   -> Testing Loss:  0.3432\n",
      "\n",
      "----- Epoch: 20/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.68 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 102.60 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3771\n",
      "\n",
      "   -> Testing Loss:  0.3434\n",
      "\n",
      "----- Epoch: 21/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.70 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 103.08 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3753\n",
      "\n",
      "   -> Testing Loss:  0.3468\n",
      "\n",
      "----- Epoch: 22/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.63 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 102.01 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3695\n",
      "\n",
      "   -> Testing Loss:  0.3312\n",
      "\n",
      "----- Epoch: 23/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.66 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 102.81 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3661\n",
      "\n",
      "   -> Testing Loss:  0.3328\n",
      "\n",
      "----- Epoch: 24/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.43 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 102.89 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3628\n",
      "\n",
      "   -> Testing Loss:  0.3336\n",
      "\n",
      "----- Epoch: 25/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.84 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 103.64 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3584\n",
      "\n",
      "   -> Testing Loss:  0.3230\n",
      "\n",
      "----- Epoch: 26/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.73 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 103.35 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3538\n",
      "\n",
      "   -> Testing Loss:  0.3228\n",
      "\n",
      "----- Epoch: 27/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 51.88 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 102.39 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3520\n",
      "\n",
      "   -> Testing Loss:  0.3310\n",
      "\n",
      "----- Epoch: 28/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 51.24 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 102.24 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3462\n",
      "\n",
      "   -> Testing Loss:  0.3166\n",
      "\n",
      "----- Epoch: 29/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.35 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 103.72 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3460\n",
      "\n",
      "   -> Testing Loss:  0.3191\n",
      "\n",
      "----- Epoch: 30/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.63 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 103.58 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3446\n",
      "\n",
      "   -> Testing Loss:  0.3106\n",
      "\n",
      "----- Epoch: 31/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.43 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 102.01 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3380\n",
      "\n",
      "   -> Testing Loss:  0.3262\n",
      "\n",
      "----- Epoch: 32/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.57 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 102.92 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3361\n",
      "\n",
      "   -> Testing Loss:  0.3198\n",
      "\n",
      "----- Epoch: 33/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.86 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 104.04 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3320\n",
      "\n",
      "   -> Testing Loss:  0.3125\n",
      "\n",
      "----- Epoch: 34/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.98 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 103.11 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3284\n",
      "\n",
      "   -> Testing Loss:  0.3111\n",
      "\n",
      "----- Epoch: 35/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 54.05 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 102.32 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3271\n",
      "\n",
      "   -> Testing Loss:  0.3031\n",
      "\n",
      "----- Epoch: 36/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.76 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 101.85 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3240\n",
      "\n",
      "   -> Testing Loss:  0.3063\n",
      "\n",
      "----- Epoch: 37/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.77 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 103.00 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3199\n",
      "\n",
      "   -> Testing Loss:  0.2987\n",
      "\n",
      "----- Epoch: 38/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.78 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 105.12 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3179\n",
      "\n",
      "   -> Testing Loss:  0.3036\n",
      "\n",
      "----- Epoch: 39/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.64 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 102.76 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3114\n",
      "\n",
      "   -> Testing Loss:  0.3009\n",
      "\n",
      "----- Epoch: 40/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 51.91 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 102.75 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3109\n",
      "\n",
      "   -> Testing Loss:  0.3065\n",
      "\n",
      "----- Epoch: 41/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 52.51 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 100.18 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3078\n",
      "\n",
      "   -> Testing Loss:  0.3071\n",
      "\n",
      "----- Epoch: 42/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 52.78 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 100.18 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3045\n",
      "\n",
      "   -> Testing Loss:  0.3010\n",
      "\n",
      "----- Epoch: 43/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 51.87 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 100.79 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.3013\n",
      "\n",
      "   -> Testing Loss:  0.2990\n",
      "\n",
      "----- Epoch: 44/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 51.91 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:02<00:00, 99.57 batches/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2970\n",
      "\n",
      "   -> Testing Loss:  0.2971\n",
      "\n",
      "----- Epoch: 45/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:18<00:00, 53.01 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 100.86 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2964\n",
      "\n",
      "   -> Testing Loss:  0.2951\n",
      "\n",
      "----- Epoch: 46/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 51.70 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:02<00:00, 99.89 batches/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2923\n",
      "\n",
      "   -> Testing Loss:  0.2976\n",
      "\n",
      "----- Epoch: 47/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 51.96 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:02<00:00, 99.01 batches/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2897\n",
      "\n",
      "   -> Testing Loss:  0.2957\n",
      "\n",
      "----- Epoch: 48/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 51.37 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:02<00:00, 93.60 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2857\n",
      "\n",
      "   -> Testing Loss:  0.2943\n",
      "\n",
      "----- Epoch: 49/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 51.77 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 100.68 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2846\n",
      "\n",
      "   -> Testing Loss:  0.2923\n",
      "\n",
      "----- Epoch: 50/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1000/1000 [00:19<00:00, 51.62 batch/s]\n",
      "Testing: 100%|██████████| 200/200 [00:01<00:00, 100.33 batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.2808\n",
      "\n",
      "   -> Testing Loss:  0.2898\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter setup\n",
    "epochs = 50\n",
    "batch_size = 50\n",
    "learning_rate = 5e-4\n",
    "decay_rate = 1e-3\n",
    "\n",
    "c_dropout = 0.30\n",
    "f_dropout = 0.30\n",
    "\n",
    "\n",
    "print('######## Begining training for CIFAR10 classifier on rotated images ##########')\n",
    "\n",
    "# Setup data loaders\n",
    "trainset_loader_CIFAR10_R = data.DataLoader(trainset_full_CIFAR10,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   # num_workers=5,\n",
    "                                   pin_memory=True)\n",
    "\n",
    "testset_loader_CIFAR10_R = data.DataLoader(testset_full_CIFAR10,\n",
    "                                   batch_size=batch_size,\n",
    "                                   # num_workers=5,\n",
    "                                   shuffle=False,\n",
    "                                   pin_memory=True)\n",
    "\n",
    "model = CIFAR10_Transformed_Classifier(c_dropout, f_dropout)\n",
    "model.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                       lr=learning_rate, \n",
    "                       weight_decay=decay_rate\n",
    "                       )\n",
    "\n",
    "# Have references to variables outside of the epoch loop\n",
    "avg_training_loss = 0\n",
    "avg_testing_loss = 0\n",
    "\n",
    "# Epoch Loop\n",
    "for epoch in range(epochs):\n",
    "    print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "    \n",
    "    avg_training_loss = 0\n",
    "    avg_testing_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for x, _ in tqdm(trainset_loader_CIFAR10_R, desc='Training', unit=' batch'):\n",
    "\n",
    "        labels_upright = torch.zeros(x.size(0), dtype=torch.long)\n",
    "\n",
    "        images_rotated = torch.rot90(x, 1, [2, 3])\n",
    "        labels_rotated = torch.ones(x.size(0), dtype=torch.long)\n",
    "\n",
    "        all_images = torch.cat([x, images_rotated])\n",
    "        all_labels = torch.cat([labels_upright, labels_rotated])\n",
    "        \n",
    "        \n",
    "        # Transfer images to GPU\n",
    "        all_images = all_images.to(device)\n",
    "        all_labels = all_labels.to(device)\n",
    "\n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Send images to model\n",
    "        x_pred = model(all_images)\n",
    "\n",
    "        # Calc loss\n",
    "        loss = loss_function(x_pred, all_labels)\n",
    "\n",
    "        # Calc gradient and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            avg_training_loss += loss.item()\n",
    "\n",
    "    # Switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in tqdm(testset_loader_CIFAR10_R, desc='Testing', unit=' batches'):\n",
    "            \n",
    "            labels_upright = torch.zeros(x.size(0), dtype=torch.long)\n",
    "\n",
    "            images_rotated = torch.rot90(x, 1, [2, 3])\n",
    "            labels_rotated = torch.ones(x.size(0), dtype=torch.long)\n",
    "\n",
    "            all_images = torch.cat([x, images_rotated])\n",
    "            all_labels = torch.cat([labels_upright, labels_rotated])\n",
    "            \n",
    "            # Transfer images to GPU\n",
    "            all_images = all_images.to(device)\n",
    "            all_labels = all_labels.to(device)\n",
    "            \n",
    "            # Move the images to the GPU\n",
    "            all_images = all_images.to(device)\n",
    "            all_labels = all_labels.to(device)\n",
    "\n",
    "            # Get logits and sum up total loss\n",
    "            x_pred = model(all_images)\n",
    "            avg_testing_loss += loss_function(x_pred, all_labels).item()\n",
    "\n",
    "    # Get training loss\n",
    "    avg_training_loss /= len(trainset_loader_CIFAR10_R)\n",
    "\n",
    "     # Get testing loss\n",
    "    avg_testing_loss /= len(testset_loader_CIFAR10_R)\n",
    "\n",
    "    # Switch model back to training mode\n",
    "    model.train()\n",
    "\n",
    "    epoch_over_training_loss_CIFAR10_R.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"training_loss\": avg_training_loss\n",
    "        })\n",
    "    \n",
    "    epoch_over_testing_loss_CIFAR10_R.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"testing_loss\": avg_testing_loss\n",
    "        })\n",
    "    \n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    print(f'   -> Training Loss: {avg_training_loss: .4f}\\n')\n",
    "    print(f'   -> Testing Loss: {avg_testing_loss: .4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3443e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights from problem 5\n",
    "torch.save(model.state_dict(), 'Q4_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c4c9ea61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.87015\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for x, _ in testset_loader_CIFAR10_R:\n",
    "        \n",
    "        labels_upright = torch.zeros(x.size(0), dtype=torch.long)\n",
    "\n",
    "        images_rotated = torch.rot90(x, 1, [2, 3])\n",
    "        labels_rotated = torch.ones(x.size(0), dtype=torch.long)\n",
    "\n",
    "        all_images = torch.cat([x, images_rotated])\n",
    "        all_labels = torch.cat([labels_upright, labels_rotated])\n",
    "        \n",
    "        # Transfer images to GPU\n",
    "        all_images = all_images.to(device)\n",
    "        all_labels = all_labels.to(device)\n",
    "        \n",
    "        # Move the images to the GPU\n",
    "        all_images = all_images.to(device)\n",
    "        all_labels = all_labels.to(device)\n",
    "\n",
    "        # Get logits and sum up total loss\n",
    "        logits = model(all_images)\n",
    "        \n",
    "        pred = logits.argmax(1)\n",
    "        \n",
    "        correct += (pred == all_labels).sum().item()\n",
    "        total += all_labels.numel()\n",
    "\n",
    "final_accuracy = float(correct / total)\n",
    "\n",
    "print(f\"Final Accuracy: {final_accuracy}\")\n",
    "\n",
    "\n",
    "epochs = [d[\"epoch\"] for d in epoch_over_training_loss_CIFAR10_R]\n",
    "train_loss = [d[\"training_loss\"] for d in epoch_over_training_loss_CIFAR10_R]\n",
    "test_loss = [d[\"testing_loss\"] for d in epoch_over_testing_loss_CIFAR10_R]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_loss, label=\"Training Loss\")\n",
    "plt.plot(epochs, test_loss, label=\"Testing Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"CIFAR10 Rotation Classifer: Training and Testing Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2396329",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Testing Accuracy: 87.015%\n",
    "\n",
    "Training Loss: 0.2808\n",
    "\n",
    "Testing Loss: 0.2898\n",
    "\n",
    "#### Final Model Hyperparameters\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "learning_rate = 5e-4\n",
    "\n",
    "decay_rate = 1e-3\n",
    "\n",
    "c_dropout = 0.30\n",
    "\n",
    "f_dropout = 0.30\n",
    "\n",
    "#### Model Architecture\n",
    "\n",
    "For this problem, I am forced to maintain the original architecture for the best model that I found in Question 3. Of course, the final output layer is of size two for the two different classes to predict.\n",
    "\n",
    "#### Initial Model Training\n",
    "\n",
    "To start, I wanted to use different initial hyperparameters rather than the ones that I found for my model in Question 3 since I belived that doing so might allow me to learn more about my current model. As such, I used batch_size = 100, learning_rate = 1e-4, decay_rate = 5e-4, c_dropout = 0.10, and f_dropout = 0.10. For the first training loop, the model was able to achieve a training and testing loss of around 0.32 at the 11th epoch before it began to overfit. Unlike the previous models, the training and testing loss were pretty close to each other per epoch with not considerable gap, for the other models, the gap between the training and testing loss would be around 0.15 with the testing loss always being lower. Seeing this, I decided to go straight to tunning the dropout, from c_dropout = f_dropout = 0.10 to c_dropout = f_dropout = 0.20.\n",
    "\n",
    "In the second trial, my model was only able to get a testing loss around 0.3180 at the 19th epoch before it began overfitting. Though, by the 4th epoch (around 0.40 loss), the model began learning way too slowly. At first, I wanted to change the dropout rate, but I wanted to fiddle around with the batch size a little bit more, so I decided to change the batch size from 100 to 125 first. Yet, this did not help as the model still was stuck at a similar loss. Decreasing the batch size to 50 gave me the same results, so I decided to move onto changing the learning rate.\n",
    "\n",
    "For the fifth trial, I increased the learning rate from 1e-4 to 5e-4, and the model was able to get a testing loss = 0.3073 at the 17th epoch. By this point, I have started to notice that getting the loss lower than 0.25 would probably require better architecture. Seeing the results, I was satisfied with the learning rate, so I decided to once again change the dropouts since learning rate does not seem to be the reason as to why the model is learning slower.\n",
    "\n",
    "In the sixth attempt, I changed the dropouts to be both 0.30 which let the model achieve a new minimum testing loss of 0.2960 at the 46th epoch. As can be seen, it seems the model requires a lot more training time at its current state. While it is training though, the training and testing losses decrement very slowly often with a minimal loss gap.\n",
    "\n",
    "For my final change to training, I decided that I should try increasing the weight decay from 5e-4 to 1e-3 to see if it would benefit due to the model's slow training. From this change, my model was able to achieve a new testing loss of 0.2898 at the 50th epoch. I believed that increasing the weight decay would be helpful since the model training was very slow from the 0.35 loss range and below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "96a5475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_weights(model_final, model_src, k, is_frozen):\n",
    "    \n",
    "    for i in range(len(model_final.forward_funnel_1)):\n",
    "\n",
    "        if k == 0: return\n",
    "\n",
    "        src_layer = model_src.forward_funnel_1[i]\n",
    "        final_layer = model_final.forward_funnel_1[i]\n",
    "\n",
    "        if (hasattr(src_layer, 'weight') and hasattr(final_layer, 'weight')):\n",
    "            final_layer.weight.data = src_layer.weight.data.clone()\n",
    "            if is_frozen:\n",
    "                final_layer.weight.requires_grad = False\n",
    "            \n",
    "            # This will always run if we get here. I do not intend on making layers with biases\n",
    "            if (hasattr(src_layer, 'bias') and hasattr(final_layer, 'bias')):\n",
    "                final_layer.bias.data = src_layer.bias.data.clone()\n",
    "                if is_frozen:\n",
    "                    final_layer.bias.requires_grad = False\n",
    "\n",
    "                k -= 1\n",
    "\n",
    "    \n",
    "    for i in range(len(model_final.classifer)):\n",
    "        \n",
    "        if k == 0: return\n",
    "\n",
    "        src_layer = model_src.classifer[i]\n",
    "        final_layer = model_final.classifer[i]\n",
    "\n",
    "        if (hasattr(src_layer, 'weight') and hasattr(final_layer, 'weight')):\n",
    "            final_layer.weight.data = src_layer.weight.data.clone()\n",
    "            if is_frozen:\n",
    "                final_layer.weight.requires_grad = False\n",
    "            \n",
    "            # This will always run if we get here. I do not intend on making layers with biases\n",
    "            if (hasattr(src_layer, 'bias') and hasattr(final_layer, 'bias')):\n",
    "                final_layer.bias.data = src_layer.bias.data.clone()\n",
    "                if is_frozen:\n",
    "                    final_layer.bias.requires_grad = False\n",
    "\n",
    "                k -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da72851a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor of the data for this list is...\\n\\ndata = {\\n    k: int\\n    epoch: int\\n    training/testing loss: floats\\n    frozen: True or False\\n}\\n\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_over_training_loss_CIFAR10_per_k = []\n",
    "epoch_over_testing_loss_CIFAR10_per_k = []\n",
    "\n",
    "'''\n",
    "For of the data for this list is...\n",
    "\n",
    "data = {\n",
    "    k: int\n",
    "    epoch: int\n",
    "    training/testing loss: floats\n",
    "    frozen: True or False\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1dea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Begining training for CIFAR10 classifier + transfer learning ##########\n",
      ":::: Training model with Frozen weights ::::\n",
      "----- Epoch: 1/30 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:13<00:00, 57.87 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:02<00:00, 77.19 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.8896\n",
      "\n",
      "   -> Testing Loss:  0.7002\n",
      "\n",
      "----- Epoch: 2/30 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:12<00:00, 60.41 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:02<00:00, 75.46 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.7024\n",
      "\n",
      "   -> Testing Loss:  0.6759\n",
      "\n",
      "----- Epoch: 3/30 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:13<00:00, 57.49 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:02<00:00, 69.92 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.6552\n",
      "\n",
      "   -> Testing Loss:  0.6116\n",
      "\n",
      "----- Epoch: 4/30 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:13<00:00, 56.91 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:02<00:00, 73.69 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.6054\n",
      "\n",
      "   -> Testing Loss:  0.5775\n",
      "\n",
      "----- Epoch: 5/30 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:13<00:00, 56.97 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:02<00:00, 78.12 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.5787\n",
      "\n",
      "   -> Testing Loss:  0.5490\n",
      "\n",
      "----- Epoch: 6/30 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  83%|████████▎ | 649/782 [00:11<00:02, 58.88 batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     80\u001b[39m optimizer.zero_grad()\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Send images to model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m x_pred = \u001b[43mmodel_final\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Calc loss\u001b[39;00m\n\u001b[32m     86\u001b[39m loss = loss_function(x_pred, all_labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects and Related Work\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects and Related Work\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mCIFAR10_Classifier.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpartial_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     logits = \u001b[38;5;28mself\u001b[39m.output_layer(x)\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mCIFAR10_Classifier.partial_forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     71\u001b[39m x = \u001b[38;5;28mself\u001b[39m.forward_funnel_1(x)\n\u001b[32m     72\u001b[39m x = \u001b[38;5;28mself\u001b[39m.forward_funnel_2(x)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclassifer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects and Related Work\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects and Related Work\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects and Related Work\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects and Related Work\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects and Related Work\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects and Related Work\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter setup\n",
    "epochs = 15\n",
    "batch_size = 64\n",
    "learning_rate = 5e-4\n",
    "decay_rate = 4e-4\n",
    "\n",
    "c_dropout = 0.25\n",
    "f_dropout = 0.25\n",
    "\n",
    "k = 5 # Max number of layers for the current model ~~ Excluding the output layer\n",
    "\n",
    "# Initialize model for the sake of weight transfer\n",
    "src_model = CIFAR10_Transformed_Classifier(c_dropout, f_dropout)\n",
    "src_model.load_state_dict(torch.load('Q4_model_weights.pth'))\n",
    "\n",
    "\n",
    "print('######## Begining training for CIFAR10 classifier + transfer learning ##########')\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    is_frozen = True if i == 0 else False\n",
    "\n",
    "    print(f':::: Training model with {'Frozen' if is_frozen else 'Unfrozen'} weights ::::')\n",
    "    \n",
    "    for current_k in range(1, k, 1):\n",
    "        # Setup data loaders\n",
    "        trainset_loader_CIFAR10 = data.DataLoader(trainset_full_CIFAR10,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        # num_workers=5,\n",
    "                                        pin_memory=True)\n",
    "\n",
    "        testset_loader_CIFAR10 = data.DataLoader(testset_full_CIFAR10,\n",
    "                                        batch_size=batch_size,\n",
    "                                        # num_workers=5,\n",
    "                                        shuffle=False,\n",
    "                                        pin_memory=True)\n",
    "\n",
    "        model_final = CIFAR10_Classifier(c_dropout, f_dropout)\n",
    "\n",
    "        transfer_weights(model_final, src_model, k, is_frozen)\n",
    "\n",
    "        model_final.to(device)\n",
    "\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model_final.parameters(), \n",
    "                            lr=learning_rate, \n",
    "                            weight_decay=decay_rate\n",
    "                            )\n",
    "\n",
    "        # Have references to variables outside of the epoch loop\n",
    "        avg_training_loss = 0\n",
    "        avg_testing_loss = 0\n",
    "\n",
    "        # Epoch Loop\n",
    "        for epoch in range(epochs):\n",
    "            print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "            \n",
    "            avg_training_loss = 0\n",
    "            avg_testing_loss = 0\n",
    "\n",
    "            model_final.train()\n",
    "\n",
    "            for x, _ in tqdm(trainset_loader_CIFAR10, desc='Training', unit=' batch'):\n",
    "\n",
    "                labels_upright = torch.zeros(x.size(0), dtype=torch.long)\n",
    "\n",
    "                images_rotated = torch.rot90(x, 1, [2, 3])\n",
    "                labels_rotated = torch.ones(x.size(0), dtype=torch.long)\n",
    "\n",
    "                all_images = torch.cat([x, images_rotated])\n",
    "                all_labels = torch.cat([labels_upright, labels_rotated])\n",
    "                \n",
    "                \n",
    "                # Transfer images to GPU\n",
    "                all_images = all_images.to(device)\n",
    "                all_labels = all_labels.to(device)\n",
    "\n",
    "                # Zero out gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Send images to model\n",
    "                x_pred = model_final(all_images)\n",
    "\n",
    "                # Calc loss\n",
    "                loss = loss_function(x_pred, all_labels)\n",
    "\n",
    "                # Calc gradient and update weights\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    avg_training_loss += loss\n",
    "\n",
    "            # Switch to eval mode\n",
    "            model_final.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for x, _ in tqdm(testset_loader_CIFAR10, desc='Testing', unit=' batches'):\n",
    "                    \n",
    "                    labels_upright = torch.zeros(x.size(0), dtype=torch.long)\n",
    "\n",
    "                    images_rotated = torch.rot90(x, 1, [2, 3])\n",
    "                    labels_rotated = torch.ones(x.size(0), dtype=torch.long)\n",
    "\n",
    "                    all_images = torch.cat([x, images_rotated])\n",
    "                    all_labels = torch.cat([labels_upright, labels_rotated])\n",
    "                    \n",
    "                    # Transfer images to GPU\n",
    "                    all_images = all_images.to(device)\n",
    "                    all_labels = all_labels.to(device)\n",
    "                    \n",
    "                    # Move the images to the GPU\n",
    "                    all_images = all_images.to(device)\n",
    "                    all_labels = all_labels.to(device)\n",
    "\n",
    "                    # Get logits and sum up total loss\n",
    "                    x_pred = model_final(all_images)\n",
    "                    avg_testing_loss += loss_function(x_pred, all_labels).item()\n",
    "\n",
    "            # Get training loss\n",
    "            avg_training_loss /= len(trainset_loader_CIFAR10)\n",
    "\n",
    "            # Get testing loss\n",
    "            avg_testing_loss /= len(testset_loader_CIFAR10)\n",
    "\n",
    "            # Switch model back to training mode\n",
    "            model_final.train()\n",
    "\n",
    "            epoch_over_training_loss_CIFAR10_per_k.append({\n",
    "                \"k\": current_k,\n",
    "                \"epoch\": epoch,\n",
    "                \"training_loss\": avg_training_loss\n",
    "                \"frozen\": is_frozen\n",
    "                })\n",
    "            \n",
    "            epoch_over_testing_loss_CIFAR10_per_k.append({\n",
    "                \"k\": current_k,\n",
    "                \"epoch\": epoch,\n",
    "                \"training_loss\": avg_training_loss\n",
    "                \"frozen\": is_frozen\n",
    "                })\n",
    "            \n",
    "\n",
    "            print(\"\")\n",
    "\n",
    "            print(f'   -> Training Loss: {avg_training_loss: .4f}\\n')\n",
    "            print(f'   -> Testing Loss: {avg_testing_loss: .4f}\\n')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4754fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate frozen and unfrozen runs\n",
    "for frozen_status, title in zip([True, False], [\"Frozen Transfer Learning\", \"Unfrozen Transfer Learning\"]):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ks = sorted(set(d[\"k\"] for d in epoch_over_training_loss if d[\"frozen\"] == frozen_status))\n",
    "    for k in ks:\n",
    "        epochs = [d[\"epoch\"] for d in epoch_over_training_loss if d[\"k\"] == k and d[\"frozen\"] == frozen_status]\n",
    "        train_loss = [d[\"training_loss\"] for d in epoch_over_training_loss if d[\"k\"] == k and d[\"frozen\"] == frozen_status]\n",
    "        test_loss = [d[\"training_loss\"] for d in epoch_over_testing_loss if d[\"k\"] == k and d[\"frozen\"] == frozen_status]\n",
    "        plt.plot(epochs, train_loss, label=f\"Train k={k}\")\n",
    "        plt.plot(epochs, test_loss, '--', label=f\"Test k={k}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title + \": Epoch vs. Loss (per k)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e40d68a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10_Encoder(CIFAR10_Classifier):\n",
    "    def __init__(self, C_dropout, F_dropout, embedding_size=2):\n",
    "        super().__init__(C_dropout, F_dropout)\n",
    "\n",
    "        self.output_layer = nn.Linear(in_features=self.output_nodes, out_features=embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.partial_forward(x)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d348d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveCIFAR10(data.Dataset):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.labels = np.array(self.base_dataset.targets)\n",
    "\n",
    "        # Pre-compute a dictionary mapping each class to a list of its indices\n",
    "        self.labels_to_indices = {label: np.where(self.labels == label)[0]\n",
    "                                  for label in set(self.labels)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the anchor image and its label\n",
    "        img1, label1 = self.base_dataset[index]\n",
    "\n",
    "        # Decide whether to sample a positive or negative pair (50% chance)\n",
    "        is_similar = random.random() > 0.5\n",
    "\n",
    "        if is_similar:\n",
    "            # Positive pair: sample another image from the same class\n",
    "            positive_indices = self.labels_to_indices[label1]\n",
    "            # Make sure we don't pick the same image\n",
    "            positive_index = index\n",
    "            while positive_index == index:\n",
    "                positive_index = np.random.choice(positive_indices)\n",
    "            img2, _ = self.base_dataset[positive_index]\n",
    "            similarity = 1.0 # Similarity label is 1 for positive pairs\n",
    "        else:\n",
    "            # Negative pair: sample an image from a different class\n",
    "            negative_label = np.random.choice(list(set(self.labels) - {label1}))\n",
    "            negative_indices = self.labels_to_indices[negative_label]\n",
    "            negative_index = np.random.choice(negative_indices)\n",
    "            img2, _ = self.base_dataset[negative_index]\n",
    "            similarity = 0.0 # Similarity label is 0 for negative pairs\n",
    "\n",
    "        return img1, img2, torch.tensor(similarity, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf11b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, embedding1, embedding2, similarity_label):\n",
    "        # Calculate the euclidean distance squared between the embeddings\n",
    "        euclidean_distance = F.pairwise_distance(embedding1, embedding2, keepdim=True)\n",
    "        dist_sq = torch.pow(euclidean_distance, 2)\n",
    "\n",
    "        # Loss for similar pairs (S=1): we want their distance to be small\n",
    "        loss_similar = similarity_label * dist_sq\n",
    "\n",
    "        # Loss for dissimilar pairs (S=0): we want their distance to be large, at least > margin\n",
    "        # The loss is max(0, margin - distance)^2\n",
    "        loss_dissimilar = (1 - similarity_label) * torch.pow(\n",
    "            torch.clamp(self.margin - euclidean_distance, min=0.0), 2\n",
    "        )\n",
    "\n",
    "        # Combine the losses and average over the batch\n",
    "        total_loss = torch.mean(loss_similar + loss_dissimilar)\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea658b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_embeddings(model, loader):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            embs = model(images).cpu().numpy()\n",
    "            embeddings.append(embs)\n",
    "            all_labels.append(labels.numpy())\n",
    "    return np.concatenate(embeddings), np.concatenate(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b755285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(embeddings, labels, title=\"\"):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    class_names = trainset_full_CIFAR10.classes\n",
    "    for i in range(len(class_names)):\n",
    "        # Select embeddings that correspond to the current class\n",
    "        inds = np.where(labels == i)[0]\n",
    "        plt.scatter(embeddings[inds, 0], embeddings[inds, 1], alpha=0.5, label=class_names[i])\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae054d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Starting training of CIFAR10 embedding model ##########\n",
      "----- Epoch: 1/30 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/782 [00:00<?, ? batchs/s]"
     ]
    }
   ],
   "source": [
    "# Hyperparameter setup\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "learning_rate = 7.5e-5\n",
    "decay_rate = 5e-4\n",
    "\n",
    "c_dropout = 0.30\n",
    "f_dropout = 0.62\n",
    "\n",
    "embedding_model = CIFAR10_Encoder(c_dropout, f_dropout).to(device)\n",
    "contrastive_loss_fn = ContrastiveLoss(margin=1.0)\n",
    "optimizer = optim.Adam(embedding_model.parameters(), lr=learning_rate, weight_decay=decay_rate)\n",
    "\n",
    "contrastive_train_ds = ContrastiveCIFAR10(trainset_full_CIFAR10)\n",
    "contrastive_train_loader = data.DataLoader(contrastive_train_ds, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True)\n",
    "\n",
    "print(f'######## Starting training of CIFAR10 embedding model ##########')\n",
    "\n",
    "embedding_model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for img1, img2, sim in tqdm(contrastive_train_loader, desc='Training', unit=' batchs'):\n",
    "        img1, img2, sim = img1.to(device), img2.to(device), sim.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        emb1, emb2 = embedding_model(img1), embedding_model(img2)\n",
    "        loss = contrastive_loss_fn(emb1, emb2, sim)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"  -> [Contrastive] Epoch {epoch+1}/3 | Loss={running_loss/len(contrastive_train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_vis = data.DataLoader(testset_full_CIFAR10, batch_size=256, shuffle=False)\n",
    "\n",
    "final_embeddings, final_labels = get_all_embeddings(embedding_model, test_loader_vis)\n",
    "plot_embeddings(final_embeddings, final_labels, \"Embeddings After Training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
