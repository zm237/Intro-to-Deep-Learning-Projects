{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b3234c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for this project\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ea66ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the MNIST dataset\n",
    "training_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "testing_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "040f5af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "The system GPU | NVIDIA GeForce RTX 4080 | is AVAILABLE\n"
     ]
    }
   ],
   "source": [
    "# Verify that GPU is connected and available\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f'The system GPU | {torch.cuda.get_device_name(0) if not None else 'Error'} | is {'AVAILABLE' if torch.cuda.is_available() else 'NOT AVAILABLE' }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2c0c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Auto_Encoder(nn.Module):\n",
    "    def __init__(self, bottleneck, hidden_layer_size=784):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Defined number of features for the image\n",
    "        input_size=784\n",
    "\n",
    "        # Encoder section\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=input_size, out_features=hidden_layer_size), # Input to hidden\n",
    "            nn.ReLU(inplace=True),                                             # Activation Function\n",
    "            nn.Linear(in_features=hidden_layer_size, out_features=bottleneck)  # Hidden to bottleneck\n",
    "        )\n",
    "\n",
    "        # Decoder Section\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=bottleneck, out_features=hidden_layer_size), # Bottleneck to hidden\n",
    "            nn.ReLU(inplace=True),                                             # Activation Function\n",
    "            nn.Linear(in_features=hidden_layer_size, out_features=input_size), # Hidden to output\n",
    "            nn.Sigmoid(inplace=True)                                           # Force output pixels to be between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        \n",
    "        # Extract batch size and resize input into the nn\n",
    "        batch_size = x.size(0)\n",
    "        x_flattened = x.view(batch_size, -1)\n",
    "\n",
    "        # Encode then decode the input\n",
    "        x_encoded = self.encoder(x_flattened)\n",
    "        x_decoded = self.decoder(x_encoded)\n",
    "\n",
    "        # Reshape decoded x for final result\n",
    "        x_predicted = x_decoded.view(batch_size, 1, 28, 28)\n",
    "        \n",
    "        return x_predicted\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63aac2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the data for this test\n",
    "k_over_training_loss = []\n",
    "k_over_testing_loss = [] \n",
    "\n",
    "\n",
    "epoch_over_training_loss = []\n",
    "epoch_over_testing_loss = []\n",
    "\n",
    "'''\n",
    "Data for the graph will be in the form of\n",
    "{\n",
    "    k: int\n",
    "    final_training/testing_loss: float\n",
    "}\n",
    "'''\n",
    "\n",
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144822a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Question specific hyperparameters\n",
    "hidden_layer_size = 1000\n",
    "k_start = 748\n",
    "k_end = 0\n",
    "k_step = 160\n",
    "\n",
    "# Loop through each bottlenecks\n",
    "for k in range(k_start, k_end, k_step):\n",
    "    \n",
    "    print(f'########## With bottleneck = {k} ############')\n",
    "    \n",
    "    train_loader = data.DataLoader(training_set, \n",
    "                                    batch_size=batch_size, \n",
    "                                    shuffle=True,\n",
    "                                    num_workers=5, \n",
    "                                    pin_memory=True\n",
    "                               )\n",
    "    \n",
    "    test_loader = data.DataLoader(testing_set, \n",
    "                                    batch_size=batch_size, \n",
    "                                    shuffle=False,\n",
    "                                    num_workers=5, \n",
    "                                    pin_memory=True\n",
    "                               )\n",
    "\n",
    "    model = MNIST_Auto_Encoder(k, hidden_layer_size=hidden_layer_size)\n",
    "    model.to(device)\n",
    "\n",
    "    total_params, trainable_params = count_parameters(model)\n",
    "\n",
    "    print(f' ~~ Total/Trainable Parameters: {total_params}')\n",
    "\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Have references to variables outside of the epoch loop\n",
    "    avg_training_loss = 0\n",
    "    avg_testing_loss = 0\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    # Epoch Loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "\n",
    "        # Per epoch reset accumulated loss\n",
    "        avg_training_loss = 0\n",
    "        avg_testing_loss = 0\n",
    "\n",
    "        # Run through each batch\n",
    "        for images, _ in tqdm(train_loader, desc='Training', unit=' batch'):\n",
    "            # Move data into the GPU\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Zero out gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calc loss\n",
    "            generated_images = model(images)\n",
    "            loss = loss_function(generated_images, images)\n",
    "\n",
    "            # Calc gradient and step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                avg_training_loss += loss.item()\n",
    "\n",
    "        # Set up for calculating testing loss\n",
    "        avg_training_loss /=  len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        # Calc training loss\n",
    "        with torch.no_grad():\n",
    "            for test_images, _ in tqdm(test_loader, desc='Testing', unit=' batch'):\n",
    "                test_images = test_images.to(device)\n",
    "                generated_test_images = model(test_images)\n",
    "                \n",
    "                avg_testing_loss += loss_function(generated_test_images, test_images).item()\n",
    "\n",
    "        avg_testing_loss /=  len(test_loader)\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        print(\"\")\n",
    "\n",
    "        print(f'   -> Training Loss: {avg_training_loss: .4f}\\n')\n",
    "        print(f'   -> Testing Loss: {avg_testing_loss: .4f}\\n')\n",
    "\n",
    "    # Add end results to the lists\n",
    "    k_over_training_loss.append({\n",
    "        \"k\": k,\n",
    "        \"training_loss\": avg_training_loss \n",
    "    })\n",
    "\n",
    "    k_over_testing_loss.append({\n",
    "        \"k\": k,\n",
    "        \"testing_loss\": avg_testing_loss\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5446db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [d[\"k\"] for d in k_over_training_loss]\n",
    "train_loss = [d[\"training_loss\"] for d in k_over_training_loss]\n",
    "test_loss = [d[\"testing_loss\"] for d in k_over_testing_loss]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(ks, train_loss, label=\"Training Loss\")\n",
    "plt.plot(ks, test_loss, label=\"Testing Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"MNIST AutoEncoder: Bottleneck vs. Training/Testing Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47bdb94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor of the data will be....\\n\\ndata = {\\n    epoch: int\\n    training/testing loss: float\\n    model_name: str\\n}\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def freeze_module(m):\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "def unfreeze_module(m):\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "epoch_over_training_loss_per_model = []\n",
    "epoch_over_testing_loss_per_model = []\n",
    "\n",
    "'''\n",
    "For of the data will be....\n",
    "\n",
    "data = {\n",
    "    epoch: int\n",
    "    training/testing loss: float\n",
    "    model_name: str\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f262febb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m optimal_k = \u001b[32m10\u001b[39m \u001b[38;5;66;03m# PLACEHOLDER\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Have reference\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m src_model = \u001b[43mmodel\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# 1. Untrained encoder not frozen\u001b[39;00m\n\u001b[32m      7\u001b[39m encoder_clean_unfrozen = MNIST_Auto_Encoder(optimal_k, hidden_layer_size=hidden_layer_size).to(device).encoder\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "optimal_k = 10 # PLACEHOLDER\n",
    "\n",
    "# Have reference\n",
    "src_model = model\n",
    "\n",
    "# 1. Untrained encoder not frozen\n",
    "encoder_clean_unfrozen = MNIST_Auto_Encoder(optimal_k, hidden_layer_size=hidden_layer_size).to(device).encoder\n",
    "\n",
    "# 2. Untrained encoder but frozen\n",
    "encoder_clean_frozen = MNIST_Auto_Encoder(optimal_k, hidden_layer_size=hidden_layer_size).to(device).encoder\n",
    "freeze_module(encoder_clean_frozen)\n",
    "unfreeze_module(encoder_clean_frozen[-1])\n",
    "\n",
    "# 3. Trained encoder not frozen\n",
    "encoder_trained_unfrozen = MNIST_Auto_Encoder(optimal_k, hidden_layer_size=hidden_layer_size).to(device).encoder\n",
    "encoder_trained_unfrozen.load_state_dict(src_model.encoder.state_dict())\n",
    "\n",
    "# 4. Trained encoder but frozen\n",
    "encoder_trained_frozen = MNIST_Auto_Encoder(optimal_k, hidden_layer_size=hidden_layer_size).to(device).encoder\n",
    "encoder_trained_frozen.load_state_dict(src_model.encoder.state_dict())\n",
    "\n",
    "freeze_module(encoder_trained_frozen)\n",
    "unfreeze_module(encoder_trained_frozen[-1])\n",
    "\n",
    "models = [encoder_clean_unfrozen, encoder_clean_unfrozen, encoder_trained_unfrozen, encoder_trained_frozen]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ceeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "\n",
    "print(f'######## Begin Training Encoders on bottleneck={optimal_k} ########')\n",
    "\n",
    "\n",
    "train_loader = data.DataLoader(training_set, \n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=True,\n",
    "                                num_workers=5, \n",
    "                                pin_memory=True\n",
    "                            )\n",
    "\n",
    "test_loader = data.DataLoader(testing_set, \n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=False,\n",
    "                                num_workers=5, \n",
    "                                pin_memory=True\n",
    "                            )\n",
    "\n",
    "for sample_model, model_name in zip(models, ['Unfrozen Clean Encoder', 'Frozen Clean Encoder', 'Unfrozen Pretrained Encoder', 'Frozen Pretrained Encoder']):\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(sample_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    avg_training_loss = 0\n",
    "    avg_testing_loss = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "\n",
    "        # Per epoch reset accumulated loss\n",
    "        avg_training_loss = 0\n",
    "        avg_testing_loss = 0\n",
    "\n",
    "        # Run through each batch\n",
    "        for images, labels in tqdm(train_loader, desc='Training', unit=' batch'):\n",
    "            # Move data into the GPU\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Zero out gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calc loss\n",
    "            logits = sample_model(images)\n",
    "            loss = loss_function(logits, labels)\n",
    "\n",
    "            # Calc gradient and step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                avg_training_loss += loss.item()\n",
    "\n",
    "        # Set up for calculating testing loss\n",
    "        avg_training_loss /=  len(train_loader)\n",
    "\n",
    "        sample_model.eval()\n",
    "\n",
    "        # Calc training loss\n",
    "        with torch.no_grad():\n",
    "            for test_images, labels in tqdm(test_loader, desc='Testing', unit=' batch'):\n",
    "                test_images = test_images.to(device)\n",
    "                logits = sample_model(test_images)\n",
    "                \n",
    "                avg_testing_loss += loss_function(logits, labels).item()\n",
    "\n",
    "        avg_testing_loss /=  len(test_loader)\n",
    "\n",
    "        sample_model.train()\n",
    "        \n",
    "        print(\"\")\n",
    "\n",
    "        print(f'   -> Training Loss: {avg_training_loss: .4f}\\n')\n",
    "        print(f'   -> Testing Loss: {avg_testing_loss: .4f}\\n')\n",
    "\n",
    "        # Add end results to the lists\n",
    "        epoch_over_training_loss_per_model.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"training_loss\": avg_training_loss,\n",
    "            \"model_name\": model_name\n",
    "        })\n",
    "\n",
    "        epoch_over_testing_loss_per_model.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"training_loss\": avg_training_loss,\n",
    "            \"model_name\": model_name\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2150c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f663ff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset_full_CIFAR10 = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=tfm)\n",
    "testset_full_CIFAR10  = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8295131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10_Classifier(nn.Module):\n",
    "    def __init__(self, C_dropout, F_dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        conv2d_dropout = C_dropout\n",
    "\n",
    "        conv_layer_1 = 30\n",
    "        conv_layer_2 = 64\n",
    "        \n",
    "        conv_layer_3 = 128\n",
    "        conv_layer_4 = 256\n",
    "\n",
    "        self.forward_funnel_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=conv_layer_1, kernel_size=5),   # Extract useful features from the beginning\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(conv2d_dropout),\n",
    "\n",
    "            nn.Conv2d(in_channels=conv_layer_1, out_channels=conv_layer_2, kernel_size=3),  # Extract useful features from the learned features\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(conv2d_dropout),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),                       # Reduce dimensionality\n",
    "        )\n",
    "\n",
    "        self.forward_funnel_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=conv_layer_2, out_channels=conv_layer_3, kernel_size=3),   # Extract useful features from the beginning\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(conv2d_dropout),\n",
    "\n",
    "            nn.Conv2d(in_channels=conv_layer_3, out_channels=conv_layer_4, kernel_size=3),  # Extract useful features from the learned features\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout2d(conv2d_dropout),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # Compute the number of features after the input has passed the funnel\n",
    "        with torch.no_grad():\n",
    "            test_input = torch.zeros(1, 3, 32, 32)\n",
    "\n",
    "            test_input.to(device)\n",
    "\n",
    "            features = self.forward_funnel_1(test_input)\n",
    "            features = self.forward_funnel_2(features)\n",
    "\n",
    "            total_count = features.view(1, -1).size(1)\n",
    "\n",
    "        full_node_dropout = F_dropout\n",
    "        self.output_nodes = 100\n",
    "\n",
    "        self.classifer = nn.Sequential(\n",
    "            nn.Flatten(),                                           # Flatten the image from the funnel\n",
    "            nn.Linear(in_features=total_count, out_features=1000), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(full_node_dropout),\n",
    "\n",
    "            nn.Linear(in_features=1000, out_features=500),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(full_node_dropout),\n",
    "\n",
    "            nn.Linear(in_features=500, out_features=250),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(full_node_dropout),\n",
    "\n",
    "            nn.Linear(in_features=250, out_features=self.output_nodes),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(full_node_dropout),\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(in_features=self.output_nodes, out_features=10)\n",
    "    \n",
    "    def partial_forward(self, x):\n",
    "        x = self.forward_funnel_1(x)\n",
    "        x = self.forward_funnel_2(x)\n",
    "        x = self.classifer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.partial_forward(x)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "d588522c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nForm of the data\\n\\ndata = \\n{\\n    epoch: int\\n    training/testing loss: float\\n}\\n'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_over_training_loss_CIFAR10 = []\n",
    "epoch_over_testing_loss_CIFAR10 = []\n",
    "\n",
    "'''\n",
    "Form of the data\n",
    "\n",
    "data = \n",
    "{\n",
    "    epoch: int\n",
    "    training/testing loss: float\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b6d8d1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Begining training for CIFAR10 classifier ##########\n",
      "----- Epoch: 1/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/782 [00:00<?, ? batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 75.09 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 120.58 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.9963\n",
      "\n",
      "   -> Testing Loss:  1.6910\n",
      "\n",
      "----- Epoch: 2/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 75.36 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 108.10 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.6505\n",
      "\n",
      "   -> Testing Loss:  1.4490\n",
      "\n",
      "----- Epoch: 3/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 76.09 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 120.63 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.4962\n",
      "\n",
      "   -> Testing Loss:  1.3172\n",
      "\n",
      "----- Epoch: 4/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 76.74 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 108.10 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.3862\n",
      "\n",
      "   -> Testing Loss:  1.1961\n",
      "\n",
      "----- Epoch: 5/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 74.87 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 114.97 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.2924\n",
      "\n",
      "   -> Testing Loss:  1.1504\n",
      "\n",
      "----- Epoch: 6/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 76.60 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 119.60 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.2183\n",
      "\n",
      "   -> Testing Loss:  1.0517\n",
      "\n",
      "----- Epoch: 7/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 77.46 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 123.26 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.1488\n",
      "\n",
      "   -> Testing Loss:  0.9975\n",
      "\n",
      "----- Epoch: 8/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 77.39 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 120.05 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0851\n",
      "\n",
      "   -> Testing Loss:  0.9811\n",
      "\n",
      "----- Epoch: 9/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 76.37 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 119.96 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0437\n",
      "\n",
      "   -> Testing Loss:  0.9334\n",
      "\n",
      "----- Epoch: 10/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 76.47 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 121.19 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9998\n",
      "\n",
      "   -> Testing Loss:  0.8996\n",
      "\n",
      "----- Epoch: 11/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 76.88 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 120.40 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9507\n",
      "\n",
      "   -> Testing Loss:  0.8822\n",
      "\n",
      "----- Epoch: 12/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 75.94 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 120.02 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9130\n",
      "\n",
      "   -> Testing Loss:  0.8465\n",
      "\n",
      "----- Epoch: 13/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 75.92 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 113.41 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.8801\n",
      "\n",
      "   -> Testing Loss:  0.8275\n",
      "\n",
      "----- Epoch: 14/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 75.80 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 115.06 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.8455\n",
      "\n",
      "   -> Testing Loss:  0.7980\n",
      "\n",
      "----- Epoch: 15/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 75.63 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 114.46 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.8014\n",
      "\n",
      "   -> Testing Loss:  0.7995\n",
      "\n",
      "----- Epoch: 16/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:10<00:00, 76.31 batch/s]\n",
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 120.17 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.7743\n",
      "\n",
      "   -> Testing Loss:  0.8110\n",
      "\n",
      "----- Epoch: 17/50 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  89%|████████▉ | 699/782 [00:09<00:01, 73.61 batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     43\u001b[39m avg_testing_loss = \u001b[32m0\u001b[39m\n\u001b[32m     45\u001b[39m model.train()\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainset_loader_CIFAR10\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTraining\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m batch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Transfer images to GPU\u001b[39;49;00m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mY\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001b[39m, in \u001b[36mCIFAR10.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    116\u001b[39m img = Image.fromarray(img)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    122\u001b[39m     target = \u001b[38;5;28mself\u001b[39m.target_transform(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, pic)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[32m    130\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torchvision\\transforms\\functional.py:172\u001b[39m, in \u001b[36mto_tensor\u001b[39m\u001b[34m(pic)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pic.mode == \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    171\u001b[39m     img = \u001b[32m255\u001b[39m * img\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m img = \u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpic\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF_pil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_image_num_channels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[32m    174\u001b[39m img = img.permute((\u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m)).contiguous()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameter setup\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "learning_rate = 5e-4\n",
    "decay_rate = 4e-4\n",
    "\n",
    "c_dropout = 0.25\n",
    "f_dropout = 0.25\n",
    "\n",
    "print('######## Begining training for CIFAR10 classifier ##########')\n",
    "\n",
    "# Setup data loaders\n",
    "trainset_loader_CIFAR10 = data.DataLoader(trainset_full_CIFAR10,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   # num_workers=5,\n",
    "                                   pin_memory=True)\n",
    "\n",
    "testset_loader_CIFAR10 = data.DataLoader(testset_full_CIFAR10,\n",
    "                                   batch_size=batch_size,\n",
    "                                   # num_workers=5,\n",
    "                                   shuffle=False,\n",
    "                                   pin_memory=True)\n",
    "\n",
    "model = CIFAR10_Classifier(c_dropout, f_dropout)\n",
    "model.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                       lr=learning_rate, \n",
    "                       weight_decay=decay_rate\n",
    "                       )\n",
    "\n",
    "# Have references to variables outside of the epoch loop\n",
    "avg_training_loss = 0\n",
    "avg_testing_loss = 0\n",
    "\n",
    "# Epoch Loop\n",
    "for epoch in range(epochs):\n",
    "    print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "    \n",
    "    avg_training_loss = 0\n",
    "    avg_testing_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for x, Y in tqdm(trainset_loader_CIFAR10, desc='Training', unit=' batch'):\n",
    "        # Transfer images to GPU\n",
    "        x = x.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Send images to model\n",
    "        x_pred = model(x)\n",
    "\n",
    "        # Calc loss\n",
    "        loss = loss_function(x_pred, Y)\n",
    "\n",
    "        # Calc gradient and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            avg_training_loss += loss.item()\n",
    "\n",
    "    # Switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, Y in tqdm(testset_loader_CIFAR10, desc='Testing', unit=' batches'):\n",
    "            # Move the images to the GPU\n",
    "            x = x.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            # Get logits and sum up total loss\n",
    "            x_pred = model(x)\n",
    "            avg_testing_loss += loss_function(x_pred, Y).item()\n",
    "\n",
    "    # Get training loss\n",
    "    avg_training_loss /= len(trainset_loader_CIFAR10)\n",
    "\n",
    "     # Get testing loss\n",
    "    avg_testing_loss /= len(testset_loader_CIFAR10)\n",
    "\n",
    "    # Switch model back to training mode\n",
    "    model.train()\n",
    "\n",
    "    epoch_over_training_loss_CIFAR10.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"training_loss\": avg_training_loss\n",
    "        })\n",
    "    \n",
    "    epoch_over_testing_loss_CIFAR10.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"testing_loss\": avg_testing_loss\n",
    "        })\n",
    "    \n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    print(f'   -> Training Loss: {avg_training_loss: .4f}\\n')\n",
    "    print(f'   -> Testing Loss: {avg_testing_loss: .4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75e63add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.6757\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in testset_loader_CIFAR10:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        pred = logits.argmax(1)\n",
    "        correct += (pred == yb).sum().item()\n",
    "        total += yb.numel()\n",
    "\n",
    "final_accuracy = float(correct / total)\n",
    "\n",
    "print(f\"Final Accuracy: {final_accuracy}\")\n",
    "\n",
    "epochs = [d[\"epoch\"] for d in epoch_over_training_loss_CIFAR10]\n",
    "train_loss = [d[\"training_loss\"] for d in epoch_over_training_loss_CIFAR10]\n",
    "test_loss = [d[\"testing_loss\"] for d in epoch_over_testing_loss_CIFAR10]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_loss, label=\"Training Loss\")\n",
    "plt.plot(epochs, test_loss, label=\"Testing Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"CIFAR10 Classifer: Training and Testing Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cce761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d146c3dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019c18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a605f5ad",
   "metadata": {},
   "source": [
    "RESULTS.....\n",
    "\n",
    "\n",
    "#### Model Architecture 1\n",
    "\n",
    "In the beginning, I decided that I wanted to have two convolutional layers, one that extracted a lot of features from the image data and one that found more advanced features. Since the first layer would find as many initial features to start, I decided to make the out channel 128 and kernel size 5x5. Then, the second layer would condense those features, so I chose an out channel size of 64 and kernel size of 3x3. Additionally, I stacked 4 decreasing, funneling, fully-connected layers, 1000 -> 1000 -> 500 -> 100, that finally connected to the output layer. My reasoning was that doing this would allow me to find complex features that would become more and more concentrated and meaningful. Finally, I added a max pooling layer after the convolutional layers to help reduce dimensionality of the data and dropout layers after every layer to prevent overfitting. \n",
    "\n",
    "#### Optimizer\n",
    "I went with Adam since I wanted to experiment with using different options. Furthermore, I wanted my loss to be as low as possible, so I wanted to use weight decay. As such, the final optimizer I used was AdamW since it handles weight decay better.\n",
    "\n",
    "#### Initial training\n",
    "\n",
    "I started with initial hyperparamters of batch_size = 64, learning_rate = 1e-3, decay_rate = 5e-4 (weight decay), c_dropout = 0.40 (the dropout probability for each convolutional layer), and f_dropout = 0.60 (the dropout probability for each fully-connected layer). After the first training, I noticed that my losses were not really good for this data set, training loss ~= 1.3 and testing loss ~= 1.25. Specially, they could never get past the 1.2 barrier and started to stagnate at the 14th epoch. Since I did not think overfitting was an issue yet, I decided to tweak the learning rate from 1e-3 to 5e-4.\n",
    "\n",
    "In the second trial, I noticed that the issue of the model not being able to learn was still present, losses similar to the first trial, so I thought the maybe my dropout p's were a bit too high. This caused me to set both of them to 0.4.\n",
    "\n",
    "In my next training attempt, I saw I was able to break past the 1.2 loss barrier by the 10th epoch which solidified my reasoning that my p's were too high, but I was unable to get a testing loss lower than ~1.10 after the 15th epoch. However, I did notice that model was still learning well, it was able to get training loss lower than 1.10, so I decided that I should try changing the weight decay to 4e-4 to see if it would help. I did not increase the weight decay since I thought that would cause my losses to be greater since I was already using a good amount of dropout.\n",
    "\n",
    "For the fourth trial, I saw that I got similar testing loss, ~1.08, so I decided to try changing the other hyperparameters. Yet, no matter which ones I tried to change, I was unable to break the testing loss = 1.00 barrier, even though the model was learning well beyond that point. This led me to think that my architecture was a bit too complex, so I decided to rethink of what I was doing and change my model.\n",
    "\n",
    "#### Model Architecture 2\n",
    "\n",
    "For this iteration of the model, I decided to change the number of channels in my convolutional layers from 128 -> 64 to 30 -> 64. This happened because I decided to look at the image size which is 32x32 and realized that I might be trying to find too many features in the beginning but then sampling them down too much in the second layer. Instead, it would probably be better to find fewer initial features and then find complex relationships between them in the second convolutional layer. The second major change I made was making the fully-connected layers continously decreasing from 1000 -> 1000 -> 500 -> 100 to 1000 -> 500 -> 250 -> 100 since I wanted to downscale the model.\n",
    "\n",
    "#### Training for Second Model\n",
    "\n",
    "I decided to keep the hyperparameters that I obtained from the initial model since I thought they might work well with this new model, batch_size = 64, learning_rate = 5e-4, and decay_rate = 4e-4. However, I chose to decrease the dropouts both to 0.25 since I wanted to start at a new, lower point and tweak the dropout as I trained this new model.\n",
    "\n",
    "For the first training attempt, my model performed way better than the first model both in terms of speed and accuracy since it was able to get training loss ~= 0.9 and testing loss ~= 0.89 at the 5th epoch. Beyond that point though, the model started overfitting severally and could not get past the testing loss being ~0.89. Instead of trying\n",
    "\n",
    "#### Model Architecture 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ccfc756",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10_Transformed_Classifier(CIFAR10_Classifier):\n",
    "    def __init__(self, C_dropout, F_dropout):\n",
    "        super().__init__(C_dropout, F_dropout)\n",
    "        \n",
    "        self.output_layer = nn.Linear(in_features=self.output_nodes, out_features=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.partial_forward(x)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50be51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_over_training_loss_CIFAR10_R = []\n",
    "epoch_over_testing_loss_CIFAR10_R = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bfa844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter setup\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "decay_rate = 1e-4\n",
    "\n",
    "c_dropout = 0.40\n",
    "f_dropout = 0.50\n",
    "\n",
    "\n",
    "print('######## Begining training for CIFAR10 classifier on rotated images ##########')\n",
    "\n",
    "# Setup data loaders\n",
    "trainset_loader_CIFAR10_R = data.DataLoader(trainset_full_CIFAR10,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   # num_workers=5,\n",
    "                                   pin_memory=True)\n",
    "\n",
    "testset_loader_CIFAR10_R = data.DataLoader(testset_full_CIFAR10,\n",
    "                                   batch_size=batch_size,\n",
    "                                   # num_workers=5,\n",
    "                                   shuffle=False,\n",
    "                                   pin_memory=True)\n",
    "\n",
    "model = CIFAR10_Transformed_Classifier(c_dropout, f_dropout)\n",
    "model.to(device)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                       lr=learning_rate, \n",
    "                       weight_decay=decay_rate\n",
    "                       )\n",
    "\n",
    "# Have references to variables outside of the epoch loop\n",
    "avg_training_loss = 0\n",
    "avg_testing_loss = 0\n",
    "\n",
    "# Epoch Loop\n",
    "for epoch in range(epochs):\n",
    "    print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "    \n",
    "    avg_training_loss = 0\n",
    "    avg_testing_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for x, _ in tqdm(trainset_loader_CIFAR10_R, desc='Training', unit=' batch'):\n",
    "\n",
    "        labels_upright = torch.zeros(x.size(0), dtype=torch.long)\n",
    "\n",
    "        images_rotated = torch.rot90(x, 1, [2, 3])\n",
    "        labels_rotated = torch.ones(x.size(0), dtype=torch.long)\n",
    "\n",
    "        all_images = torch.cat([x, images_rotated])\n",
    "        all_labels = torch.cat([labels_upright, labels_rotated])\n",
    "        \n",
    "        \n",
    "        # Transfer images to GPU\n",
    "        all_images = all_images.to(device)\n",
    "        all_labels = all_labels.to(device)\n",
    "\n",
    "        # Zero out gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Send images to model\n",
    "        x_pred = model(all_images)\n",
    "\n",
    "        # Calc loss\n",
    "        loss = loss_function(x_pred, all_labels)\n",
    "\n",
    "        # Calc gradient and update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            avg_training_loss += loss\n",
    "\n",
    "    # Switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in tqdm(testset_loader_CIFAR10_R, desc='Testing', unit=' batches'):\n",
    "            \n",
    "            labels_upright = torch.zeros(x.size(0), dtype=torch.long)\n",
    "\n",
    "            images_rotated = torch.rot90(x, 1, [2, 3])\n",
    "            labels_rotated = torch.ones(x.size(0), dtype=torch.long)\n",
    "\n",
    "            all_images = torch.cat([x, images_rotated])\n",
    "            all_labels = torch.cat([labels_upright, labels_rotated])\n",
    "            \n",
    "            # Transfer images to GPU\n",
    "            all_images = all_images.to(device)\n",
    "            all_labels = all_labels.to(device)\n",
    "            \n",
    "            # Move the images to the GPU\n",
    "            all_images = all_images.to(device)\n",
    "            all_labels = all_labels.to(device)\n",
    "\n",
    "            # Get logits and sum up total loss\n",
    "            x_pred = model(all_images)\n",
    "            avg_testing_loss += loss_function(x_pred, all_labels).item()\n",
    "\n",
    "    # Get training loss\n",
    "    avg_training_loss /= len(trainset_loader_CIFAR10)\n",
    "\n",
    "     # Get testing loss\n",
    "    avg_testing_loss /= len(testset_loader_CIFAR10)\n",
    "\n",
    "    # Switch model back to training mode\n",
    "    model.train()\n",
    "\n",
    "    epoch_over_training_loss_CIFAR10_R.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"training_loss\": avg_training_loss\n",
    "        })\n",
    "    \n",
    "    epoch_over_testing_loss_CIFAR10_R.append({\n",
    "        \"epoch\": epoch,\n",
    "        \"testing_loss\": avg_testing_loss\n",
    "        })\n",
    "    \n",
    "\n",
    "    print(\"\")\n",
    "\n",
    "    print(f'   -> Training Loss: {avg_training_loss: .4f}\\n')\n",
    "    print(f'   -> Testing Loss: {avg_testing_loss: .4f}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3443e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights from problem 5\n",
    "torch.save(model.state_dict(), 'Q4_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c9ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for x, _ in testset_loader_CIFAR10_R:\n",
    "        \n",
    "        labels_upright = torch.zeros(x.size(0), dtype=torch.long)\n",
    "\n",
    "        images_rotated = torch.rot90(x, 1, [2, 3])\n",
    "        labels_rotated = torch.ones(x.size(0), dtype=torch.long)\n",
    "\n",
    "        all_images = torch.cat([x, images_rotated])\n",
    "        all_labels = torch.cat([labels_upright, labels_rotated])\n",
    "        \n",
    "        # Transfer images to GPU\n",
    "        all_images = all_images.to(device)\n",
    "        all_labels = all_labels.to(device)\n",
    "        \n",
    "        # Move the images to the GPU\n",
    "        all_images = all_images.to(device)\n",
    "        all_labels = all_labels.to(device)\n",
    "\n",
    "        # Get logits and sum up total loss\n",
    "        logits = model(all_images)\n",
    "        \n",
    "        pred = logits.argmax(1)\n",
    "        \n",
    "        correct += (pred == all_labels).sum().item()\n",
    "        total += all_labels.numel()\n",
    "\n",
    "final_accuracy = float(correct / total)\n",
    "\n",
    "print(f\"Final Accuracy: {final_accuracy}\")\n",
    "\n",
    "\n",
    "epochs = [d[\"epoch\"] for d in epoch_over_training_loss_CIFAR10_R]\n",
    "train_loss = [d[\"training_loss\"] for d in epoch_over_training_loss_CIFAR10_R]\n",
    "test_loss = [d[\"testing_loss\"] for d in epoch_over_testing_loss_CIFAR10_R]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_loss, label=\"Training Loss\")\n",
    "plt.plot(epochs, test_loss, label=\"Testing Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"CIFAR10 Rotation Classifer: Training and Testing Loss per Epoch\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2396329",
   "metadata": {},
   "source": [
    "Results\n",
    "\n",
    "---\n",
    "\n",
    "Training Loss = \n",
    "\n",
    "Testing Loss = \n",
    "\n",
    "Final Testing Loss = \n",
    "\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "learning_rate = 1e-4\n",
    "\n",
    "decay_rate = 5e-4\n",
    "\n",
    "\n",
    "c_dropout = 0.40\n",
    "\n",
    "f_dropout = 0.50\n",
    "\n",
    "---\n",
    "\n",
    "Initially, I decided to start with the same hyperparameters as the final model in Question 3 just to see what the current losses were looking like. As a result, I was able to get a training and testing loss around 0.38. Though in this run, I decided to end the training early since I noticed that the model was overtraining starting from the 14th epoch and beyond.\n",
    "\n",
    "Noticing the issue of overtraining, I decided to switch to the AdamW optimizer since it handles weight decay better. However, I still got similar results at the 14th epoch. Instead of focusining on changing the optimizer, I decided to try and change the dropout percentages. \n",
    "\n",
    "In the third trial, I changed the dropout rates from 0.3, convolutional dropout, to 0.4 which allowed me to achieve a better training loss around 0.33 and testing loss around 0.31. During this run, I noticed that the model began to learn slower by the 10th epoch and then began overtraining by the 14th epoch. \n",
    "\n",
    "For the forth attempt, I tried to increase the learning rate from 7.5e-5 to 1e-4 and sadly did not see any better results by the time the model started overtraining at the 14 epoch, however, I was able to achieve similar losses around 0.31.\n",
    "\n",
    "On the fifth trial, I decided that the model learning rate was being affected by the dropout rate on the fully connected layer, so I decreased it to 0.50. This allowed me to get a training loss around 0.301 and testing loss around 0.297 at the 14th epoch. The decrease in loss was nice, but I still needed to deal with overtraining since the model could never get past this testing loss barrier at around 0.29.\n",
    "\n",
    "On the sixth run, I wanted to experiment with a higher weight decay rate, increased from 5e-4 to 7.5e-4, but this resulted in my model starting to overtrain at the 15th epoch with a min testing loss of 0.3032. Though, I still wanted to see if it would be wise to tinker with the decay rate, so I instead decreased it to 2.5e-4 on the 7th run which gave me the same issue of overtraining past the 15th epoch and a final testing loss of around 0.29.\n",
    "\n",
    "At this point, I believed that I have found the most optimal hyperparameters for my model based on the current architecture, so I did one final training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96a5475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_weights(model_final, model_src, k, is_frozen):\n",
    "    \n",
    "    for i in range(len(model_final.forward_funnel_1)):\n",
    "\n",
    "        if k == 0: return\n",
    "\n",
    "        src_layer = model_src.forward_funnel_1[i]\n",
    "        final_layer = model_final.forward_funnel_1[i]\n",
    "\n",
    "        if (hasattr(src_layer, 'weight') and hasattr(final_layer, 'weight')):\n",
    "            final_layer.weight.data = src_layer.weight.data.clone()\n",
    "            if is_frozen:\n",
    "                final_layer.weight.requires_grad = False\n",
    "            \n",
    "            # This will always run if we get here. I do not intend on making layers with biases\n",
    "            if (hasattr(src_layer, 'bias') and hasattr(final_layer, 'bias')):\n",
    "                final_layer.bias.data = src_layer.bias.data.clone()\n",
    "                if is_frozen:\n",
    "                    final_layer.bias.requires_grad = False\n",
    "\n",
    "                k -= 1\n",
    "\n",
    "    \n",
    "    for i in range(len(model_final.classifer)):\n",
    "        \n",
    "        if k == 0: return\n",
    "\n",
    "        src_layer = model_src.classifer[i]\n",
    "        final_layer = model_final.classifer[i]\n",
    "\n",
    "        if (hasattr(src_layer, 'weight') and hasattr(final_layer, 'weight')):\n",
    "            final_layer.weight.data = src_layer.weight.data.clone()\n",
    "            if is_frozen:\n",
    "                final_layer.weight.requires_grad = False\n",
    "            \n",
    "            # This will always run if we get here. I do not intend on making layers with biases\n",
    "            if (hasattr(src_layer, 'bias') and hasattr(final_layer, 'bias')):\n",
    "                final_layer.bias.data = src_layer.bias.data.clone()\n",
    "                if is_frozen:\n",
    "                    final_layer.bias.requires_grad = False\n",
    "\n",
    "                k -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da72851a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor of the data for this list is...\\n\\ndata = {\\n    k: int\\n    epoch: int\\n    training/testing loss: floats\\n    frozen: True or False\\n}\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_over_training_loss_CIFAR10_per_k = []\n",
    "epoch_over_testing_loss_CIFAR10_per_k = []\n",
    "\n",
    "'''\n",
    "For of the data for this list is...\n",
    "\n",
    "data = {\n",
    "    k: int\n",
    "    epoch: int\n",
    "    training/testing loss: floats\n",
    "    frozen: True or False\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1dea2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Q4_model_weights.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Initialize model for the sake of weight transfer\u001b[39;00m\n\u001b[32m     13\u001b[39m src_model = CIFAR10_Classifier(c_dropout, f_dropout)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m src_model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mQ4_model_weights.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m######## Begining training for CIFAR10 classifier + transfer learning ##########\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m current_k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, k, \u001b[32m1\u001b[39m):\n\u001b[32m     21\u001b[39m \n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# Setup data loaders\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects and Related Work\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torch\\serialization.py:1425\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1423\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1428\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1429\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1430\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects and Related Work\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torch\\serialization.py:751\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    750\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mukad\\Desktop\\Projects and Related Work\\Intro-to-Deep-Learning-Projects\\Project 2\\.venv\\Lib\\site-packages\\torch\\serialization.py:732\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'Q4_model_weights.pth'"
     ]
    }
   ],
   "source": [
    "# Hyperparameter setup\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "decay_rate = 1e-4\n",
    "\n",
    "c_dropout = 0.40\n",
    "f_dropout = 0.50\n",
    "\n",
    "k = 5 # Max number of layers for the current model ~~ Excluding the output layer\n",
    "\n",
    "# Initialize model for the sake of weight transfer\n",
    "src_model = CIFAR10_Transformed_Classifier(c_dropout, f_dropout)\n",
    "src_model.load_state_dict(torch.load('Q4_model_weights.pth'))\n",
    "\n",
    "\n",
    "print('######## Begining training for CIFAR10 classifier + transfer learning ##########')\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    is_frozen = False if i == 0 else True\n",
    "    \n",
    "    for current_k in range(1, k, 1):\n",
    "        # Setup data loaders\n",
    "        trainset_loader_CIFAR10 = data.DataLoader(trainset_full_CIFAR10,\n",
    "                                        batch_size=batch_size,\n",
    "                                        shuffle=True,\n",
    "                                        # num_workers=5,\n",
    "                                        pin_memory=True)\n",
    "\n",
    "        testset_loader_CIFAR10 = data.DataLoader(testset_full_CIFAR10,\n",
    "                                        batch_size=batch_size,\n",
    "                                        # num_workers=5,\n",
    "                                        shuffle=False,\n",
    "                                        pin_memory=True)\n",
    "\n",
    "        model_final = CIFAR10_Classifier(c_dropout, f_dropout)\n",
    "\n",
    "        transfer_weights(model_final, src_model, k, is_frozen)\n",
    "\n",
    "        model_final.to(device)\n",
    "\n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.AdamW(model_final.parameters(), \n",
    "                            lr=learning_rate, \n",
    "                            weight_decay=decay_rate\n",
    "                            )\n",
    "\n",
    "        # Have references to variables outside of the epoch loop\n",
    "        avg_training_loss = 0\n",
    "        avg_testing_loss = 0\n",
    "\n",
    "        # Epoch Loop\n",
    "        for epoch in range(epochs):\n",
    "            print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "            \n",
    "            avg_training_loss = 0\n",
    "            avg_testing_loss = 0\n",
    "\n",
    "            model_final.train()\n",
    "\n",
    "            for x, _ in tqdm(trainset_loader_CIFAR10, desc='Training', unit=' batch'):\n",
    "\n",
    "                labels_upright = torch.zeros(x.size(0), dtype=torch.long)\n",
    "\n",
    "                images_rotated = torch.rot90(x, 1, [2, 3])\n",
    "                labels_rotated = torch.ones(x.size(0), dtype=torch.long)\n",
    "\n",
    "                all_images = torch.cat([x, images_rotated])\n",
    "                all_labels = torch.cat([labels_upright, labels_rotated])\n",
    "                \n",
    "                \n",
    "                # Transfer images to GPU\n",
    "                all_images = all_images.to(device)\n",
    "                all_labels = all_labels.to(device)\n",
    "\n",
    "                # Zero out gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Send images to model\n",
    "                x_pred = model_final(all_images)\n",
    "\n",
    "                # Calc loss\n",
    "                loss = loss_function(x_pred, all_labels)\n",
    "\n",
    "                # Calc gradient and update weights\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    avg_training_loss += loss\n",
    "\n",
    "            # Switch to eval mode\n",
    "            model_final.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for x, _ in tqdm(testset_loader_CIFAR10, desc='Testing', unit=' batches'):\n",
    "                    \n",
    "                    labels_upright = torch.zeros(x.size(0), dtype=torch.long)\n",
    "\n",
    "                    images_rotated = torch.rot90(x, 1, [2, 3])\n",
    "                    labels_rotated = torch.ones(x.size(0), dtype=torch.long)\n",
    "\n",
    "                    all_images = torch.cat([x, images_rotated])\n",
    "                    all_labels = torch.cat([labels_upright, labels_rotated])\n",
    "                    \n",
    "                    # Transfer images to GPU\n",
    "                    all_images = all_images.to(device)\n",
    "                    all_labels = all_labels.to(device)\n",
    "                    \n",
    "                    # Move the images to the GPU\n",
    "                    all_images = all_images.to(device)\n",
    "                    all_labels = all_labels.to(device)\n",
    "\n",
    "                    # Get logits and sum up total loss\n",
    "                    x_pred = model_final(all_images)\n",
    "                    avg_testing_loss += loss_function(x_pred, all_labels).item()\n",
    "\n",
    "            # Get training loss\n",
    "            avg_training_loss /= len(trainset_loader_CIFAR10)\n",
    "\n",
    "            # Get testing loss\n",
    "            avg_testing_loss /= len(testset_loader_CIFAR10)\n",
    "\n",
    "            # Switch model back to training mode\n",
    "            model_final.train()\n",
    "\n",
    "            epoch_over_training_loss_CIFAR10_per_k.append({\n",
    "                \"k\": current_k,\n",
    "                \"epoch\": epoch,\n",
    "                \"training_loss\": avg_training_loss\n",
    "                })\n",
    "            \n",
    "            epoch_over_testing_loss_CIFAR10_per_k.append({\n",
    "                \"k\": current_k,\n",
    "                \"epoch\": epoch,\n",
    "                \"training_loss\": avg_training_loss\n",
    "                })\n",
    "            \n",
    "\n",
    "            print(\"\")\n",
    "\n",
    "            print(f'   -> Training Loss: {avg_training_loss: .4f}\\n')\n",
    "            print(f'   -> Testing Loss: {avg_testing_loss: .4f}\\n')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4754fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate frozen and unfrozen runs\n",
    "for frozen_status, title in zip([True, False], [\"Frozen Transfer Learning\", \"Unfrozen Transfer Learning\"]):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ks = sorted(set(d[\"k\"] for d in epoch_over_training_loss if d[\"frozen\"] == frozen_status))\n",
    "    for k in ks:\n",
    "        epochs = [d[\"epoch\"] for d in epoch_over_training_loss if d[\"k\"] == k and d[\"frozen\"] == frozen_status]\n",
    "        train_loss = [d[\"training_loss\"] for d in epoch_over_training_loss if d[\"k\"] == k and d[\"frozen\"] == frozen_status]\n",
    "        test_loss = [d[\"training_loss\"] for d in epoch_over_testing_loss if d[\"k\"] == k and d[\"frozen\"] == frozen_status]\n",
    "        plt.plot(epochs, train_loss, label=f\"Train k={k}\")\n",
    "        plt.plot(epochs, test_loss, '--', label=f\"Test k={k}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(title + \": Epoch vs. Loss (per k)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e40d68a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10_Encoder(CIFAR10_Classifier):\n",
    "    def __init__(self, C_dropout, F_dropout, embedding_size=2):\n",
    "        super().__init__(C_dropout, F_dropout)\n",
    "\n",
    "        self.output_layer = nn.Linear(in_features=self.output_nodes, out_features=embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.partial_forward(x)\n",
    "        logits = self.output_layer(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d348d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveCIFAR10(data.Dataset):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.labels = np.array(self.base_dataset.targets)\n",
    "\n",
    "        # Pre-compute a dictionary mapping each class to a list of its indices\n",
    "        self.labels_to_indices = {label: np.where(self.labels == label)[0]\n",
    "                                  for label in set(self.labels)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the anchor image and its label\n",
    "        img1, label1 = self.base_dataset[index]\n",
    "\n",
    "        # Decide whether to sample a positive or negative pair (50% chance)\n",
    "        is_similar = random.random() > 0.5\n",
    "\n",
    "        if is_similar:\n",
    "            # Positive pair: sample another image from the same class\n",
    "            positive_indices = self.labels_to_indices[label1]\n",
    "            # Make sure we don't pick the same image\n",
    "            positive_index = index\n",
    "            while positive_index == index:\n",
    "                positive_index = np.random.choice(positive_indices)\n",
    "            img2, _ = self.base_dataset[positive_index]\n",
    "            similarity = 1.0 # Similarity label is 1 for positive pairs\n",
    "        else:\n",
    "            # Negative pair: sample an image from a different class\n",
    "            negative_label = np.random.choice(list(set(self.labels) - {label1}))\n",
    "            negative_indices = self.labels_to_indices[negative_label]\n",
    "            negative_index = np.random.choice(negative_indices)\n",
    "            img2, _ = self.base_dataset[negative_index]\n",
    "            similarity = 0.0 # Similarity label is 0 for negative pairs\n",
    "\n",
    "        return img1, img2, torch.tensor(similarity, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf11b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, embedding1, embedding2, similarity_label):\n",
    "        # Calculate the euclidean distance squared between the embeddings\n",
    "        euclidean_distance = F.pairwise_distance(embedding1, embedding2, keepdim=True)\n",
    "        dist_sq = torch.pow(euclidean_distance, 2)\n",
    "\n",
    "        # Loss for similar pairs (S=1): we want their distance to be small\n",
    "        loss_similar = similarity_label * dist_sq\n",
    "\n",
    "        # Loss for dissimilar pairs (S=0): we want their distance to be large, at least > margin\n",
    "        # The loss is max(0, margin - distance)^2\n",
    "        loss_dissimilar = (1 - similarity_label) * torch.pow(\n",
    "            torch.clamp(self.margin - euclidean_distance, min=0.0), 2\n",
    "        )\n",
    "\n",
    "        # Combine the losses and average over the batch\n",
    "        total_loss = torch.mean(loss_similar + loss_dissimilar)\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea658b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_embeddings(model, loader):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            embs = model(images).cpu().numpy()\n",
    "            embeddings.append(embs)\n",
    "            all_labels.append(labels.numpy())\n",
    "    return np.concatenate(embeddings), np.concatenate(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b755285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(embeddings, labels, title=\"\"):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    class_names = trainset_full_CIFAR10.classes\n",
    "    for i in range(len(class_names)):\n",
    "        # Select embeddings that correspond to the current class\n",
    "        inds = np.where(labels == i)[0]\n",
    "        plt.scatter(embeddings[inds, 0], embeddings[inds, 1], alpha=0.5, label=class_names[i])\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae054d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Starting training of CIFAR10 embedding model ##########\n",
      "----- Epoch: 1/30 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/782 [00:00<?, ? batchs/s]"
     ]
    }
   ],
   "source": [
    "# Hyperparameter setup\n",
    "epochs = 30\n",
    "batch_size = 64\n",
    "learning_rate = 7.5e-5\n",
    "decay_rate = 5e-4\n",
    "\n",
    "c_dropout = 0.30\n",
    "f_dropout = 0.62\n",
    "\n",
    "embedding_model = CIFAR10_Encoder(c_dropout, f_dropout).to(device)\n",
    "contrastive_loss_fn = ContrastiveLoss(margin=1.0)\n",
    "optimizer = optim.Adam(embedding_model.parameters(), lr=learning_rate, weight_decay=decay_rate)\n",
    "\n",
    "contrastive_train_ds = ContrastiveCIFAR10(trainset_full_CIFAR10)\n",
    "contrastive_train_loader = data.DataLoader(contrastive_train_ds, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True,\n",
    "                                           pin_memory=True)\n",
    "\n",
    "print(f'######## Starting training of CIFAR10 embedding model ##########')\n",
    "\n",
    "embedding_model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for img1, img2, sim in tqdm(contrastive_train_loader, desc='Training', unit=' batchs'):\n",
    "        img1, img2, sim = img1.to(device), img2.to(device), sim.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        emb1, emb2 = embedding_model(img1), embedding_model(img2)\n",
    "        loss = contrastive_loss_fn(emb1, emb2, sim)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"  -> [Contrastive] Epoch {epoch+1}/3 | Loss={running_loss/len(contrastive_train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d584a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader_vis = data.DataLoader(testset_full_CIFAR10, batch_size=256, shuffle=False)\n",
    "\n",
    "final_embeddings, final_labels = get_all_embeddings(embedding_model, test_loader_vis)\n",
    "plot_embeddings(final_embeddings, final_labels, \"Embeddings After Training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
