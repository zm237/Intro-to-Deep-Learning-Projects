{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T00:13:33.363005Z",
     "start_time": "2025-12-14T00:13:33.359682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy.ma.core import shape\n",
    "from torch.nn import MSELoss\n",
    "from torch.utils import data\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from triton.language import dtype\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ],
   "id": "51d9cd7f105a3ecd",
   "outputs": [],
   "execution_count": 248
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verify that GPU is connected and available\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(torch.cuda.get_device_name(0))"
   ],
   "id": "4dc4960e2b98d73c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T01:13:15.322698Z",
     "start_time": "2025-12-14T01:13:15.268286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class MSGameManager:\n",
    "\n",
    "\n",
    "    def __init__(self):\n",
    "        self.player_board = torch.zeros([22, 22], dtype=torch.int8)\n",
    "        self.mine_board = torch.zeros([22, 22], dtype=torch.int8) # Location of All Mines, 9 for mine, 0-8 for clue\n",
    "        self.flagged_board = torch.zeros([22, 22], dtype=torch.int8)\n",
    "        self.opened_board = torch.zeros([22, 22], dtype=torch.int8)\n",
    "        self.number_of_mines = -1\n",
    "\n",
    "        # Logic bot variables\n",
    "        self.cells_remaining = set()\n",
    "        self.inferred_safe = set()\n",
    "        self.inferred_mine = set()\n",
    "        self.clue_number = dict()\n",
    "        self.size = 22\n",
    "        self.moves_taken = 0\n",
    "        self.initial_start_coords = None\n",
    "        self.is_game_over = False\n",
    "        self.mines_triggered = 0\n",
    "\n",
    "    def reset_game(self):\n",
    "        # This method handles resetting ALL state variables for a new game\n",
    "        self.player_board = torch.zeros([self.size, self.size], dtype=torch.int8)\n",
    "        self.mine_board = torch.zeros([self.size, self.size], dtype=torch.int8)\n",
    "        self.flagged_board = torch.zeros([self.size, self.size], dtype=torch.int8)\n",
    "        self.opened_board = torch.zeros([self.size, self.size], dtype=torch.int8)\n",
    "        self.number_of_mines = -1 # Set back to default, to be set later\n",
    "        self.cells_remaining = set()\n",
    "        self.inferred_safe = set()\n",
    "        self.inferred_mine = set()\n",
    "        self.clue_number = dict()\n",
    "        self.moves_taken = 0\n",
    "        self.initial_start_coords = None\n",
    "        self.is_game_over = False\n",
    "        self.mines_triggered = 0\n",
    "\n",
    "    def initialize_board(self, difficulty: str):\n",
    "\n",
    "        self.reset_game()\n",
    "\n",
    "        self.number_of_mines = 50 if difficulty == \"easy\" \\\n",
    "            else 80 if difficulty == \"medium\" \\\n",
    "                else 100\n",
    "\n",
    "        # Add all cells into the remaining set and reset boards\n",
    "        self.cells_remaining = set((r, c) for r in range(self.size) for c in range(self.size))\n",
    "        self.mine_board = torch.zeros([self.size, self.size], dtype=torch.int8)\n",
    "\n",
    "        # Generate a random starting location\n",
    "        initial_start = torch.randint(0, self.size, (2, ), dtype=torch.int8)\n",
    "        start_r, start_c = initial_start[0].item(), initial_start[1].item()\n",
    "        self.initial_start_coords = (start_r, start_c)\n",
    "\n",
    "        # Remove the starting cell from the board\n",
    "        self.cells_remaining.remove(self.initial_start_coords)\n",
    "\n",
    "        # All possible coordinates excluding the initial start cell\n",
    "        possible_mine_locations = list(self.cells_remaining)\n",
    "\n",
    "        # Grab the squares that will be set to mines\n",
    "        mine_locations = random.sample(possible_mine_locations, self.number_of_mines)\n",
    "\n",
    "        # Place mines on the mine board\n",
    "        MINE = 9\n",
    "        for r, c in mine_locations:\n",
    "            self.mine_board[r, c] = MINE\n",
    "\n",
    "        # Calculate and place clues on the mine_board\n",
    "        self.calculate_clues()\n",
    "\n",
    "        # Perform the initial move so we can have a starting board\n",
    "        self.open_cell(start_r, start_c)\n",
    "\n",
    "\n",
    "    def get_neighbors(self, r, c):\n",
    "        \"\"\"Returns a list of valid neighboring coordinates (up to 8).\"\"\"\n",
    "        neighbors = []\n",
    "        for dr in [-1, 0, 1]:\n",
    "            for dc in [-1, 0, 1]:\n",
    "                if dr == 0 and dc == 0:\n",
    "                    continue\n",
    "                nr, nc = r + dr, c + dc\n",
    "\n",
    "                # Check for boundary conditions\n",
    "                if 0 <= nr < self.size and 0 <= nc < self.size:\n",
    "                    neighbors.append((nr, nc))\n",
    "        return neighbors\n",
    "\n",
    "    def calculate_clues(self):\n",
    "        \"\"\"Iterates over the board and calculates the clue number for non-mine cells.\"\"\"\n",
    "        MINE = 9\n",
    "\n",
    "        for r in range(self.size):\n",
    "            for c in range(self.size):\n",
    "\n",
    "                # Skip square if it is a mine\n",
    "                if self.mine_board[r, c] == MINE:\n",
    "                    continue\n",
    "\n",
    "                mine_count = 0\n",
    "                for nr, nc in self.get_neighbors(r, c):\n",
    "                    # Check if the neighbor is a mine\n",
    "                    if self.mine_board[nr, nc] == MINE:\n",
    "                        mine_count += 1\n",
    "\n",
    "                # Store the clue number (0-8) on the mine_board\n",
    "                self.mine_board[r, c] = mine_count\n",
    "\n",
    "    def open_cell(self, r, c, allow_mine: bool = False):\n",
    "        \"\"\"\n",
    "        Performs the action of opening a cell, updating all state boards,\n",
    "        and handling the chain reaction if a blank (clue 0) is hit.\n",
    "        Returns: True if the move was successful (not a mine), False otherwise.\n",
    "        \"\"\"\n",
    "        if self.opened_board[r, c] == 1:\n",
    "            return True # Already opened, treat as successful non-fatal move\n",
    "\n",
    "        # 1. Update general state\n",
    "        self.opened_board[r, c] = 1\n",
    "        coord = (r, c)\n",
    "        if coord in self.cells_remaining:\n",
    "             self.cells_remaining.remove(coord)\n",
    "\n",
    "        # Also remove from inferred sets if it was picked\n",
    "        if coord in self.inferred_safe:\n",
    "            self.inferred_safe.remove(coord)\n",
    "\n",
    "        # 2. Get the ground truth value\n",
    "        value = self.mine_board[r, c].item()\n",
    "\n",
    "        MINE_CODE = 9\n",
    "\n",
    "        if value == MINE_CODE: # Mine is hit\n",
    "            self.is_game_over = True if not allow_mine else False\n",
    "            self.mines_triggered += 1\n",
    "            self.player_board[r, c] = -1 # A code for 'detonated mine'\n",
    "            return False # Game over, move was unsuccessful\n",
    "\n",
    "        elif value == 0: # Blank cell (Clue 0) - start chain reaction\n",
    "            self.player_board[r, c] = 9 # Use 9 for revealed blank\n",
    "            self.clue_number[coord] = 0\n",
    "\n",
    "            # --- Chain Reaction Logic (using BFS for cascade) ---\n",
    "            # ... (The full chain reaction logic from the previous answer goes here) ...\n",
    "\n",
    "            queue = [(r, c)]\n",
    "            visited = set([(r, c)])\n",
    "\n",
    "            while queue:\n",
    "                curr_r, curr_c = queue.pop(0)\n",
    "\n",
    "                for nr, nc in self.get_neighbors(curr_r, curr_c):\n",
    "                    neighbor_coord = (nr, nc)\n",
    "                    if neighbor_coord in visited or self.opened_board[nr, nc] == 1:\n",
    "                        continue\n",
    "\n",
    "                    visited.add(neighbor_coord)\n",
    "                    true_value = self.mine_board[nr, nc].item()\n",
    "\n",
    "                    if true_value == 0:\n",
    "                        # Continue the cascade (reveal blank, add to queue)\n",
    "                        self.player_board[nr, nc] = 9\n",
    "                        self.opened_board[nr, nc] = 1\n",
    "                        self.clue_number[neighbor_coord] = 0\n",
    "                        if neighbor_coord in self.cells_remaining:\n",
    "                            self.cells_remaining.remove(neighbor_coord)\n",
    "                        queue.append(neighbor_coord)\n",
    "\n",
    "                    elif 1 <= true_value <= 8:\n",
    "                        # Stop the cascade (reveal clue)\n",
    "                        self.player_board[nr, nc] = true_value\n",
    "                        self.opened_board[nr, nc] = 1\n",
    "                        self.clue_number[neighbor_coord] = true_value\n",
    "                        if neighbor_coord in self.cells_remaining:\n",
    "                            self.cells_remaining.remove(neighbor_coord)\n",
    "\n",
    "            return True\n",
    "\n",
    "        elif 1 <= value <= 8: # Clue cell\n",
    "            self.player_board[r, c] = value\n",
    "            self.clue_number[coord] = value\n",
    "            return True\n",
    "\n",
    "        return True # Should not be reached, but ensures return value\n",
    "\n",
    "    def _run_logic_inferences(self):\n",
    "        \"\"\"\n",
    "        Applies the two main Minesweeper logic rules iteratively until no new inferences are found.\n",
    "        Returns True if any new inference was made, False otherwise.\n",
    "        \"\"\"\n",
    "        inferences_made = False\n",
    "\n",
    "        # Loop until a full pass yields no new inferences [cite: 24]\n",
    "        while True:\n",
    "            new_inferences_in_pass = False\n",
    "\n",
    "            # Iterate over all cells where a clue has been revealed\n",
    "            for r, c in list(self.clue_number.keys()):\n",
    "                clue = self.clue_number[(r, c)]\n",
    "                neighbors = self.get_neighbors(r, c)\n",
    "\n",
    "                # --- Categorize Neighbors ---\n",
    "                unrevealed_neighbors = set()\n",
    "                mines_inferred_count = 0\n",
    "                safe_inferred_count = 0\n",
    "\n",
    "                for nr, nc in neighbors:\n",
    "                    coord = (nr, nc)\n",
    "                    if coord in self.inferred_mine:\n",
    "                        mines_inferred_count += 1\n",
    "                    elif coord in self.inferred_safe or self.opened_board[nr, nc] == 1:\n",
    "                        safe_inferred_count += 1\n",
    "                    elif self.opened_board[nr, nc] == 0:\n",
    "                        # This cell is truly unrevealed and un-inferred\n",
    "                        unrevealed_neighbors.add(coord)\n",
    "\n",
    "                num_unrevealed = len(unrevealed_neighbors)\n",
    "\n",
    "                # 1. Mine Inference (Rule 1)\n",
    "                # If clue - (known mines) == (unrevealed cells)\n",
    "                if clue - mines_inferred_count == num_unrevealed and num_unrevealed > 0:\n",
    "                    for nr, nc in unrevealed_neighbors:\n",
    "                        coord = (nr, nc)\n",
    "\n",
    "                        # Only infer if not already marked as mine\n",
    "                        if coord not in self.inferred_mine:\n",
    "                            self.inferred_mine.add(coord)\n",
    "                            # Remove from cells_remaining as it is now determined\n",
    "                            if coord in self.cells_remaining:\n",
    "                                self.cells_remaining.remove(coord)\n",
    "\n",
    "                            new_inferences_in_pass = True\n",
    "\n",
    "                # 2. Safe Inference (Rule 2)\n",
    "                # If (Total Neighbors - Clue) - (known safe cells) == (unrevealed cells)\n",
    "                # A simpler check: if all mines are accounted for by inferred mines/flags\n",
    "                num_non_mines_required = len(neighbors) - clue\n",
    "\n",
    "                # Total cells known to be safe (opened + inferred safe)\n",
    "                total_known_safe = safe_inferred_count + num_unrevealed\n",
    "\n",
    "                # If the total known safe cells (including all unrevealed) equals\n",
    "                # the total number of non-mines possible\n",
    "                if num_non_mines_required == total_known_safe and num_unrevealed > 0:\n",
    "                    for nr, nc in unrevealed_neighbors:\n",
    "                        coord = (nr, nc)\n",
    "\n",
    "                        # Only infer if not already marked as safe\n",
    "                        if coord not in self.inferred_safe and self.opened_board[nr, nc] == 0:\n",
    "                            self.inferred_safe.add(coord)\n",
    "                            # Remove from cells_remaining as it is now determined\n",
    "                            if coord in self.cells_remaining:\n",
    "                                self.cells_remaining.remove(coord)\n",
    "\n",
    "                            new_inferences_in_pass = True\n",
    "\n",
    "            # If no new inferences were made in this full pass, the loop terminates [cite: 24]\n",
    "            if not new_inferences_in_pass:\n",
    "                break\n",
    "            else:\n",
    "                inferences_made = True\n",
    "\n",
    "        return inferences_made\n",
    "\n",
    "\n",
    "    def get_logic_bot_move(self):\n",
    "        \"\"\"\n",
    "        The main move logic for the simple bot.\n",
    "        1. Runs inferences to find safe cells.\n",
    "        2. Picks an inferred safe cell if available, otherwise picks randomly.\n",
    "        Returns: (r, c) tuple of the next move.\n",
    "        \"\"\"\n",
    "        # Step 1: Run inferences until the board is stable [cite: 24]\n",
    "        self._run_logic_inferences()\n",
    "\n",
    "        # Step 2: Select a cell to open [cite: 18]\n",
    "        if self.inferred_safe:\n",
    "            # Pick one of the inferred safe cells\n",
    "            r, c = self.inferred_safe.pop()\n",
    "        elif self.cells_remaining:\n",
    "            # No safe inference found, pick a cell from the remaining pool at random [cite: 18]\n",
    "            r, c = random.choice(list(self.cells_remaining))\n",
    "            # Note: The assignment implies the cells_remaining only holds cells *not* inferred as mine.\n",
    "        else:\n",
    "            # Game is likely won or stuck\n",
    "            return None\n",
    "\n",
    "        return r, c\n",
    "\n",
    "    def make_move(self, r, c, allow_mine: bool = False):\n",
    "        \"\"\"\n",
    "        Executes a move at (r, c) and updates game state and metrics.\n",
    "        Returns: Tuple (success: bool, is_game_over: bool)\n",
    "        \"\"\"\n",
    "        if self.is_game_over or self.opened_board[r, c] == 1:\n",
    "            return True, self.is_game_over # Cannot make move or already open\n",
    "\n",
    "        # If the move is flagged, we assume the bot unflags it first (optional)\n",
    "        if self.flagged_board[r, c] == 1:\n",
    "             self.flagged_board[r, c] = 0 # Unflag before opening\n",
    "\n",
    "        self.moves_taken += 1 # Increment step counter\n",
    "\n",
    "        move_successful = self.open_cell(r, c, allow_mine)\n",
    "\n",
    "        # Check for win condition after a successful move\n",
    "        if move_successful and self.check_win_condition():\n",
    "            self.is_game_over = True\n",
    "\n",
    "        return move_successful, self.is_game_over\n",
    "\n",
    "    def check_win_condition(self):\n",
    "        \"\"\"Checks if all safe cells have been opened.\"\"\"\n",
    "        total_cells = self.size * self.size\n",
    "        cells_opened = torch.sum(self.opened_board).item()\n",
    "\n",
    "        # Win condition: Number of opened cells equals (Total Cells - Number of Mines)\n",
    "        return cells_opened == (total_cells - self.number_of_mines)\n",
    "\n",
    "    def start_bot_game(self, difficulty: str, allow_mine: bool = False):\n",
    "\n",
    "        self.initialize_board(difficulty) # Initializes board, places mines, and performs initial click\n",
    "        self.is_game_over = False\n",
    "        self.moves_taken = 0\n",
    "        self.mines_triggered = 0\n",
    "\n",
    "        # Ensure the initial move is tracked as a move taken (if it wasn't a blank that cascaded)\n",
    "        # Note: initialize_board already opened the first cell. We start tracking from the second move.\n",
    "\n",
    "        # The main simulation loop\n",
    "        while not self.is_game_over:\n",
    "\n",
    "            # 1. Get the bot's next move choice\n",
    "            r, c = self.get_logic_bot_move()\n",
    "\n",
    "            if r is None:\n",
    "                # Bot could not find any remaining cell to click (likely won)\n",
    "                self.is_game_over = True\n",
    "                break\n",
    "\n",
    "            # 2. Execute the move\n",
    "            success, game_over = self.make_move(r, c, allow_mine)\n",
    "\n",
    "            # If the game is over (due to a mine hit or win), the loop terminates\n",
    "\n",
    "        # Return the required metrics for comparison\n",
    "        return {\n",
    "            \"success\": self._check_win_condition(),\n",
    "            \"moves_survived\": self.moves_taken,\n",
    "            \"mines_triggered\": self.mines_triggered\n",
    "        }\n",
    "\n",
    "    def run_logic_inferences(self):\n",
    "        \"\"\"\n",
    "        Applies the two main Minesweeper logic rules iteratively until no new inferences are found.\n",
    "        Returns True if any new inference was made, False otherwise.\n",
    "        \"\"\"\n",
    "        inferences_made = False\n",
    "\n",
    "        # [cite_start]Loop until a full pass yields no new inferences [cite: 24]\n",
    "        while True:\n",
    "            new_inferences_in_pass = False\n",
    "\n",
    "            # Iterate over all cells where a clue has been revealed\n",
    "            for r, c in list(self.clue_number.keys()):\n",
    "                clue = self.clue_number[(r, c)]\n",
    "                neighbors = self.get_neighbors(r, c)\n",
    "\n",
    "                # --- Categorize Neighbors ---\n",
    "                unrevealed_neighbors = set()\n",
    "                mines_inferred_count = 0\n",
    "                safe_inferred_count = 0\n",
    "\n",
    "                for nr, nc in neighbors:\n",
    "                    coord = (nr, nc)\n",
    "                    if coord in self.inferred_mine:\n",
    "                        mines_inferred_count += 1\n",
    "                    elif coord in self.inferred_safe or self.opened_board[nr, nc] == 1:\n",
    "                        safe_inferred_count += 1\n",
    "                    elif self.opened_board[nr, nc] == 0:\n",
    "                        # This cell is truly unrevealed and un-inferred\n",
    "                        unrevealed_neighbors.add(coord)\n",
    "\n",
    "                num_unrevealed = len(unrevealed_neighbors)\n",
    "\n",
    "                # 1. Mine Inference (Rule 1)\n",
    "                # [cite_start]If (cell clue) - (# neighbors inferred to be mines) = (# unrevealed neighbors) [cite: 22]\n",
    "                if clue - mines_inferred_count == num_unrevealed and num_unrevealed > 0:\n",
    "                    for nr, nc in unrevealed_neighbors:\n",
    "                        coord = (nr, nc)\n",
    "\n",
    "                        # Only infer if not already marked as mine\n",
    "                        if coord not in self.inferred_mine:\n",
    "                            self.inferred_mine.add(coord)\n",
    "\n",
    "                            # --- FIX: FLAG THE INFERRED MINE ---\n",
    "                            self.flagged_board[nr, nc] = 1 # Mark the cell as flagged\n",
    "\n",
    "                            # [cite_start]Remove them from cells_remaining [cite: 22]\n",
    "                            if coord in self.cells_remaining:\n",
    "                                self.cells_remaining.remove(coord)\n",
    "\n",
    "                            new_inferences_in_pass = True\n",
    "\n",
    "                # 2. Safe Inference (Rule 2)\n",
    "                # [cite_start]If ((# neighbors) - (cell clue)) - (# neighbors revealed or inferred to be safe) = (# unrevealed neighbors) [cite: 23]\n",
    "                num_non_mines_required = len(neighbors) - clue\n",
    "\n",
    "                # Total cells known to be safe (opened + inferred safe)\n",
    "                total_known_safe = safe_inferred_count + num_unrevealed\n",
    "\n",
    "                # If the total known safe cells (including all unrevealed) equals\n",
    "                # the total number of non-mines possible\n",
    "                if num_non_mines_required == total_known_safe and num_unrevealed > 0:\n",
    "                    for nr, nc in unrevealed_neighbors:\n",
    "                        coord = (nr, nc)\n",
    "\n",
    "                        # Only infer if not already marked as safe\n",
    "                        if coord not in self.inferred_safe and self.opened_board[nr, nc] == 0:\n",
    "                            self.inferred_safe.add(coord)\n",
    "                            # [cite_start]Remove from cells_remaining [cite: 23]\n",
    "                            if coord in self.cells_remaining:\n",
    "                                self.cells_remaining.remove(coord)\n",
    "\n",
    "                            new_inferences_in_pass = True\n",
    "\n",
    "            # [cite_start]If no new inferences were made in this full pass, the loop terminates [cite: 24]\n",
    "            if not new_inferences_in_pass:\n",
    "                break\n",
    "            else:\n",
    "                inferences_made = True\n",
    "\n",
    "        return inferences_made\n",
    "\n",
    "    def get_nn_input_state(self):\n",
    "        \"\"\"\n",
    "        Creates the 4-channel input tensor (4, 22, 22) for the Neural Network.\n",
    "        \"\"\"\n",
    "        size = self.size\n",
    "\n",
    "        # Channel 1: Clue Values (0-8)\n",
    "        # Use player_board, but replace the 'blank' code (9) with 0 for clean clue representation\n",
    "        clues_channel = self.player_board.clone().float()\n",
    "        clues_channel[clues_channel == 9] = 0.0\n",
    "        clues_channel[clues_channel == -1] = 0.0 # Ignore detonated mines if applicable\n",
    "\n",
    "        # Channel 2: Opened Mask (1/0)\n",
    "        opened_mask_channel = self.opened_board.clone().float()\n",
    "\n",
    "        # Channel 3: Flag Mask (1/0)\n",
    "        flag_mask_channel = self.flagged_board.clone().float()\n",
    "\n",
    "        # Channel 4: Global Context (e.g., Mine Density)\n",
    "        # Represented as the ratio of mines to total cells\n",
    "        mine_density = self.number_of_mines / (size * size)\n",
    "        global_context_channel = torch.full((size, size), mine_density, dtype=torch.float32)\n",
    "\n",
    "        # Stack the channels\n",
    "        nn_input = torch.stack([\n",
    "            clues_channel,\n",
    "            opened_mask_channel,\n",
    "            flag_mask_channel,\n",
    "            global_context_channel\n",
    "        ], dim=0)\n",
    "\n",
    "        return nn_input\n",
    "\n",
    "    def get_safety_label(self):\n",
    "        \"\"\"\n",
    "        Creates ground-truth safety map.\n",
    "        - 1.0 for safe, unopened cells\n",
    "        - 0.0 for mines (unopened)\n",
    "        - -1.0 for opened cells (to be masked in loss)\n",
    "        \"\"\"\n",
    "        MINE_CODE = 9\n",
    "\n",
    "        # Start with mine detection: 1.0 for safe, 0.0 for mine\n",
    "        safety_label = (self.mine_board != MINE_CODE).float()\n",
    "\n",
    "        # Mask out opened cells with -1 so they're ignored in loss\n",
    "        safety_label[self.opened_board == 1] = -1.0\n",
    "\n",
    "        return safety_label\n",
    "\n",
    "    def generate_training_data(self, difficulty: str, num_games: int, allow_mine: bool = False):\n",
    "\n",
    "        # Lists to store the collected Input (X) and Output (Y) tensors\n",
    "        X_data = [] # List of (4, 22, 22) input tensors\n",
    "        Y_data = [] # List of (22, 22) output safety label tensors\n",
    "\n",
    "        for game_idx in range(num_games):\n",
    "\n",
    "            # Start a fresh game\n",
    "            self.reset_game()\n",
    "            self.initialize_board(difficulty)\n",
    "\n",
    "            # Print after initialize_board sets number_of_mines\n",
    "            if game_idx == 0:\n",
    "                mode_str = \"with mines allowed\" if allow_mine else \"standard mode\"\n",
    "                print(f\"Generating data for {difficulty.upper()} difficulty ({self.number_of_mines} mines) - {mode_str}...\")\n",
    "\n",
    "            self.is_game_over = False\n",
    "            self.moves_taken = 0\n",
    "            self.mines_triggered = 0\n",
    "\n",
    "            # Logic Bot Play Loop\n",
    "            while not self.is_game_over:\n",
    "\n",
    "                # --- DATA COLLECTION POINT (BEFORE THE MOVE) ---\n",
    "\n",
    "                # 1. Capture the current state (X)\n",
    "                current_input_state = self.get_nn_input_state()\n",
    "\n",
    "                # 2. Capture the ground truth label (Y)\n",
    "                current_safety_label = self.get_safety_label()\n",
    "\n",
    "                # Store the data pair\n",
    "                X_data.append(current_input_state)\n",
    "                Y_data.append(current_safety_label)\n",
    "\n",
    "                # --- BOT DECISION AND EXECUTION ---\n",
    "\n",
    "                # 3. Get the bot's next move choice (r, c)\n",
    "                move = self.get_logic_bot_move()\n",
    "\n",
    "                # CHECK FOR None BEFORE UNPACKING\n",
    "                if move is None:\n",
    "                    # Game ended (won or no moves left)\n",
    "                    self.is_game_over = True\n",
    "                    break\n",
    "\n",
    "                r, c = move  # Now safe to unpack\n",
    "\n",
    "                # 4. Execute the move (updates game state)\n",
    "                success, game_over = self.make_move(r, c, allow_mine)\n",
    "\n",
    "            if (game_idx + 1) % 100 == 0:\n",
    "                print(f\"Completed {game_idx + 1} games. Total data points: {len(X_data)}\")\n",
    "\n",
    "        # Convert lists to final PyTorch tensors\n",
    "        X_tensor = torch.stack(X_data)\n",
    "        Y_tensor = torch.stack(Y_data)\n",
    "\n",
    "        return X_tensor, Y_tensor\n",
    "\n",
    "    def generate_critic_training_data(self, difficulty: str, num_games: int, actor: nn.Module):\n",
    "        \"\"\"\n",
    "        Generates training data for Critic using 5-channel input.\n",
    "\n",
    "        Input: (5, 22, 22) where channel 4 is the selected move\n",
    "        Output: number of moves survived\n",
    "        \"\"\"\n",
    "        X_data = []  # List of (5, 22, 22) tensors\n",
    "        Y_data = []  # List of survival counts\n",
    "\n",
    "        for game_idx in range(num_games):\n",
    "            self.reset_game()\n",
    "            self.initialize_board(difficulty)\n",
    "\n",
    "            if game_idx == 0:\n",
    "                print(f\"Generating Critic data for {difficulty.upper()} (5-channel approach)...\")\n",
    "\n",
    "            # Store game trajectory\n",
    "            game_inputs = []\n",
    "\n",
    "            while not self.is_game_over:\n",
    "                # Get current 4-channel state\n",
    "                current_state = self.get_nn_input_state()  # (4, 22, 22)\n",
    "\n",
    "                # Get move from actor\n",
    "                if actor is None:\n",
    "                    move = self.get_logic_bot_move()\n",
    "                else:\n",
    "                    pass\n",
    "                    move = self.get_actor_move(actor)  # Your trained actor\n",
    "\n",
    "                if move is None:\n",
    "                    break\n",
    "\n",
    "                r, c = move\n",
    "\n",
    "                # Create move channel (one-hot encoding of the move)\n",
    "                move_channel = torch.zeros((self.size, self.size), dtype=torch.float32)\n",
    "                move_channel[r, c] = 1.0\n",
    "\n",
    "                # Combine into 5-channel input\n",
    "                full_input = torch.cat([\n",
    "                    current_state,  # (4, 22, 22)\n",
    "                    move_channel.unsqueeze(0)  # (1, 22, 22)\n",
    "                ], dim=0)  # Result: (5, 22, 22)\n",
    "\n",
    "                game_inputs.append(full_input)\n",
    "\n",
    "                # Execute move (allow continuing after mines)\n",
    "                success, game_over = self.make_move(r, c, allow_mine=True)\n",
    "\n",
    "            # Retrospectively label with survival counts\n",
    "            total_moves = len(game_inputs)\n",
    "            for i in range(total_moves):\n",
    "                moves_survived = total_moves - i\n",
    "\n",
    "                X_data.append(game_inputs[i])\n",
    "                Y_data.append(moves_survived)\n",
    "\n",
    "            if (game_idx + 1) % 100 == 0:\n",
    "                print(f\"Completed {game_idx + 1} games. Total data points: {len(X_data)}\")\n",
    "\n",
    "        return torch.stack(X_data), torch.tensor(Y_data, dtype=torch.float32)\n",
    "\n",
    "    def generate_actor_training_data(self, difficulty: str, num_games: int, critic_model):\n",
    "        \"\"\"\n",
    "        Generate training data for the Actor network.\n",
    "\n",
    "        Process:\n",
    "        1. For each game state, evaluate ALL possible moves using the Critic\n",
    "        2. Create targets based on Critic's predictions\n",
    "        3. Actor learns to predict these values directly from board state\n",
    "\n",
    "        Returns:\n",
    "            X_data: (N, 4, 22, 22) - board states\n",
    "            Y_data: (N, 22, 22) - value map for each cell\n",
    "        \"\"\"\n",
    "        X_data = []  # Board states\n",
    "        Y_data = []  # Value maps (predicted survival for each cell)\n",
    "\n",
    "        for game_idx in range(num_games):\n",
    "            self.reset_game()\n",
    "            self.initialize_board(difficulty)\n",
    "\n",
    "            if game_idx == 0:\n",
    "                print(f\"Generating Actor training data for {difficulty.upper()}...\")\n",
    "\n",
    "            while not self.is_game_over:\n",
    "                current_state = self.get_nn_input_state()  # (4, 22, 22)\n",
    "\n",
    "                # Create value map by querying Critic for all possible moves\n",
    "                value_map = self.create_value_map_from_critic(current_state, critic_model)\n",
    "\n",
    "                # Store this training pair\n",
    "                X_data.append(current_state)\n",
    "                Y_data.append(value_map)\n",
    "\n",
    "                # Make a move using current Actor (or logic bot initially)\n",
    "                move = self.get_best_move_from_value_map(value_map)\n",
    "\n",
    "                if move is None:\n",
    "                    break\n",
    "\n",
    "                r, c = move\n",
    "                success, game_over = self.make_move(r, c, allow_mine=False)\n",
    "\n",
    "                if not success:\n",
    "                    break\n",
    "\n",
    "            if (game_idx + 1) % 100 == 0:\n",
    "                print(f\"Actor data: Completed {game_idx + 1} games. Data points: {len(X_data)}\")\n",
    "\n",
    "        return (\n",
    "            torch.stack(X_data),   # (N, 4, 22, 22)\n",
    "            torch.stack(Y_data)    # (N, 22, 22)\n",
    "        )\n",
    "\n",
    "\n",
    "    def create_value_map_from_critic(self, current_state, critic_model):\n",
    "        \"\"\"\n",
    "        Query the Critic for all possible moves to create a value map.\n",
    "\n",
    "        Returns:\n",
    "            value_map: (22, 22) tensor where each cell contains predicted survival\n",
    "                       -1 for already opened cells\n",
    "        \"\"\"\n",
    "        value_map = torch.full((self.size, self.size), -1.0, dtype=torch.float32)\n",
    "\n",
    "        critic_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for r in range(self.size):\n",
    "                for c in range(self.size):\n",
    "                    if self.opened_board[r, c] == 0:  # Unopened cell\n",
    "                        # Create 5-channel input for this move\n",
    "                        move_channel = torch.zeros((self.size, self.size), dtype=torch.float32)\n",
    "                        move_channel[r, c] = 1.0\n",
    "\n",
    "                        full_input = torch.cat([\n",
    "                            current_state,\n",
    "                            move_channel.unsqueeze(0)\n",
    "                        ], dim=0).unsqueeze(0)  # (1, 5, 22, 22)\n",
    "\n",
    "                        full_input = full_input.to(device)\n",
    "\n",
    "                        # Get Critic's prediction\n",
    "                        predicted_survival = critic_model(full_input)\n",
    "                        value_map[r, c] = predicted_survival.item()\n",
    "\n",
    "        return value_map\n",
    "\n",
    "\n",
    "    def get_best_move_from_value_map(self, value_map):\n",
    "        \"\"\"\n",
    "        Select the best move from value map.\n",
    "        \"\"\"\n",
    "        # Get valid moves (value >= 0)\n",
    "        valid_moves = []\n",
    "        values = []\n",
    "\n",
    "        for r in range(self.size):\n",
    "            for c in range(self.size):\n",
    "                if value_map[r, c] >= 0:  # Valid move\n",
    "                    valid_moves.append((r, c))\n",
    "                    values.append(value_map[r, c])\n",
    "\n",
    "        if not valid_moves:\n",
    "            return None\n",
    "\n",
    "        # Select move with highest value\n",
    "        values_tensor = torch.tensor(values)\n",
    "        best_idx = torch.argmax(values_tensor).item()\n",
    "\n",
    "        return valid_moves[best_idx]\n",
    "\n",
    "\n",
    "    def get_actor_move(self, actor):\n",
    "        \"\"\"\n",
    "        Get the best move from the Actor network.\n",
    "\n",
    "        Args:\n",
    "            actor: Trained Actor network\n",
    "\n",
    "        Returns:\n",
    "            (r, c) tuple of selected move, or None if no moves available\n",
    "        \"\"\"\n",
    "        actor.eval()\n",
    "        with torch.no_grad():\n",
    "            # Get current board state\n",
    "            current_state = self.get_nn_input_state().to(device)  # (4, 22, 22)\n",
    "\n",
    "            # Get Actor's value predictions for all cells\n",
    "            value_map = actor(current_state.unsqueeze(0)).squeeze(0)  # (22, 22)\n",
    "\n",
    "            value_map = value_map.reshape((self.size, self.size))\n",
    "\n",
    "            # Get all valid (unopened) moves and their predicted values\n",
    "            valid_moves = []\n",
    "            values = []\n",
    "\n",
    "            for r in range(self.size):\n",
    "                for c in range(self.size):\n",
    "                    if self.opened_board[r, c] == 0:  # Cell is unopened\n",
    "                        valid_moves.append((r, c))\n",
    "                        values.append(value_map[r, c].item())\n",
    "\n",
    "            if not valid_moves:\n",
    "                return None\n",
    "\n",
    "            # Select move with highest predicted value\n",
    "            values_tensor = torch.tensor(values)\n",
    "            best_idx = torch.argmax(values_tensor).item()\n",
    "\n",
    "            return valid_moves[best_idx]\n",
    "\n",
    "\n",
    "    def train_critc(self, critic, train_loader, test_loader, hp):\n",
    "        epoch_over_training = []\n",
    "        epoch_over_testing = []\n",
    "\n",
    "        # Hyperparameter setup\n",
    "        epochs = hp['epochs']\n",
    "        learning_rate = hp['learning_rate']\n",
    "        decay_rate = hp['decay_rate']\n",
    "\n",
    "        c_dropout = hp['cnn_dropout']\n",
    "        f_dropout = hp['linear_dropout']\n",
    "\n",
    "        print('######## Beginning training for MS Critic ##########')\n",
    "\n",
    "        model = critic\n",
    "        model.to(device)\n",
    "\n",
    "        loss_function = nn.MSELoss()\n",
    "\n",
    "        optimizer = optim.AdamW(model.parameters(),\n",
    "                               lr=learning_rate,\n",
    "                               weight_decay=decay_rate\n",
    "                               )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "\n",
    "        # Have references to variables outside of the epoch loop\n",
    "\n",
    "        avg_training_loss = 0\n",
    "        avg_testing_loss = 0\n",
    "\n",
    "        # Epoch Loop\n",
    "        for epoch in range(epochs):\n",
    "            print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "\n",
    "            avg_training_loss = 0\n",
    "            avg_testing_loss = 0\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            for x, Y in tqdm(train_loader, desc='Training', unit=' batch'):\n",
    "                # Transfer images to GPU\n",
    "                x = x.to(device)\n",
    "                Y = Y.to(device)\n",
    "\n",
    "                # Zero out gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Send images to model\n",
    "                x_pred = model(x)\n",
    "\n",
    "                Y = Y.unsqueeze(1)\n",
    "\n",
    "                # Calc loss\n",
    "                loss = loss_function(x_pred, Y)\n",
    "\n",
    "                # Calc gradient and update weights\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    avg_training_loss += loss.item()\n",
    "\n",
    "            # Switch to eval mode\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for x, Y in tqdm(test_loader, desc='Testing', unit=' batches'):\n",
    "                    # Move the images to the GPU\n",
    "                    x = x.to(device)\n",
    "                    Y = Y.to(device)\n",
    "\n",
    "                    # Get logits and sum up total loss\n",
    "                    x_pred = model(x)\n",
    "                    Y = Y.unsqueeze(1)\n",
    "                    avg_testing_loss += loss_function(x_pred, Y).item()\n",
    "\n",
    "            # Get training loss\n",
    "            avg_training_loss /= len(train_loader)\n",
    "\n",
    "             # Get testing loss\n",
    "            avg_testing_loss /= len(test_loader)\n",
    "\n",
    "            # Monitor learning\n",
    "            scheduler.step(avg_testing_loss)\n",
    "\n",
    "            # Switch model back to training mode\n",
    "            model.train()\n",
    "\n",
    "            epoch_over_training.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"training_loss\": avg_training_loss\n",
    "                })\n",
    "\n",
    "            epoch_over_testing.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"testing_loss\": avg_testing_loss\n",
    "                })\n",
    "\n",
    "\n",
    "            print(\"\")\n",
    "\n",
    "            print(f'   -> Training Loss: {avg_training_loss: .4f}\\n')\n",
    "            print(f'   -> Testing Loss: {avg_testing_loss: .4f}\\n')\n",
    "\n",
    "\n",
    "        return epoch_over_training, epoch_over_testing\n",
    "\n",
    "\n",
    "    def train_actor(self, actor, train_loader, test_loader, hp):\n",
    "        epoch_over_training = []\n",
    "        epoch_over_testing = []\n",
    "\n",
    "        # Hyperparameter setup\n",
    "        epochs = hp['epochs']\n",
    "        learning_rate = hp['learning_rate']\n",
    "        decay_rate = hp['decay_rate']\n",
    "\n",
    "        c_dropout = hp['cnn_dropout']\n",
    "        f_dropout = hp['linear_dropout']\n",
    "\n",
    "        print('######## Beginning training for MS Actor ##########')\n",
    "\n",
    "        model = actor\n",
    "        model.to(device)\n",
    "\n",
    "        pos_weight_tensor = torch.tensor(30.0, device=device)\n",
    "\n",
    "        loss_function = masked_bce_loss\n",
    "\n",
    "        optimizer = optim.AdamW(model.parameters(),\n",
    "                               lr=learning_rate,\n",
    "                               weight_decay=decay_rate\n",
    "                               )\n",
    "\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "\n",
    "        # Have references to variables outside of the epoch loop\n",
    "\n",
    "        avg_training_loss = 0\n",
    "        avg_testing_loss = 0\n",
    "\n",
    "\n",
    "        # Epoch Loop\n",
    "        for epoch in range(epochs):\n",
    "            print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "\n",
    "            avg_training_loss = 0\n",
    "            avg_testing_loss = 0\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            for x, Y in tqdm(train_loader, desc='Training', unit=' batch'):\n",
    "                # Transfer images to GPU\n",
    "                x = x.to(device)\n",
    "                Y = Y.to(device)\n",
    "\n",
    "                # Zero out gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Send images to model\n",
    "                x_pred = model(x)\n",
    "\n",
    "                # Calc loss\n",
    "                loss = loss_function(x_pred, Y, pos_weight_tensor)\n",
    "\n",
    "                # Calc gradient and update weights\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    avg_training_loss += loss.item()\n",
    "\n",
    "            # Switch to eval mode\n",
    "            model.eval()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for x, Y in tqdm(test_loader, desc='Testing', unit=' batches'):\n",
    "                    # Move the images to the GPU\n",
    "                    x = x.to(device)\n",
    "                    Y = Y.to(device)\n",
    "\n",
    "                    # Get logits and sum up total loss\n",
    "                    x_pred = model(x)\n",
    "                    avg_testing_loss += loss_function(x_pred, Y, pos_weight_tensor).item()\n",
    "\n",
    "            # Get training loss\n",
    "            avg_training_loss /= len(train_loader)\n",
    "\n",
    "             # Get testing loss\n",
    "            avg_testing_loss /= len(test_loader)\n",
    "\n",
    "            scheduler.step(avg_testing_loss)\n",
    "\n",
    "            # Switch model back to training mode\n",
    "            model.train()\n",
    "\n",
    "            epoch_over_training.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"training_loss\": avg_training_loss\n",
    "                })\n",
    "\n",
    "            epoch_over_testing.append({\n",
    "                \"epoch\": epoch,\n",
    "                \"testing_loss\": avg_testing_loss\n",
    "                })\n",
    "\n",
    "\n",
    "            print(\"\")\n",
    "\n",
    "            print(f'   -> Training Loss: {avg_training_loss: .4f}\\n')\n",
    "            print(f'   -> Testing Loss: {avg_testing_loss: .4f}\\n')\n",
    "\n",
    "\n",
    "        return epoch_over_training, epoch_over_testing\n",
    "\n",
    "\n",
    "    def critic_and_actor_training_loop(self, hp_c, hp_a, num_of_games = 5, epochs=10, difficulty: str = 'medium'):\n",
    "\n",
    "        old_actor: nn.Module | None = None\n",
    "        old_critic: nn.Module | None = None\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Get new models\n",
    "            critic = Critic()\n",
    "            actor = Actor()\n",
    "\n",
    "            # Generate Initial Critic data based off of logic bot\n",
    "            x, y = self.generate_critic_training_data(num_games=num_of_games, difficulty=difficulty, actor=old_actor)\n",
    "\n",
    "            # Convert to dataloader\n",
    "            critic_train_loader, critic_test_loader = get_dataloader(x, y, 64)\n",
    "\n",
    "            # Train Critic\n",
    "            self.train_critc(critic=critic, train_loader=critic_train_loader, test_loader=critic_test_loader, hp=hp_c)\n",
    "\n",
    "            # Get actor data\n",
    "            x, y = self.generate_actor_training_data('medium', num_of_games, critic)\n",
    "\n",
    "            # Convert to dataloader\n",
    "            actor_train_loader, actor_test_loader = get_dataloader(x, y, 64)\n",
    "\n",
    "            # Train the actor\n",
    "            self.train_actor(actor=actor, train_loader=actor_train_loader, test_loader=actor_test_loader, hp=hp_a)\n",
    "\n",
    "            old_actor = actor\n",
    "\n",
    "\n",
    "        return old_actor, old_critic"
   ],
   "id": "532c2b183f7a8cd",
   "outputs": [],
   "execution_count": 278
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T19:34:04.317120Z",
     "start_time": "2025-12-13T19:34:04.310221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def start_nn_game(manager, model, difficulty: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    Runs a complete game simulation using the trained Neural Network model.\n",
    "    \"\"\"\n",
    "    manager.initialize_board(difficulty)\n",
    "    manager.is_game_over = False\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while not manager.is_game_over:\n",
    "            # 1. Get the current state (Input: X)\n",
    "            X_current = manager.get_nn_input_state().to(device)\n",
    "            # Add batch dimension: (4, 22, 22) -> (1, 4, 22, 22)\n",
    "            X_batch = X_current.unsqueeze(0)\n",
    "\n",
    "            # 2. Get the model's prediction (Output: Y_pred)\n",
    "            logits = model(X_batch)  # Shape: (1, 484)\n",
    "\n",
    "            # Convert logits to probabilities using sigmoid\n",
    "            probs = torch.sigmoid(logits)  # Range [0, 1]\n",
    "\n",
    "            # Reshape to 22x22 safety map\n",
    "            safety_map = probs.view(manager.size, manager.size)  # Shape: (22, 22)\n",
    "\n",
    "            # 3. Choose the move (The NN's decision logic)\n",
    "            r, c = get_nn_move_choice(safety_map, manager.opened_board.to(device), threshold=0.5)\n",
    "\n",
    "            if r is None:\n",
    "                # No safe moves found, game likely won or stuck\n",
    "                manager.is_game_over = True\n",
    "                break\n",
    "\n",
    "            # 4. Execute the move\n",
    "            success, game_over = manager.make_move(r, c)\n",
    "\n",
    "    # Return the results for metric calculation\n",
    "    return {\n",
    "        \"success\": manager.check_win_condition(),\n",
    "        \"moves_taken\": manager.moves_taken,\n",
    "        \"mines_triggered\": manager.mines_triggered\n",
    "    }\n",
    "\n",
    "def get_nn_move_choice(safety_map, opened_board, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Randomly selects from unopened cells predicted to be safe.\n",
    "    Args:\n",
    "        safety_map: (22, 22) tensor of safety predictions [0, 1]\n",
    "        opened_board: (22, 22) tensor marking opened cells\n",
    "        threshold: minimum score to consider a cell \"safe\"\n",
    "    \"\"\"\n",
    "    # Create mask for unopened cells\n",
    "    unopened_mask = (opened_board == 0)\n",
    "\n",
    "    # Find cells that are both unopened AND predicted safe\n",
    "    safe_unopened = (safety_map >= threshold) & unopened_mask\n",
    "\n",
    "    # Get coordinates of all safe unopened cells\n",
    "    safe_coords = torch.where(safe_unopened)\n",
    "\n",
    "    if len(safe_coords[0]) == 0:\n",
    "        # No cells predicted as safe, fall back to highest score\n",
    "        # (This handles situations where all remaining cells look risky)\n",
    "        masked_safety_map = safety_map.clone()\n",
    "        masked_safety_map[opened_board == 1] = -1e9\n",
    "\n",
    "        max_score = masked_safety_map.max()\n",
    "        if max_score < -1e8:\n",
    "            return None, None\n",
    "\n",
    "        r_idx, c_idx = torch.where(masked_safety_map == max_score)\n",
    "        choice = torch.randint(0, len(r_idx), (1,)).item() if len(r_idx) > 1 else 0\n",
    "        return r_idx[choice].item(), c_idx[choice].item()\n",
    "\n",
    "    # Randomly select from safe cells\n",
    "    choice = torch.randint(0, len(safe_coords[0]), (1,)).item()\n",
    "    return safe_coords[0][choice].item(), safe_coords[1][choice].item()"
   ],
   "id": "bb6ab080429d4137",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T19:34:38.483009Z",
     "start_time": "2025-12-13T19:34:38.475063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def start_nn_game(manager, model, difficulty: str, device: torch.device):\n",
    "    \"\"\"\n",
    "    Runs a complete game simulation using the trained Neural Network model.\n",
    "    \"\"\"\n",
    "    manager.initialize_board(difficulty)\n",
    "    manager.is_game_over = False\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while not manager.is_game_over:\n",
    "            # 1. Get the current state (Input: X)\n",
    "            X_current = manager.get_nn_input_state().to(device)\n",
    "            # Add batch dimension: (4, 22, 22) -> (1, 4, 22, 22)\n",
    "            X_batch = X_current.unsqueeze(0)\n",
    "\n",
    "            # 2. Get the model's prediction (Output: Y_pred)\n",
    "            logits = model(X_batch)  # Shape: (1, 484)\n",
    "\n",
    "            # Convert logits to probabilities using sigmoid\n",
    "            probs = torch.sigmoid(logits)  # Range [0, 1]\n",
    "\n",
    "            # Reshape to 22x22 safety map\n",
    "            safety_map = probs.view(manager.size, manager.size)  # Shape: (22, 22)\n",
    "\n",
    "            # 3. Choose the move (The NN's decision logic)\n",
    "            r, c = get_nn_move_choice(safety_map, manager.opened_board.to(device))\n",
    "\n",
    "            if r is None:\n",
    "                # No safe moves found, game likely won or stuck\n",
    "                manager.is_game_over = True\n",
    "                break\n",
    "\n",
    "            # 4. Execute the move\n",
    "            success, game_over = manager.make_move(r, c)\n",
    "\n",
    "    # Return the results for metric calculation\n",
    "    return {\n",
    "        \"success\": manager.check_win_condition(),\n",
    "        \"moves_taken\": manager.moves_taken,\n",
    "        \"mines_triggered\": manager.mines_triggered\n",
    "    }\n",
    "\n",
    "\n",
    "def get_nn_move_choice(safety_map, opened_board):\n",
    "    \"\"\"\n",
    "    Finds the unopened cell with the highest predicted safety score.\n",
    "    Args:\n",
    "        safety_map: (22, 22) tensor of safety predictions [0, 1]\n",
    "        opened_board: (22, 22) tensor marking opened cells\n",
    "    \"\"\"\n",
    "    # Mask out opened cells by setting their scores to a very low value\n",
    "    masked_safety_map = safety_map.clone()\n",
    "    masked_safety_map[opened_board == 1] = -1e9\n",
    "\n",
    "    # Find the maximum score\n",
    "    max_score = masked_safety_map.max()\n",
    "\n",
    "    # Check if all cells are opened/invalid\n",
    "    if max_score < -1e8:\n",
    "        return None, None\n",
    "\n",
    "    # Find coordinates of the max score\n",
    "    r_idx, c_idx = torch.where(masked_safety_map == max_score)\n",
    "\n",
    "    # If there are ties, pick one randomly\n",
    "    if len(r_idx) > 1:\n",
    "        choice = torch.randint(0, len(r_idx), (1,)).item()\n",
    "        return r_idx[choice].item(), c_idx[choice].item()\n",
    "    else:\n",
    "        return r_idx[0].item(), c_idx[0].item()"
   ],
   "id": "f7b7e59ac57a1085",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T22:45:50.456491Z",
     "start_time": "2025-12-13T22:45:44.705857Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "cce773cba041bddd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Critic data for MEDIUM (5-channel approach)...\n"
     ]
    }
   ],
   "execution_count": 245
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T00:53:41.195803Z",
     "start_time": "2025-12-14T00:53:41.188628Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MSDataset(Dataset):\n",
    "\n",
    "    def __init__(self, X: torch.Tensor, y: torch.Tensor):\n",
    "\n",
    "        self.X = X.float()\n",
    "\n",
    "        # CRITICAL FIX: Only reshape Y if it is not scalar data (i.e., not Critic data).\n",
    "        # We need to preserve the shape of scalar Critic data (N,)\n",
    "        if y.ndim > 1:\n",
    "            self.y = y.float().view(y.size(0), -1) # Reshape the label to match model output (N x 484)\n",
    "        else:\n",
    "            # If y is already a 1D tensor of scalar values (Critic data), keep it as is.\n",
    "            self.y = y.float()\n",
    "\n",
    "        self.size = X.size(-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Retrieve the original sample\n",
    "        # X is (C, 22, 22). Y is either a scalar (Critic) or (484) vector (Actor).\n",
    "\n",
    "        # Use .clone().contiguous() to ensure data is independently owned\n",
    "        X = self.X[idx].clone().contiguous()\n",
    "        Y = self.y[idx].clone().contiguous()\n",
    "\n",
    "        # --- NO AUGMENTATION LOGIC ---\n",
    "        # All original steps 2, 3, 4, and the intermediate reshaping of Y are removed.\n",
    "\n",
    "        # 2. Final Step: Ensure contiguity (though less critical without flips/rotations)\n",
    "        X = X.contiguous()\n",
    "        Y = Y.contiguous()\n",
    "\n",
    "        # Y is already in the final required shape (e.g., (484) for Actor, (1) for Critic)\n",
    "        # from the __init__ method, so no final reshape is needed here.\n",
    "\n",
    "        return X, Y"
   ],
   "id": "4a4a09826abeef0f",
   "outputs": [],
   "execution_count": 256
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T20:12:18.903995Z",
     "start_time": "2025-12-13T20:12:18.895059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MSSafeSquares(nn.Module):\n",
    "\n",
    "    def __init__(self, lin_dropout=0, cnn_dropout=0):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.BOARD_SIZE = 22\n",
    "\n",
    "        cnn_layer_1_size = 32\n",
    "        cnn_layer_2_size = 64\n",
    "        cnn_layer_3_size = 128\n",
    "        cnn_layer_4_size = 256\n",
    "\n",
    "        lin_layer_1_size = 2000\n",
    "        lin_layer_2_size = 1500\n",
    "        lin_layer_3_size = 1000\n",
    "        lin_layer_4_size = 500\n",
    "\n",
    "        self.CNN = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=cnn_layer_1_size, kernel_size=5, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(cnn_layer_1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=cnn_dropout),\n",
    "\n",
    "\n",
    "            nn.Conv2d(in_channels=cnn_layer_1_size, out_channels=cnn_layer_2_size, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(cnn_layer_2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout2d(p=cnn_dropout),\n",
    "\n",
    "            nn.Conv2d(in_channels=cnn_layer_2_size, out_channels=cnn_layer_3_size, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(cnn_layer_3_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True),\n",
    "            nn.Dropout2d(p=cnn_dropout),\n",
    "\n",
    "            nn.Conv2d(in_channels=cnn_layer_3_size, out_channels=cnn_layer_4_size, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(cnn_layer_4_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(p=cnn_dropout),\n",
    "        )\n",
    "\n",
    "        total_count = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_input = torch.zeros(1, 4, self.BOARD_SIZE, self.BOARD_SIZE)\n",
    "\n",
    "            test_input.to(device)\n",
    "\n",
    "            features = self.CNN(test_input)\n",
    "\n",
    "            total_count = features.view(1, -1).size(1)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=total_count, out_features=lin_layer_1_size),\n",
    "            nn.BatchNorm1d(lin_layer_1_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(lin_dropout),\n",
    "\n",
    "            nn.Linear(in_features=lin_layer_1_size, out_features=lin_layer_2_size),\n",
    "            nn.BatchNorm1d(lin_layer_2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(lin_dropout),\n",
    "\n",
    "            nn.Linear(in_features=lin_layer_2_size, out_features=lin_layer_3_size),\n",
    "            nn.BatchNorm1d(lin_layer_3_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(lin_dropout),\n",
    "\n",
    "            nn.Linear(in_features=lin_layer_3_size, out_features=lin_layer_4_size),\n",
    "            nn.BatchNorm1d(lin_layer_4_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(lin_dropout),\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(in_features=lin_layer_4_size, out_features=self.BOARD_SIZE * self.BOARD_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        logits = self.CNN(x)\n",
    "        logits = self.classifier(logits)\n",
    "\n",
    "        logits = self.output_layer(logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "def masked_bce_loss(predictions, targets: torch.Tensor, pos_weight):\n",
    "    \"\"\"\n",
    "    BCE loss that ignores masked cells (where target == -1)\n",
    "    \"\"\"\n",
    "    # Create mask: only compute loss where target != -1\n",
    "    mask = (targets != -1.0).float()\n",
    "\n",
    "    # Compute BCE loss\n",
    "    bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight, reduction='none')\n",
    "    loss = bce(predictions, targets.clamp(0, 1))  # Clamp to convert -1 to 0 for loss calculation\n",
    "\n",
    "    # Apply mask and average only over valid cells\n",
    "    masked_loss = (loss * mask).sum() / mask.sum()\n",
    "\n",
    "    return masked_loss"
   ],
   "id": "b62cb52c0252819b",
   "outputs": [],
   "execution_count": 225
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MS_X_easy = torch.load('./data/X_train_easy.pt')\n",
    "MS_X_med = torch.load('./data/X_train_medium.pt')\n",
    "MS_X_hard = torch.load('./data/X_train_hard.pt')\n",
    "\n",
    "MS_Y_easy = torch.load('./data/Y_train_easy.pt')\n",
    "MS_Y_med = torch.load('./data/Y_train_medium.pt')\n",
    "MS_Y_hard = torch.load('./data/Y_train_hard.pt')\n"
   ],
   "id": "4bb9637bf8fdeff7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T20:51:18.035375Z",
     "start_time": "2025-12-13T20:51:16.604202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataloader(X_dataset, Y_dataset, batch_size):\n",
    "    total_size = len(X_dataset)\n",
    "\n",
    "    train_size = int(0.8 * total_size)\n",
    "\n",
    "    label_size = total_size - train_size\n",
    "\n",
    "    X_train, X_test = torch.split(X_dataset, [train_size, label_size])\n",
    "    Y_train, Y_test = torch.split(Y_dataset, [train_size, label_size])\n",
    "\n",
    "    train_dataset = MSDataset(X_train, Y_train)\n",
    "    test_dataset = MSDataset(X_test, Y_test)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=5,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=5,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "MS_train_loader_easy, MS_test_loader_easy = get_dataloader(X, y, 64)\n",
    "MS_train_loader_medium, MS_test_loader_medium = get_dataloader(MS_X_med, MS_Y_med, 64)\n",
    "MS_train_loader_hard,MS_test_loader_hard = get_dataloader(MS_X_hard, MS_Y_hard, 64)\n",
    "\n",
    "\n",
    "MS_X_all = torch.concat([MS_X_easy, MS_X_med, MS_X_hard], dim=0)\n",
    "MS_Y_all = torch.concat([MS_Y_easy, MS_Y_med, MS_Y_hard], dim=0)\n",
    "\n",
    "MS_train_loader_all, MS_test_loader_all = get_dataloader(MS_X_all, MS_Y_all, 64)"
   ],
   "id": "2d1087cbe2234832",
   "outputs": [],
   "execution_count": 235
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T20:12:26.046288Z",
     "start_time": "2025-12-13T20:12:26.038777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def training_loop(train_loader, test_loader, hp: dict):\n",
    "\n",
    "    epoch_over_training = []\n",
    "    epoch_over_testing = []\n",
    "\n",
    "    # Hyperparameter setup\n",
    "    epochs = hp['epochs']\n",
    "    learning_rate = hp['learning_rate']\n",
    "    decay_rate = hp['decay_rate']\n",
    "\n",
    "    c_dropout = hp['cnn_dropout']\n",
    "    f_dropout = hp['linear_dropout']\n",
    "\n",
    "    print('######## Beginning training for MS Safe Square Predictor ##########')\n",
    "\n",
    "    model = MSSafeSquares(lin_dropout=f_dropout, cnn_dropout=c_dropout)\n",
    "    model.to(device)\n",
    "\n",
    "    pos_weight_tensor = torch.tensor(30.0, device=device)\n",
    "\n",
    "    loss_function = masked_bce_loss\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(),\n",
    "                           lr=learning_rate,\n",
    "                           weight_decay=decay_rate\n",
    "                           )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    # Have references to variables outside of the epoch loop\n",
    "\n",
    "    avg_training_loss = 0\n",
    "    avg_testing_loss = 0\n",
    "\n",
    "\n",
    "    # Epoch Loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f'----- Epoch: {epoch + 1}/{epochs} -----')\n",
    "\n",
    "        avg_training_loss = 0\n",
    "        avg_testing_loss = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for x, Y in tqdm(train_loader, desc='Training', unit=' batch'):\n",
    "            # Transfer images to GPU\n",
    "            x = x.to(device)\n",
    "            Y = Y.to(device)\n",
    "\n",
    "            # Zero out gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Send images to model\n",
    "            x_pred = model(x)\n",
    "\n",
    "            # Calc loss\n",
    "            loss = loss_function(x_pred, Y, pos_weight_tensor)\n",
    "\n",
    "            # Calc gradient and update weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                avg_training_loss += loss.item()\n",
    "\n",
    "        # Switch to eval mode\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, Y in tqdm(test_loader, desc='Testing', unit=' batches'):\n",
    "                # Move the images to the GPU\n",
    "                x = x.to(device)\n",
    "                Y = Y.to(device)\n",
    "\n",
    "                # Get logits and sum up total loss\n",
    "                x_pred = model(x)\n",
    "                avg_testing_loss += loss_function(x_pred, Y, pos_weight_tensor).item()\n",
    "\n",
    "        # Get training loss\n",
    "        avg_training_loss /= len(train_loader)\n",
    "\n",
    "         # Get testing loss\n",
    "        avg_testing_loss /= len(test_loader)\n",
    "\n",
    "        scheduler.step(avg_testing_loss)\n",
    "\n",
    "        # Switch model back to training mode\n",
    "        model.train()\n",
    "\n",
    "        epoch_over_training.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"training_loss\": avg_training_loss\n",
    "            })\n",
    "\n",
    "        epoch_over_testing.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"testing_loss\": avg_testing_loss\n",
    "            })\n",
    "\n",
    "\n",
    "        print(\"\")\n",
    "\n",
    "        print(f'   -> Training Loss: {avg_training_loss: .4f}\\n')\n",
    "        print(f'   -> Testing Loss: {avg_testing_loss: .4f}\\n')\n",
    "\n",
    "    return model, epoch_over_training, epoch_over_testing"
   ],
   "id": "865f3b3eef788e21",
   "outputs": [],
   "execution_count": 226
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T21:01:45.784977Z",
     "start_time": "2025-12-13T20:51:21.720367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hyperparameters = {\n",
    "    'epochs': 100,\n",
    "    'learning_rate': 1e-2,\n",
    "    'decay_rate': 1e-3,\n",
    "    'cnn_dropout': 0.0,\n",
    "    'linear_dropout': 0.25\n",
    "}\n",
    "\n",
    "model_easy, train, test = training_loop(MS_train_loader_easy, MS_test_loader_easy, hyperparameters)"
   ],
   "id": "8bb7304cd495c694",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## Beginning training for MS Safe Square Predictor ##########\n",
      "----- Epoch: 1/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 224.18 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 352.89 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.4672\n",
      "\n",
      "   -> Testing Loss:  1.3955\n",
      "\n",
      "----- Epoch: 2/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 227.02 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 355.77 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.3096\n",
      "\n",
      "   -> Testing Loss:  1.2867\n",
      "\n",
      "----- Epoch: 3/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 230.50 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 349.30 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.2461\n",
      "\n",
      "   -> Testing Loss:  1.2281\n",
      "\n",
      "----- Epoch: 4/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 230.67 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 350.02 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.2073\n",
      "\n",
      "   -> Testing Loss:  1.2117\n",
      "\n",
      "----- Epoch: 5/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 226.77 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 365.88 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.1920\n",
      "\n",
      "   -> Testing Loss:  1.1985\n",
      "\n",
      "----- Epoch: 6/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 220.37 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 343.16 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.1765\n",
      "\n",
      "   -> Testing Loss:  1.1815\n",
      "\n",
      "----- Epoch: 7/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 229.31 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 355.69 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.1532\n",
      "\n",
      "   -> Testing Loss:  1.1500\n",
      "\n",
      "----- Epoch: 8/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 226.60 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 358.17 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.1279\n",
      "\n",
      "   -> Testing Loss:  1.1318\n",
      "\n",
      "----- Epoch: 9/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 222.66 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 352.23 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.1120\n",
      "\n",
      "   -> Testing Loss:  1.1223\n",
      "\n",
      "----- Epoch: 10/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 226.23 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 353.23 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.1013\n",
      "\n",
      "   -> Testing Loss:  1.1115\n",
      "\n",
      "----- Epoch: 11/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 224.58 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 344.59 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0890\n",
      "\n",
      "   -> Testing Loss:  1.1006\n",
      "\n",
      "----- Epoch: 12/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 223.77 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 353.26 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0791\n",
      "\n",
      "   -> Testing Loss:  1.0977\n",
      "\n",
      "----- Epoch: 13/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 226.41 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 351.35 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0730\n",
      "\n",
      "   -> Testing Loss:  1.0956\n",
      "\n",
      "----- Epoch: 14/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 220.26 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 357.66 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0696\n",
      "\n",
      "   -> Testing Loss:  1.0849\n",
      "\n",
      "----- Epoch: 15/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 222.95 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 347.51 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0594\n",
      "\n",
      "   -> Testing Loss:  1.0798\n",
      "\n",
      "----- Epoch: 16/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.46 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 353.32 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0554\n",
      "\n",
      "   -> Testing Loss:  1.0795\n",
      "\n",
      "----- Epoch: 17/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 225.36 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 351.99 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0525\n",
      "\n",
      "   -> Testing Loss:  1.0815\n",
      "\n",
      "----- Epoch: 18/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 223.32 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 346.95 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0485\n",
      "\n",
      "   -> Testing Loss:  1.0672\n",
      "\n",
      "----- Epoch: 19/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 229.31 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 350.62 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0435\n",
      "\n",
      "   -> Testing Loss:  1.0645\n",
      "\n",
      "----- Epoch: 20/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 228.81 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 355.70 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0379\n",
      "\n",
      "   -> Testing Loss:  1.0721\n",
      "\n",
      "----- Epoch: 21/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 227.33 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 345.03 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0364\n",
      "\n",
      "   -> Testing Loss:  1.0568\n",
      "\n",
      "----- Epoch: 22/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 223.28 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 349.49 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0325\n",
      "\n",
      "   -> Testing Loss:  1.0519\n",
      "\n",
      "----- Epoch: 23/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 226.05 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 343.52 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0278\n",
      "\n",
      "   -> Testing Loss:  1.0515\n",
      "\n",
      "----- Epoch: 24/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 223.04 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 355.89 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0237\n",
      "\n",
      "   -> Testing Loss:  1.0501\n",
      "\n",
      "----- Epoch: 25/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 223.49 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 349.98 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0212\n",
      "\n",
      "   -> Testing Loss:  1.0501\n",
      "\n",
      "----- Epoch: 26/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 222.67 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 348.64 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0190\n",
      "\n",
      "   -> Testing Loss:  1.0421\n",
      "\n",
      "----- Epoch: 27/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 224.20 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 343.62 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0152\n",
      "\n",
      "   -> Testing Loss:  1.0413\n",
      "\n",
      "----- Epoch: 28/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 222.97 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 356.34 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0108\n",
      "\n",
      "   -> Testing Loss:  1.0346\n",
      "\n",
      "----- Epoch: 29/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 225.40 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 352.64 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0094\n",
      "\n",
      "   -> Testing Loss:  1.0325\n",
      "\n",
      "----- Epoch: 30/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.12 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 348.75 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0068\n",
      "\n",
      "   -> Testing Loss:  1.0353\n",
      "\n",
      "----- Epoch: 31/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 224.52 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 346.71 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0042\n",
      "\n",
      "   -> Testing Loss:  1.0279\n",
      "\n",
      "----- Epoch: 32/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 226.72 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 352.37 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  1.0019\n",
      "\n",
      "   -> Testing Loss:  1.0280\n",
      "\n",
      "----- Epoch: 33/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 224.47 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 353.26 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9996\n",
      "\n",
      "   -> Testing Loss:  1.0282\n",
      "\n",
      "----- Epoch: 34/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 224.19 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 350.54 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9986\n",
      "\n",
      "   -> Testing Loss:  1.0226\n",
      "\n",
      "----- Epoch: 35/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 222.20 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 340.92 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9962\n",
      "\n",
      "   -> Testing Loss:  1.0219\n",
      "\n",
      "----- Epoch: 36/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 219.38 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 351.87 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9948\n",
      "\n",
      "   -> Testing Loss:  1.0207\n",
      "\n",
      "----- Epoch: 37/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 223.22 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 348.18 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9927\n",
      "\n",
      "   -> Testing Loss:  1.0137\n",
      "\n",
      "----- Epoch: 38/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 224.14 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 347.63 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9906\n",
      "\n",
      "   -> Testing Loss:  1.0122\n",
      "\n",
      "----- Epoch: 39/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 222.36 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 351.54 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9900\n",
      "\n",
      "   -> Testing Loss:  1.0113\n",
      "\n",
      "----- Epoch: 40/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 226.41 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 356.97 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9871\n",
      "\n",
      "   -> Testing Loss:  1.0122\n",
      "\n",
      "----- Epoch: 41/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 223.60 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 350.21 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9886\n",
      "\n",
      "   -> Testing Loss:  1.0110\n",
      "\n",
      "----- Epoch: 42/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 223.83 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 349.72 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9851\n",
      "\n",
      "   -> Testing Loss:  1.0089\n",
      "\n",
      "----- Epoch: 43/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 227.82 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 351.72 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9831\n",
      "\n",
      "   -> Testing Loss:  1.0017\n",
      "\n",
      "----- Epoch: 44/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 225.42 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 350.40 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9830\n",
      "\n",
      "   -> Testing Loss:  1.0117\n",
      "\n",
      "----- Epoch: 45/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 224.09 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 347.87 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9813\n",
      "\n",
      "   -> Testing Loss:  1.0081\n",
      "\n",
      "----- Epoch: 46/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.27 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 347.50 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9804\n",
      "\n",
      "   -> Testing Loss:  0.9996\n",
      "\n",
      "----- Epoch: 47/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.67 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 354.27 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9779\n",
      "\n",
      "   -> Testing Loss:  0.9979\n",
      "\n",
      "----- Epoch: 48/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 228.43 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 347.57 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9776\n",
      "\n",
      "   -> Testing Loss:  0.9905\n",
      "\n",
      "----- Epoch: 49/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 222.44 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 349.72 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9755\n",
      "\n",
      "   -> Testing Loss:  0.9889\n",
      "\n",
      "----- Epoch: 50/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 228.59 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 350.65 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9748\n",
      "\n",
      "   -> Testing Loss:  0.9905\n",
      "\n",
      "----- Epoch: 51/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.81 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 340.36 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9728\n",
      "\n",
      "   -> Testing Loss:  0.9865\n",
      "\n",
      "----- Epoch: 52/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.57 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 348.92 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9705\n",
      "\n",
      "   -> Testing Loss:  0.9766\n",
      "\n",
      "----- Epoch: 53/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 223.15 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 351.16 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9701\n",
      "\n",
      "   -> Testing Loss:  0.9886\n",
      "\n",
      "----- Epoch: 54/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.17 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 353.62 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9697\n",
      "\n",
      "   -> Testing Loss:  0.9885\n",
      "\n",
      "----- Epoch: 55/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 219.66 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 347.52 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9683\n",
      "\n",
      "   -> Testing Loss:  0.9806\n",
      "\n",
      "----- Epoch: 56/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 225.26 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 355.36 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9694\n",
      "\n",
      "   -> Testing Loss:  0.9799\n",
      "\n",
      "----- Epoch: 57/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 224.00 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 346.45 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9649\n",
      "\n",
      "   -> Testing Loss:  0.9862\n",
      "\n",
      "----- Epoch: 58/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 222.20 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 350.15 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9650\n",
      "\n",
      "   -> Testing Loss:  0.9759\n",
      "\n",
      "----- Epoch: 59/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.82 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 348.20 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9665\n",
      "\n",
      "   -> Testing Loss:  0.9771\n",
      "\n",
      "----- Epoch: 60/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 223.20 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 349.67 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9657\n",
      "\n",
      "   -> Testing Loss:  0.9750\n",
      "\n",
      "----- Epoch: 61/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 222.48 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 353.16 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9635\n",
      "\n",
      "   -> Testing Loss:  0.9732\n",
      "\n",
      "----- Epoch: 62/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.71 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 338.28 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9623\n",
      "\n",
      "   -> Testing Loss:  0.9738\n",
      "\n",
      "----- Epoch: 63/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 222.34 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 346.83 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9625\n",
      "\n",
      "   -> Testing Loss:  0.9735\n",
      "\n",
      "----- Epoch: 64/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 223.03 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 343.59 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9614\n",
      "\n",
      "   -> Testing Loss:  0.9750\n",
      "\n",
      "----- Epoch: 65/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.20 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 350.40 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9620\n",
      "\n",
      "   -> Testing Loss:  0.9644\n",
      "\n",
      "----- Epoch: 66/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.49 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 355.48 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9606\n",
      "\n",
      "   -> Testing Loss:  0.9625\n",
      "\n",
      "----- Epoch: 67/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 220.70 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 349.50 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9585\n",
      "\n",
      "   -> Testing Loss:  0.9675\n",
      "\n",
      "----- Epoch: 68/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 219.33 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 349.77 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9590\n",
      "\n",
      "   -> Testing Loss:  0.9684\n",
      "\n",
      "----- Epoch: 69/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 220.24 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 353.90 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9598\n",
      "\n",
      "   -> Testing Loss:  0.9624\n",
      "\n",
      "----- Epoch: 70/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 218.78 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 352.30 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9577\n",
      "\n",
      "   -> Testing Loss:  0.9670\n",
      "\n",
      "----- Epoch: 71/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 223.23 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 345.32 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9564\n",
      "\n",
      "   -> Testing Loss:  0.9595\n",
      "\n",
      "----- Epoch: 72/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 220.17 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 342.34 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9578\n",
      "\n",
      "   -> Testing Loss:  0.9666\n",
      "\n",
      "----- Epoch: 73/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 224.15 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 352.18 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9553\n",
      "\n",
      "   -> Testing Loss:  0.9639\n",
      "\n",
      "----- Epoch: 74/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 222.24 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 356.28 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9554\n",
      "\n",
      "   -> Testing Loss:  0.9678\n",
      "\n",
      "----- Epoch: 75/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 222.76 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 340.15 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9556\n",
      "\n",
      "   -> Testing Loss:  0.9610\n",
      "\n",
      "----- Epoch: 76/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 219.40 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 352.97 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9527\n",
      "\n",
      "   -> Testing Loss:  0.9681\n",
      "\n",
      "----- Epoch: 77/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 226.65 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 351.85 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9534\n",
      "\n",
      "   -> Testing Loss:  0.9639\n",
      "\n",
      "----- Epoch: 78/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.86 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 340.15 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9335\n",
      "\n",
      "   -> Testing Loss:  0.9434\n",
      "\n",
      "----- Epoch: 79/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 219.81 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 350.70 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9283\n",
      "\n",
      "   -> Testing Loss:  0.9385\n",
      "\n",
      "----- Epoch: 80/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.41 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 342.73 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9262\n",
      "\n",
      "   -> Testing Loss:  0.9398\n",
      "\n",
      "----- Epoch: 81/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 225.37 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 348.57 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9240\n",
      "\n",
      "   -> Testing Loss:  0.9387\n",
      "\n",
      "----- Epoch: 82/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 226.33 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 345.77 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9228\n",
      "\n",
      "   -> Testing Loss:  0.9355\n",
      "\n",
      "----- Epoch: 83/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 226.06 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 344.70 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9211\n",
      "\n",
      "   -> Testing Loss:  0.9341\n",
      "\n",
      "----- Epoch: 84/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 218.10 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 339.68 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9199\n",
      "\n",
      "   -> Testing Loss:  0.9344\n",
      "\n",
      "----- Epoch: 85/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 223.54 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 329.38 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9181\n",
      "\n",
      "   -> Testing Loss:  0.9357\n",
      "\n",
      "----- Epoch: 86/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.57 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 342.83 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9177\n",
      "\n",
      "   -> Testing Loss:  0.9335\n",
      "\n",
      "----- Epoch: 87/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.61 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 344.01 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9167\n",
      "\n",
      "   -> Testing Loss:  0.9307\n",
      "\n",
      "----- Epoch: 88/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 223.24 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 354.57 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9160\n",
      "\n",
      "   -> Testing Loss:  0.9312\n",
      "\n",
      "----- Epoch: 89/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 224.15 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 345.15 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9151\n",
      "\n",
      "   -> Testing Loss:  0.9283\n",
      "\n",
      "----- Epoch: 90/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.96 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 358.24 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9145\n",
      "\n",
      "   -> Testing Loss:  0.9312\n",
      "\n",
      "----- Epoch: 91/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 224.59 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 345.64 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9133\n",
      "\n",
      "   -> Testing Loss:  0.9342\n",
      "\n",
      "----- Epoch: 92/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 220.48 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 348.79 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9125\n",
      "\n",
      "   -> Testing Loss:  0.9317\n",
      "\n",
      "----- Epoch: 93/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 221.80 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 348.42 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9117\n",
      "\n",
      "   -> Testing Loss:  0.9331\n",
      "\n",
      "----- Epoch: 94/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 219.90 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 350.95 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9111\n",
      "\n",
      "   -> Testing Loss:  0.9292\n",
      "\n",
      "----- Epoch: 95/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 223.34 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 358.50 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9109\n",
      "\n",
      "   -> Testing Loss:  0.9293\n",
      "\n",
      "----- Epoch: 96/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 224.81 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 340.56 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.9017\n",
      "\n",
      "   -> Testing Loss:  0.9194\n",
      "\n",
      "----- Epoch: 97/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 223.34 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 354.73 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.8985\n",
      "\n",
      "   -> Testing Loss:  0.9182\n",
      "\n",
      "----- Epoch: 98/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 219.02 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 351.94 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.8973\n",
      "\n",
      "   -> Testing Loss:  0.9192\n",
      "\n",
      "----- Epoch: 99/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 225.45 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 354.73 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.8964\n",
      "\n",
      "   -> Testing Loss:  0.9167\n",
      "\n",
      "----- Epoch: 100/100 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1201/1201 [00:05<00:00, 222.18 batch/s]\n",
      "Testing: 100%|| 301/301 [00:00<00:00, 345.78 batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.8954\n",
      "\n",
      "   -> Testing Loss:  0.9151\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 236
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T21:04:36.940240Z",
     "start_time": "2025-12-13T21:04:33.843251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "avg_moves = 0\n",
    "\n",
    "num_of_games = 100\n",
    "\n",
    "for i in range(num_of_games):\n",
    "    results = start_nn_game(MSGameManager(), model_easy, 'easy', device)\n",
    "    avg_moves += results['moves_taken']\n",
    "\n",
    "avg_moves /= num_of_games\n",
    "\n",
    "print(avg_moves)"
   ],
   "id": "5a8932b07054dcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.16\n"
     ]
    }
   ],
   "execution_count": 241
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T01:13:57.254925Z",
     "start_time": "2025-12-14T01:13:57.049490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, linear_dropout=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.BOARD_SIZE = 22\n",
    "\n",
    "        cnn_layer_1_size = 32\n",
    "        cnn_layer_2_size = 32\n",
    "        cnn_layer_3_size = 64\n",
    "        cnn_layer_4_size = 64\n",
    "\n",
    "        self.CNN = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=5, out_channels=cnn_layer_1_size, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=cnn_layer_1_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=cnn_layer_1_size, out_channels=cnn_layer_2_size, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=cnn_layer_2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=cnn_layer_2_size, out_channels=cnn_layer_3_size, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=cnn_layer_3_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=cnn_layer_3_size, out_channels=cnn_layer_4_size, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=cnn_layer_4_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        total_count = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_input = torch.zeros(1, 5, self.BOARD_SIZE, self.BOARD_SIZE)\n",
    "\n",
    "            test_input.to(device)\n",
    "\n",
    "            features = self.CNN(test_input)\n",
    "\n",
    "            total_count = features.view(1, -1).size(1)\n",
    "\n",
    "        lin_layer_1_size = 2000\n",
    "        lin_layer_2_size = 2000\n",
    "        lin_layer_3_size = 1500\n",
    "        lin_layer_4_size = 500\n",
    "\n",
    "\n",
    "        self.Classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=total_count, out_features=lin_layer_1_size),\n",
    "            nn.BatchNorm1d(num_features=lin_layer_1_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=linear_dropout),\n",
    "\n",
    "            nn.Linear(in_features=lin_layer_1_size, out_features=lin_layer_2_size),\n",
    "            nn.BatchNorm1d(num_features=lin_layer_2_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=linear_dropout),\n",
    "\n",
    "            nn.Linear(in_features=lin_layer_2_size, out_features=lin_layer_3_size),\n",
    "            nn.BatchNorm1d(num_features=lin_layer_3_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=linear_dropout),\n",
    "\n",
    "            nn.Linear(in_features=lin_layer_3_size, out_features=lin_layer_4_size),\n",
    "            nn.BatchNorm1d(num_features=lin_layer_4_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=linear_dropout),\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(in_features=lin_layer_4_size, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        logits = self.CNN(x)\n",
    "        logits = self.Classifier(logits)\n",
    "\n",
    "        logits = self.output_layer(logits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, linear_dropout=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.BOARD_SIZE = 22\n",
    "\n",
    "        cnn_layer_1_size = 32\n",
    "        cnn_layer_2_size = 32\n",
    "        cnn_layer_3_size = 64\n",
    "        cnn_layer_4_size = 64\n",
    "\n",
    "        self.CNN = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=cnn_layer_1_size, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=cnn_layer_1_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=cnn_layer_1_size, out_channels=cnn_layer_2_size, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=cnn_layer_2_size),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=cnn_layer_2_size, out_channels=cnn_layer_3_size, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=cnn_layer_3_size),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=cnn_layer_3_size, out_channels=cnn_layer_4_size, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_features=cnn_layer_4_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        total_count = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            test_input = torch.zeros(1, 4, self.BOARD_SIZE, self.BOARD_SIZE)\n",
    "\n",
    "            test_input.to(device)\n",
    "\n",
    "            features = self.CNN(test_input)\n",
    "\n",
    "            total_count = features.view(1, -1).size(1)\n",
    "\n",
    "        lin_layer_1_size = 2000\n",
    "        lin_layer_2_size = 2000\n",
    "        lin_layer_3_size = 1500\n",
    "        lin_layer_4_size = 500\n",
    "\n",
    "\n",
    "        self.Classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=total_count, out_features=lin_layer_1_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=linear_dropout),\n",
    "\n",
    "            nn.Linear(in_features=lin_layer_1_size, out_features=lin_layer_2_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=linear_dropout),\n",
    "\n",
    "            nn.Linear(in_features=lin_layer_2_size, out_features=lin_layer_3_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=linear_dropout),\n",
    "\n",
    "            nn.Linear(in_features=lin_layer_3_size, out_features=lin_layer_4_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=linear_dropout),\n",
    "        )\n",
    "\n",
    "        self.output_layer = nn.Linear(in_features=lin_layer_4_size, out_features=self.BOARD_SIZE * self.BOARD_SIZE)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        logits = self.CNN(x)\n",
    "        logits = self.Classifier(logits)\n",
    "\n",
    "        logits = self.output_layer(logits)\n",
    "\n",
    "        return logits\n"
   ],
   "id": "b2ca113c7a099819",
   "outputs": [],
   "execution_count": 279
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T01:14:49.506950Z",
     "start_time": "2025-12-14T01:13:58.850391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hp_a = {\n",
    "    'epochs': 5,\n",
    "    'learning_rate': 1e-2,\n",
    "    'decay_rate': 1e-3,\n",
    "    'cnn_dropout': 0.0,\n",
    "    'linear_dropout': 0.25\n",
    "}\n",
    "\n",
    "hp_c = {\n",
    "    'epochs': 5,\n",
    "    'learning_rate': 1e-4,\n",
    "    'decay_rate': 1e-3,\n",
    "    'cnn_dropout': 0.0,\n",
    "    'linear_dropout': 0.25\n",
    "}\n",
    "\n",
    "manager = MSGameManager()\n",
    "\n",
    "critic, actor = manager.critic_and_actor_training_loop(hp_a=hp_a, hp_c=hp_c)"
   ],
   "id": "75062e517944335d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Critic data for MEDIUM (5-channel approach)...\n",
      "######## Beginning training for MS Critic ##########\n",
      "----- Epoch: 1/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 12/12 [00:00<00:00, 27.34 batch/s]\n",
      "Testing: 100%|| 3/3 [00:00<00:00,  8.28 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  12417.3310\n",
      "\n",
      "   -> Testing Loss:  9302.8196\n",
      "\n",
      "----- Epoch: 2/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 12/12 [00:00<00:00, 26.65 batch/s]\n",
      "Testing: 100%|| 3/3 [00:00<00:00,  8.06 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  12090.2078\n",
      "\n",
      "   -> Testing Loss:  8511.6816\n",
      "\n",
      "----- Epoch: 3/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 12/12 [00:00<00:00, 27.06 batch/s]\n",
      "Testing: 100%|| 3/3 [00:00<00:00,  8.26 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  12030.1906\n",
      "\n",
      "   -> Testing Loss:  8510.4564\n",
      "\n",
      "----- Epoch: 4/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 12/12 [00:00<00:00, 27.41 batch/s]\n",
      "Testing: 100%|| 3/3 [00:00<00:00,  8.28 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  11873.5893\n",
      "\n",
      "   -> Testing Loss:  8632.2128\n",
      "\n",
      "----- Epoch: 5/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 12/12 [00:00<00:00, 27.09 batch/s]\n",
      "Testing: 100%|| 3/3 [00:00<00:00,  8.28 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  11785.3199\n",
      "\n",
      "   -> Testing Loss:  8640.1350\n",
      "\n",
      "Generating Actor training data for MEDIUM...\n",
      "######## Beginning training for MS Actor ##########\n",
      "----- Epoch: 1/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1/1 [00:00<00:00,  2.73 batch/s]\n",
      "Testing: 100%|| 1/1 [00:00<00:00,  2.75 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  20.7987\n",
      "\n",
      "   -> Testing Loss:  0.0000\n",
      "\n",
      "----- Epoch: 2/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1/1 [00:00<00:00,  2.74 batch/s]\n",
      "Testing: 100%|| 1/1 [00:00<00:00,  2.78 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.0000\n",
      "\n",
      "   -> Testing Loss:  0.0000\n",
      "\n",
      "----- Epoch: 3/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1/1 [00:00<00:00,  2.71 batch/s]\n",
      "Testing: 100%|| 1/1 [00:00<00:00,  2.76 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.0000\n",
      "\n",
      "   -> Testing Loss:  0.0000\n",
      "\n",
      "----- Epoch: 4/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1/1 [00:00<00:00,  2.72 batch/s]\n",
      "Testing: 100%|| 1/1 [00:00<00:00,  2.79 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.0000\n",
      "\n",
      "   -> Testing Loss:  0.0000\n",
      "\n",
      "----- Epoch: 5/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1/1 [00:00<00:00,  2.70 batch/s]\n",
      "Testing: 100%|| 1/1 [00:00<00:00,  2.72 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.0000\n",
      "\n",
      "   -> Testing Loss:  0.0000\n",
      "\n",
      "Generating Critic data for MEDIUM (5-channel approach)...\n",
      "######## Beginning training for MS Critic ##########\n",
      "----- Epoch: 1/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 12/12 [00:00<00:00, 26.98 batch/s]\n",
      "Testing: 100%|| 3/3 [00:00<00:00,  8.24 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  14639.4238\n",
      "\n",
      "   -> Testing Loss:  3744.6535\n",
      "\n",
      "----- Epoch: 2/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 12/12 [00:00<00:00, 26.74 batch/s]\n",
      "Testing: 100%|| 3/3 [00:00<00:00,  8.06 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  14423.3140\n",
      "\n",
      "   -> Testing Loss:  3391.7636\n",
      "\n",
      "----- Epoch: 3/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 12/12 [00:00<00:00, 27.20 batch/s]\n",
      "Testing: 100%|| 3/3 [00:00<00:00,  8.14 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  14335.0847\n",
      "\n",
      "   -> Testing Loss:  3420.0146\n",
      "\n",
      "----- Epoch: 4/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 12/12 [00:00<00:00, 27.02 batch/s]\n",
      "Testing: 100%|| 3/3 [00:00<00:00,  8.28 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  14340.2143\n",
      "\n",
      "   -> Testing Loss:  3495.1217\n",
      "\n",
      "----- Epoch: 5/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 12/12 [00:00<00:00, 26.85 batch/s]\n",
      "Testing: 100%|| 3/3 [00:00<00:00,  8.20 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  14326.2770\n",
      "\n",
      "   -> Testing Loss:  3512.9125\n",
      "\n",
      "Generating Actor training data for MEDIUM...\n",
      "######## Beginning training for MS Actor ##########\n",
      "----- Epoch: 1/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1/1 [00:00<00:00,  2.52 batch/s]\n",
      "Testing: 100%|| 1/1 [00:00<00:00,  2.75 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  20.7902\n",
      "\n",
      "   -> Testing Loss:  0.0000\n",
      "\n",
      "----- Epoch: 2/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1/1 [00:00<00:00,  2.68 batch/s]\n",
      "Testing: 100%|| 1/1 [00:00<00:00,  2.75 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.0000\n",
      "\n",
      "   -> Testing Loss:  0.0000\n",
      "\n",
      "----- Epoch: 3/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1/1 [00:00<00:00,  2.72 batch/s]\n",
      "Testing: 100%|| 1/1 [00:00<00:00,  2.73 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.0000\n",
      "\n",
      "   -> Testing Loss:  0.0000\n",
      "\n",
      "----- Epoch: 4/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1/1 [00:00<00:00,  2.71 batch/s]\n",
      "Testing: 100%|| 1/1 [00:00<00:00,  2.72 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.0000\n",
      "\n",
      "   -> Testing Loss:  0.0000\n",
      "\n",
      "----- Epoch: 5/5 -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|| 1/1 [00:00<00:00,  2.71 batch/s]\n",
      "Testing: 100%|| 1/1 [00:00<00:00,  2.66 batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   -> Training Loss:  0.0000\n",
      "\n",
      "   -> Testing Loss:  0.0000\n",
      "\n",
      "Generating Critic data for MEDIUM (5-channel approach)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[280], line 19\u001B[0m\n\u001B[1;32m      9\u001B[0m hp_c \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m5\u001B[39m,\n\u001B[1;32m     11\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlearning_rate\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m1e-4\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlinear_dropout\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0.25\u001B[39m\n\u001B[1;32m     15\u001B[0m }\n\u001B[1;32m     17\u001B[0m manager \u001B[38;5;241m=\u001B[39m MSGameManager()\n\u001B[0;32m---> 19\u001B[0m critic, actor \u001B[38;5;241m=\u001B[39m \u001B[43mmanager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcritic_and_actor_training_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhp_a\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhp_a\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhp_c\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhp_c\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[278], line 1006\u001B[0m, in \u001B[0;36mMSGameManager.critic_and_actor_training_loop\u001B[0;34m(self, hp_c, hp_a, num_of_games, epochs, difficulty)\u001B[0m\n\u001B[1;32m   1003\u001B[0m actor \u001B[38;5;241m=\u001B[39m Actor()\n\u001B[1;32m   1005\u001B[0m \u001B[38;5;66;03m# Generate Initial Critic data based off of logic bot\u001B[39;00m\n\u001B[0;32m-> 1006\u001B[0m x, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_critic_training_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_games\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_of_games\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdifficulty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdifficulty\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mold_actor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1008\u001B[0m \u001B[38;5;66;03m# Convert to dataloader\u001B[39;00m\n\u001B[1;32m   1009\u001B[0m critic_train_loader, critic_test_loader \u001B[38;5;241m=\u001B[39m get_dataloader(x, y, \u001B[38;5;241m64\u001B[39m)\n",
      "Cell \u001B[0;32mIn[278], line 580\u001B[0m, in \u001B[0;36mMSGameManager.generate_critic_training_data\u001B[0;34m(self, difficulty, num_games, actor)\u001B[0m\n\u001B[1;32m    578\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    579\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m--> 580\u001B[0m     move \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_actor_move\u001B[49m\u001B[43m(\u001B[49m\u001B[43mactor\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Your trained actor\u001B[39;00m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m move \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    583\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[278], line 752\u001B[0m, in \u001B[0;36mMSGameManager.get_actor_move\u001B[0;34m(self, actor)\u001B[0m\n\u001B[1;32m    750\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msize):\n\u001B[1;32m    751\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m c \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msize):\n\u001B[0;32m--> 752\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mopened_board[r, c] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:  \u001B[38;5;66;03m# Cell is unopened\u001B[39;00m\n\u001B[1;32m    753\u001B[0m             valid_moves\u001B[38;5;241m.\u001B[39mappend((r, c))\n\u001B[1;32m    754\u001B[0m             values\u001B[38;5;241m.\u001B[39mappend(value_map[r, c]\u001B[38;5;241m.\u001B[39mitem())\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 280
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4b50a0ae781c29e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
